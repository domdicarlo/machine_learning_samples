{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Sample",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domdicarlo/machine_learning_samples/blob/master/Machine_Learning_Sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UIuBzUgbNcz",
        "colab_type": "text"
      },
      "source": [
        "This was a homework for a class called Machine Learning in Medicine at UChicago, where I had to learn how to use SKLearn and Keras to learn how to classify cells as cancerous or noncancerous (tumor or normal) based on their RNA sequencing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur1vndtbn8qg",
        "colab_type": "text"
      },
      "source": [
        "# Homework 2\n",
        "## By Dominic DiCarlo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsnPrXo2pNwa",
        "colab_type": "text"
      },
      "source": [
        "Sources consulted in writing this code:\n",
        "\n",
        "https://www.digitalocean.com/community/tutorials/how-to-build-a-machine-learning-classifier-in-python-with-scikit-learn\n",
        "\n",
        "Various documentations, and lots of stack overflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adw4qXrUnT7k",
        "colab_type": "code",
        "outputId": "bb0478da-26b9-47d4-840b-a1bbe9b0b9d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "# Start with importing drive and the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppvwr0yQo9HP",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: Normal and Tumor Match Pair Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGt8dFaR-lhS",
        "colab_type": "text"
      },
      "source": [
        "### 1.a) Using SciKit Learn build a machine learning classifier that takes RNAseq profiles from matched normal tumor pairs and classifies the sample as Normal or Tumor. Compare the nt.coding.csv vs the nt.all.csv.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GZukbjEpH45",
        "colab_type": "text"
      },
      "source": [
        "First, let's simply import the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cait9bYHpFon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the data frame \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# The normal tumor coding data\n",
        "# nt_coding = pd.read_csv(\"/content/drive/My Drive/MLiM-Datasets/HW2/nt.coding.csv\")\n",
        "\n",
        "# All of the normal tumor data\n",
        "# nt_all = pd.read_csv(\"/content/drive/My Drive/MLiM-Datasets/HW2/nt.all.csv\")\n",
        "\n",
        "# Uncomment above later, debug using small sets\n",
        "\n",
        "# The normal tumor coding data\n",
        "nt_coding = pd.read_csv(\"/content/drive/My Drive/MLiM-Datasets/HW2/nt.coding.csv\")\n",
        "\n",
        "# All of the normal tumor data\n",
        "nt_all = pd.read_csv(\"/content/drive/My Drive/MLiM-Datasets/HW2/nt.all.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCYaO4oJ0heT",
        "colab_type": "text"
      },
      "source": [
        "**As a quick observation, the coding data has about 1/3 the columns as the all data. The coding data might be only the relevant columns in determining tumor or non-tumor, as coding means *protein* coding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3zqVU_mr6ZU",
        "colab_type": "text"
      },
      "source": [
        "Now, let's set up scikit-learn for our environment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7sIo97L1Lm3",
        "colab_type": "text"
      },
      "source": [
        "We need to wrangle the dataframe somewhat to make it work with scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcRfB1wm1muT",
        "colab_type": "code",
        "outputId": "efa0614b-9f15-4907-b962-339d6058dafa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "# Organize the data\n",
        "\n",
        "# target labels for data, either 0 for normal\n",
        "# or 1 for tumor\n",
        "\n",
        "# cancer df with all cols except Type (aka, features)\n",
        "# here 'axis = 1' means dropping by columns. to drop by rows,\n",
        "# type 'axis = 0'\n",
        "features_ntc = nt_coding.drop('Type', axis=1)\n",
        "\n",
        "# the labels of the data\n",
        "labels_ntc = nt_coding['Type'].to_numpy()\n",
        "# the names of the different features, in this case input gene names\n",
        "feature_names_ntc = nt_coding.columns\n",
        "# the feature data\n",
        "features_ntc = nt_coding.to_numpy()\n",
        "\n",
        "print(features_ntc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 1.50265481e+05 4.32784587e+03 ... 8.01630849e+04\n",
            "  6.83730154e+03 2.00744219e+02]\n",
            " [0.00000000e+00 9.13228182e+05 2.32628469e+03 ... 5.54904160e+04\n",
            "  7.42455814e+02 0.00000000e+00]\n",
            " [0.00000000e+00 3.59658935e+05 2.28971471e+05 ... 6.26771481e+04\n",
            "  4.31990634e+03 0.00000000e+00]\n",
            " ...\n",
            " [0.00000000e+00 5.71780211e+05 0.00000000e+00 ... 4.38786858e+05\n",
            "  1.94489970e+03 0.00000000e+00]\n",
            " [1.00000000e+00 2.33314351e+05 0.00000000e+00 ... 3.07254516e+04\n",
            "  1.30123586e+04 0.00000000e+00]\n",
            " [1.00000000e+00 4.50226528e+05 2.41894075e+03 ... 9.89409523e+04\n",
            "  7.73782481e+03 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akq84-xCpAAS",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the data in a form usable by scikit-learn, let's split it into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKWk-CanpXjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# split data into testing and training\n",
        "# test_size = 0.30 means 30% for testing and 70% for training\n",
        "# this is a basic statistic 70/30 rule. \n",
        "# random state is simply a seed \n",
        "# this also default shuffles the data before splitting, which is important\n",
        "# for this assignment (reportedly)\n",
        "train_ntc, test_ntc, train_labels_ntc, test_labels_ntc = train_test_split(\n",
        "                                                          features_ntc,\n",
        "                                                          labels_ntc,\n",
        "                                                          test_size=0.30, \n",
        "                                                          random_state=30)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd7dlDb9A_Rn",
        "colab_type": "text"
      },
      "source": [
        "I will use Support Vector Machine (sigmoid kernel), KMeans clustering, a decision tree, and Naive Bayes to get a good mix of classical machine learning methods of different types. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEv3QSUBBYP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# initialize the classifiers, mostly using default parameters\n",
        "# due to ignorance on what would be best\n",
        "sigmoid_svm = SVC(kernel=\"sigmoid\", gamma = \"auto\")\n",
        "dtree = DecisionTreeClassifier()\n",
        "kmeans = KMeans(n_clusters = 2)\n",
        "gaussian_nb = GaussianNB()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N2swRl3raf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the classifiers\n",
        "model_sigmoid_svm = sigmoid_svm.fit(train_ntc, train_labels_ntc)\n",
        "model_dtree = dtree.fit(train_ntc, train_labels_ntc)\n",
        "model_kmeans = kmeans.fit(train_ntc, train_labels_ntc)\n",
        "model_gaussian_nb = gaussian_nb.fit(train_ntc, train_labels_ntc)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Mo-3jJGRWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make predictions \n",
        "\n",
        "preds_sigmoid_svm = sigmoid_svm.predict(test_ntc)\n",
        "preds_dtree = dtree.predict(test_ntc)\n",
        "preds_kmeans = kmeans.predict(test_ntc)\n",
        "preds_gaussian_nb = gaussian_nb.predict(test_ntc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM0SDllzGCPq",
        "colab_type": "code",
        "outputId": "db62d967-5add-4f1f-d102-9862df880062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Get the accuracy scores \n",
        "print(\"SVM Sigmoid %\", 100 * accuracy_score(test_labels_ntc, preds_sigmoid_svm))\n",
        "print(\"Decision tree %\", 100 * accuracy_score(test_labels_ntc, preds_dtree))\n",
        "print(\"KMeans %\", 100 * accuracy_score(test_labels_ntc, preds_kmeans))\n",
        "print(\"Gaussian Naive Bayes %\", 100 * accuracy_score(test_labels_ntc, preds_gaussian_nb))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Sigmoid % 51.66666666666667\n",
            "Decision tree % 100.0\n",
            "KMeans % 45.666666666666664\n",
            "Gaussian Naive Bayes % 95.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2qF_8owMcV0",
        "colab_type": "text"
      },
      "source": [
        "**We see here that while two of our methods work fairly well (decision tree and Gaussian Naive Bayes), two perform pretty poorly at around the level of guessing (Scalar Vector Machine and KMeans). While I am unsure of exactly what is going on with SVM, for KMeans this isn't too unreasonable, since KMeans is computing means and the data is binary coded.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0RwT4Hafny2",
        "colab_type": "text"
      },
      "source": [
        "Let's quickly perform the same calculations on the nt_all.csv data, to see if we get anything dramatically different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yMt-jThfuQV",
        "colab_type": "code",
        "outputId": "8b27ef5b-6c51-4d96-ca7c-313f2370cedf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Organize the data\n",
        "\n",
        "# target labels for data, either 0 for normal\n",
        "# or 1 for tumor\n",
        "\n",
        "# cancer df with all cols except Type (aka, features)\n",
        "# here 'axis = 1' means dropping by columns. to drop by rows,\n",
        "# type 'axis = 0'\n",
        "features_nta = nt_all.drop('Type', axis=1)\n",
        "\n",
        "# the labels of the data\n",
        "labels_nta = nt_all['Type'].to_numpy()\n",
        "# the names of the different features, in this case input gene names\n",
        "feature_names_nta = nt_all.columns\n",
        "# the feature data\n",
        "features_nta = nt_all.to_numpy()\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# split data into testing and training\n",
        "# test_size = 0.30 means 30% for testing and 70% for training\n",
        "# this is a basic statistic 70/30 rule. \n",
        "# random state is simply a seed \n",
        "# this also default shuffles the data before splitting, which is important\n",
        "# for this assignment (reportedly)\n",
        "train_nta, test_nta, train_labels_nta, test_labels_nta = train_test_split(\n",
        "                                                          features_nta,\n",
        "                                                          labels_nta,\n",
        "                                                          test_size=0.30, \n",
        "                                                          random_state=30)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# initialize the classifiers, mostly using default parameters\n",
        "# due to ignorance on what would be best\n",
        "sigmoid_svm = SVC(kernel=\"sigmoid\", gamma = \"auto\")\n",
        "dtree = DecisionTreeClassifier()\n",
        "kmeans = KMeans(n_clusters = 2)\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# train the classifiers\n",
        "model_sigmoid_svm = sigmoid_svm.fit(train_nta, train_labels_nta)\n",
        "model_dtree = dtree.fit(train_nta, train_labels_nta)\n",
        "model_kmeans = kmeans.fit(train_nta, train_labels_nta)\n",
        "model_gaussian_nb = gaussian_nb.fit(train_nta, train_labels_nta)\n",
        "\n",
        "# make predictions \n",
        "\n",
        "preds_sigmoid_svm = sigmoid_svm.predict(test_nta)\n",
        "preds_dtree = dtree.predict(test_nta)\n",
        "preds_kmeans = kmeans.predict(test_nta)\n",
        "preds_gaussian_nb = gaussian_nb.predict(test_nta)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Get the accuracy scores \n",
        "print(\"SVM Sigmoid %\", 100 * accuracy_score(test_labels_nta, preds_sigmoid_svm))\n",
        "print(\"Decision tree %\", 100 * accuracy_score(test_labels_nta, preds_dtree))\n",
        "print(\"KMeans %\", 100 * accuracy_score(test_labels_nta, preds_kmeans))\n",
        "print(\"Gaussian Naive Bayes %\", 100 * accuracy_score(test_labels_nta, preds_gaussian_nb))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Sigmoid % 51.66666666666667\n",
            "Decision tree % 100.0\n",
            "KMeans % 45.666666666666664\n",
            "Gaussian Naive Bayes % 86.33333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwXFT2hghlCu",
        "colab_type": "text"
      },
      "source": [
        "We are getting very much the same performance, so let's just stick to the coding for computational speed for the rest of this data. This makes sense to, since the coding data stands for protein coding genes, which are the genes we will care most about in cell behavior (if not care only about), aka deciding cancerous or noncancerous. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvYMhilnOC3T",
        "colab_type": "text"
      },
      "source": [
        "### 1.b) Model Selection\n",
        "\n",
        "Let's amp up our model selection a bit more than this, and do KFold Cross Validation instead of a simple accuracy test with one split of the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY_o2Sy-U3Ul",
        "colab_type": "code",
        "outputId": "40c761be-6e6d-4589-cba0-22f9cb554b24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score \n",
        "# initialize the classifiers, mostly using default parameters\n",
        "# due to ignorance on what would be best\n",
        "sigmoid_svm = SVC(kernel=\"sigmoid\", gamma = \"auto\")\n",
        "dtree = DecisionTreeClassifier()\n",
        "kmeans = KMeans(n_clusters = 2)\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "cross_val = KFold(5)\n",
        "\n",
        "# For sigmoid svm\n",
        "acc_sigmoid_svm = cross_val_score(sigmoid_svm, features_ntc, labels_ntc, cv=cross_val)\n",
        "# For decision tree\n",
        "acc_dtree = cross_val_score(dtree, features_ntc, labels_ntc, cv=cross_val)\n",
        "# For KMeans\n",
        "acc_kmeans = cross_val_score(kmeans, features_ntc, labels_ntc, cv=cross_val)\n",
        "# For Gaussian Naive Bayes\n",
        "acc_gaussian_nb = cross_val_score(gaussian_nb, features_ntc, labels_ntc, cv=cross_val)\n",
        "\n",
        "print(\"Sigmoid SVM accuracy: \", acc_sigmoid_svm)\n",
        "print(\"Decision tree accuracy: \", acc_dtree)\n",
        "print(\"KMeans accuracy: \", acc_kmeans)\n",
        "print(\"Gaussian Naive Bayes accuracy: \", acc_gaussian_nb)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sigmoid SVM accuracy:  [0.52  0.49  0.52  0.48  0.515]\n",
            "Decision tree accuracy:  [1. 1. 1. 1. 1.]\n",
            "KMeans accuracy:  [-1.29370632e+20 -1.24611080e+20 -1.63783038e+20 -9.27141521e+19\n",
            " -1.18588730e+20]\n",
            "Gaussian Naive Bayes accuracy:  [0.96  0.94  0.905 0.925 0.945]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9DR2WxzbvnU",
        "colab_type": "text"
      },
      "source": [
        "**We see the KFold accuracy test gives us more or less the same values as a simple one train, one test set accuracy test. Decision tree performs remarkably well, which suggests there may be some key combination of genes that is always indicative of cancer, or at least in this set of data (it may be overtrained). Gaussian Naive Bayes is promising, while Sigmoid is no better than chance, and KMeans is doing something strange. Again, this is likely because we shouldn't use KMeans for binary data, the mean doesn't have the same significance it should have for a continuous data range.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrvQOtw0f7Ma",
        "colab_type": "text"
      },
      "source": [
        "### 1.c) Feature Selection (P < 100)\n",
        "\n",
        "Let's get into some feature selection now. Let's first try variance threshold, a good baseline method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seLXp_0qf5XY",
        "colab_type": "code",
        "outputId": "b6d6eccf-6a95-4b17-ecd2-8e929a9d4616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# create a variance threshold selector\n",
        "variance_selector = VarianceThreshold(threshold=300000000000000)\n",
        "\n",
        "print(\"Original number of features:\", features_ntc.shape[1])\n",
        "\n",
        "# fit the features to the selector\n",
        "features_ntc_sel = variance_selector.fit_transform(features_ntc)\n",
        "\n",
        "print(\"Variance selected features:\", features_ntc_sel.shape[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original number of features: 19562\n",
            "Variance selected features: 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xWyBuMaiS_T",
        "colab_type": "text"
      },
      "source": [
        "**We see here it takes using a varance threshold of 300 trillion to trim the number of features down to less than 100. This data has very high variance! This indicates the data is very spread out, that any given gene can be expressed very differently by different cells. By selecting the ones with the highest variance, we are looking at the gene expressions that vary the most between cells (of which we might expect that cancerous cells and noncancerous cells are most different from each other!).**\n",
        "\n",
        "Let's try using this reduced X to build a classifier. We will use Gaussian Naive Bayes because it is pretty good but not perfect like decision tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c69EQTmjrwM",
        "colab_type": "code",
        "outputId": "45adac8c-e90d-414d-eb38-a64994cdee68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# For Gaussian Naive Bayes\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# check the cross validation score for cv = KFold(5)\n",
        "acc_gaussian_nb_sel = cross_val_score(gaussian_nb, features_ntc_sel, labels_ntc, cv=cross_val)\n",
        "\n",
        "print(\"The Gaussian NB P ~20,000 data accuracy:\", acc_gaussian_nb)\n",
        "print(\"The Gaussian NB P<100 data accuracy:\", acc_gaussian_nb_sel)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Gaussian NB P ~20,000 data accuracy: [0.96  0.94  0.905 0.925 0.945]\n",
            "The Gaussian NB P<100 data accuracy: [0.77  0.76  0.725 0.755 0.73 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWTIQ_KfkB--",
        "colab_type": "text"
      },
      "source": [
        "**We see that P<100 is not quite as accurate, but still decent for a huge reduction in features (as in, better than chance).**\n",
        "\n",
        "Let's try this for decision tree now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPK0eN6ItwPh",
        "colab_type": "code",
        "outputId": "46be1667-b5d0-41de-9943-48587c13aab1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# For decision tree\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "# check the cross validation score for cv = KFold(5)\n",
        "acc_dtree_sel = cross_val_score(dtree, features_ntc_sel, labels_ntc, cv=cross_val)\n",
        "\n",
        "print(\"The Decision Tree  p~20,000 data accuracy:\", acc_dtree)\n",
        "print(\"The Decision Tree P<100 data accuracy:\", acc_dtree_sel)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Decision Tree  p~20,000 data accuracy: [1. 1. 1. 1. 1.]\n",
            "The Decision Tree P<100 data accuracy: [0.9   0.845 0.765 0.855 0.835]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsqOZDcltI8x",
        "colab_type": "text"
      },
      "source": [
        "We see that the same feature selection works alright for Decision tree too.\n",
        "\n",
        "Now let's try a different feature selector, using a Tree-based feature selection method. This might lend itself quite well to the fact that a decision tree is acting as a good classifier, perhaps there is some tree like thing going on with the important features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alx2ZmPbx5YR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import numpy as np\n",
        "\n",
        "# This uses a tree model to predict the strongest indicators\n",
        "# of the data\n",
        "tree_class= ExtraTreesClassifier(n_estimators=50)\n",
        "\n",
        "tree_class = tree_class.fit(features_ntc, labels_ntc)\n",
        "\n",
        "# here we set an infinite threshold so that we can just take the best 99 features\n",
        "model = SelectFromModel(tree_class, prefit=True, threshold=-np.inf, max_features=99)\n",
        "\n",
        "# transform old features to new set\n",
        "features_ntc_sel = model.transform(features_ntc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoowgGITx1m-",
        "colab_type": "text"
      },
      "source": [
        "First for Gaussian NB:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnI64I-byNsg",
        "colab_type": "code",
        "outputId": "d98d4f4a-9181-480c-db07-640e6b6b4f66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "# for Gaussian\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# check the cross validation score for cv = KFold(5)\n",
        "acc_gaussian_nb_sel = cross_val_score(gaussian_nb, features_ntc_sel, labels_ntc, cv=cross_val)\n",
        "\n",
        "print(\"The Gaussian NB P ~20,000 data accuracy:\", acc_gaussian_nb)\n",
        "print(\"The Gaussian NB P<100 data accuracy:\", acc_gaussian_nb_sel)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Gaussian NB P ~20,000 data accuracy: [0.96  0.94  0.905 0.925 0.945]\n",
            "The Gaussian NB P<100 data accuracy: [0.88  0.885 0.87  0.905 0.86 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEEVhFIryS5p",
        "colab_type": "text"
      },
      "source": [
        "Then for Decision Tree:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCHjDlHeuWTt",
        "colab_type": "code",
        "outputId": "5dc4145f-86f3-4fdb-a597-db3fc20d81c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# For decision tree\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "# check the cross validation score for cv = KFold(5)\n",
        "acc_dtree_sel = cross_val_score(dtree, features_ntc_sel, labels_ntc, cv=cross_val)\n",
        "\n",
        "print(\"The Decision Tree  p~20,000 data accuracy:\", acc_dtree)\n",
        "print(\"The Decision Tree P<100 data accuracy:\", acc_dtree_sel)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Decision Tree  p~20,000 data accuracy: [1. 1. 1. 1. 1.]\n",
            "The Decision Tree P<100 data accuracy: [1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUoNP09UyZsN",
        "colab_type": "text"
      },
      "source": [
        "**The Tree Based Feature selection method performs very well, supporting the idea that cancer is encoded in the gene expression in some tree-like pattern, where certain combinations of cancer genes indicate tumor/or non tumor.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bIazxCRy5I7",
        "colab_type": "text"
      },
      "source": [
        "### 1.d) Keras Deep Learning Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPT8ou0kzCnx",
        "colab_type": "text"
      },
      "source": [
        "Now we need to do create a deep learning classifier that can perform the same task as before. As I was doing this, I discovered a (what should have been) fatal error I had made before (that slowed down my progress greatly :( ...). I didn't normalize the data for the classical methods earlier. So first I will have to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN9NlJ1S03S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# scaler to normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "# fit the normalizer\n",
        "scaler.fit(features_ntc)\n",
        "# use the normalizer\n",
        "features_ntc = scaler.transform(features_ntc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngik0AkrN4M9",
        "colab_type": "text"
      },
      "source": [
        "With that being properly done, we can prepare a neural network now using Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SXkyuOM095T",
        "colab_type": "code",
        "outputId": "ab2c457e-b4be-4d13-cea2-3e9de34f226c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# reevaluate this to bring it back into environment\n",
        "train_ntc, test_ntc, train_labels_ntc, test_labels_ntc = train_test_split(\n",
        "                                                          features_ntc,\n",
        "                                                          labels_ntc,\n",
        "                                                          test_size=0.30, \n",
        "                                                          random_state=30)\n",
        "\n",
        "number_features = features_ntc.shape[1] # grab the number of columns \n",
        "\n",
        "# we put this into a function so we can use the KerasClassifier function on it\n",
        "# which allows us to do KFold Cross validation using Sklearn...\n",
        "# in the future could create some sort of simpler neural net maker function\n",
        "# but pressed for time with this homework. \n",
        "def create_model1():\n",
        "  # input layer followed by our 2 hidden layers and output\n",
        "  input_layer = Input((number_features, ))\n",
        "  hidden = Dense(8, activation = \"relu\")(input_layer) # each layer feeds into another\n",
        "  hidden  = Dense(8, activation = \"relu\")(hidden)\n",
        "  output_layer = Dense(1, activation=\"sigmoid\")(hidden)\n",
        "\n",
        "\n",
        "# create the model now\n",
        "  deep_model = Model(input = input_layer, output = output_layer)\n",
        "\n",
        "# compile with some parameters we will change\n",
        "  deep_model.compile(optimizer = SGD(lr=0.01, clipnorm=1), loss = \"mse\",\n",
        "                   metrics = [\"accuracy\"])\n",
        "  \n",
        "  return deep_model\n",
        "\n",
        "deep_model = create_model1()\n",
        "print(deep_model.summary())\n",
        "# train model\n",
        "history = deep_model.fit(train_ntc, train_labels_ntc, validation_data=(test_ntc, test_labels_ntc),\n",
        "                         epochs=150, batch_size=100)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_69\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_70 (InputLayer)        (None, 19562)             0         \n",
            "_________________________________________________________________\n",
            "dense_220 (Dense)            (None, 8)                 156504    \n",
            "_________________________________________________________________\n",
            "dense_221 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_222 (Dense)            (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 156,585\n",
            "Trainable params: 156,585\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_1RxFUKOEPS",
        "colab_type": "text"
      },
      "source": [
        "Plotting the accuracy and loss functions now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyHVg0pUPjXD",
        "colab_type": "code",
        "outputId": "82a188e3-89b8-45e2-9524-83981e1d45e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8XFed9/HPb0Ya9S65yrbcY6e5\nKA0ngZDmJJBCIKRBEsITWDYLLCxLskBYssDDwi4EsiGFJ1lCSSMFHOKQXki3XGLHXe6yHVtW79JI\n5/njXDuyI1tSovGMpO/79ZqXZm6Z+ekmmq/POfeea845REREDiUU7wJERCTxKSxERKRXCgsREemV\nwkJERHqlsBARkV4pLEREpFcKC5EBYGa/NbMf9nHbzWZ2xod9H5HDSWEhIiK9UliIiEivFBYybATd\nP98ys+Vm1mRmd5vZSDN70swazOxZM8vrtv35ZrbSzGrN7EUzm9Ft3WwzWxLs9yCQesBnfcLMlgX7\nvmZmx3zAmv+PmZWbWbWZLTCzMcFyM7NfmNluM6s3sxVmdlSw7lwzWxXUtt3M/uUDHTCRbhQWMtxc\nDJwJTAM+CTwJ/BtQhP97+CqAmU0D7ge+HqxbCDxuZhEziwB/Bn4P5AN/Ct6XYN/ZwD3Al4AC4E5g\ngZml9KdQM/s48H+BS4DRwBbggWD1WcCpwe+RE2xTFay7G/iScy4LOAp4vj+fK9IThYUMN7c653Y5\n57YDfwfedM4tdc61Ao8Bs4PtPgs84Zx7xjnXAfwXkAZ8BDgRSAZucc51OOceBhZ1+4zrgDudc286\n5zqdc/cCbcF+/XEFcI9zbolzrg24ETjJzEqADiALOAIw59xq59zOYL8OYKaZZTvnapxzS/r5uSLv\no7CQ4WZXt+ctPbzODJ6Pwf9LHgDnXBewDRgbrNvu9p+Fc0u35xOAbwZdULVmVguMC/brjwNraMS3\nHsY6554H/ge4DdhtZneZWXaw6cXAucAWM3vJzE7q5+eKvI/CQqRnO/Bf+oAfI8B/4W8HdgJjg2V7\nje/2fBvwI+dcbrdHunPu/g9ZQwa+W2s7gHPuV865ucBMfHfUt4Lli5xzFwAj8N1lD/Xzc0XeR2Eh\n0rOHgPPM7HQzSwa+ie9Keg14HYgCXzWzZDP7FHB8t31/A3zZzE4IBqIzzOw8M8vqZw33A9eY2axg\nvOPH+G6zzWZ2XPD+yUAT0Ap0BWMqV5hZTtB9Vg90fYjjIAIoLER65JxbC1wJ3ArswQ+Gf9I51+6c\nawc+BVwNVOPHNx7ttm8Z8H/w3UQ1QHmwbX9reBb4HvAIvjUzGbg0WJ2ND6UafFdVFfCzYN3ngM1m\nVg98GT/2IfKhmG5+JCIivVHLQkREeqWwEBGRXiksRESkVwoLERHpVVK8CxgohYWFrqSkJN5liIgM\nKosXL97jnCvqbbshExYlJSWUlZXFuwwRkUHFzLb0vlWMu6HMbL6ZrQ1mzbyhh/XfCGbHXG5mz5lZ\n96tVO4NZO5eZ2YJY1ikiIocWs5aFmYXx89acCVQAi8xsgXNuVbfNlgKlzrlmM/sH4Kf4C5wAWpxz\ns2JVn4iI9F0sWxbHA+XOuY3BFa8PABd038A594Jzrjl4+QZQHMN6RETkA4rlmMVY/IRqe1UAJxxi\n+2vx9xbYK9XMyvBz8PzEOffnA3cws+vw00Ezfvz4A1fT0dFBRUUFra2t/a9+kElNTaW4uJjk5OR4\nlyIiQ1BCDHCb2ZVAKfDRbosnOOe2m9kk4HkzW+Gc29B9P+fcXcBdAKWlpe+bt6SiooKsrCxKSkrY\nf4LQocU5R1VVFRUVFUycODHe5YjIEBTLbqjt+Cmd9yoOlu3HzM4AvgOcH9zgBYDg5jQ45zYCL/Le\nTWn6rLW1lYKCgiEdFABmRkFBwbBoQYlIfMQyLBYBU81sYnAbykuB/c5qCm4/eSc+KHZ3W5639xaU\nZlYIzAO6D4z32VAPir2Gy+8pIvERs24o51zUzK4HngLC+NtDrjSzm4Ey59wC/JTKmcCfgi+7rc65\n84EZwJ1m1oUPtJ8ccBbVgIl2dVHV2E5WahLpkYTolRMRSTgx/XZ0zi3E3+i++7Kbuj0/4yD7vQYc\nHcvauttV30rI0mISFrW1tdx333185Stf6dd+5557Lvfddx+5ubkDXpOISH8N+7mhwmaEzOjojM3N\nxGpra/n1r3/9vuXRaPSQ+y1cuFBBISIJY9j3u5gZyeFQzMLihhtuYMOGDcyaNYvk5GRSU1PJy8tj\nzZo1rFu3jgsvvJBt27bR2trK1772Na677jrgvelLGhsbOeecczj55JN57bXXGDt2LH/5y19IS0uL\nSb0iIj0ZNmHxg8dXsmpHfY/rWjs6AUhNDvfrPWeOyeb7nzzykNv85Cc/4Z133mHZsmW8+OKLnHfe\nebzzzjv7TnG95557yM/Pp6WlheOOO46LL76YgoKC/d5j/fr13H///fzmN7/hkksu4ZFHHuHKK6/s\nV60iIh/GsAmLQzEzOrsOz+1ljz/++P2uhfjVr37FY489BsC2bdtYv379+8Ji4sSJzJrlZz6ZO3cu\nmzdvPiy1iojsNWzC4lAtgJ11LexpbOeoMdkxPwU1IyNj3/MXX3yRZ599ltdff5309HQ+9rGP9Xit\nREpKyr7n4XCYlpaWmNYoInKgYT/ADZAcCuGci0nrIisri4aGhh7X1dXVkZeXR3p6OmvWrOGNN94Y\n8M8XERkIw6ZlcSjJYd+a6Oh0JPVv2KJXBQUFzJs3j6OOOoq0tDRGjhy5b938+fO54447mDFjBtOn\nT+fEE08c2A8XERkg5tzh6auPtdLSUnfgzY9Wr17NjBkzet23qS3KhspGSgoyyE4bvBPx9fX3FRHZ\ny8wWO+dKe9tO3VBActgfho6u2Jw+KyIy2CksgKSgGyraOTRaWSIiA01hAYTMSArF7sI8EZHBTmER\nSA4bHWpZiIj0SGERiOWUHyIig53CorMD9pSTSRNRhYWISI8UFhaC9gZS6SDa5ega4AvzDjbrbF/c\ncsstNDc3D2g9IiIfhMIiFAYLk4SfMnygT59VWIjIUKAruAHCSYSdD4topyNlAI9K9ynKzzzzTEaM\nGMFDDz1EW1sbF110ET/4wQ9oamrikksuoaKigs7OTr73ve+xa9cuduzYwWmnnUZhYSEvvPDCwBUl\nItJPwycsnrwB3l3R87qOZpKASV0RUpJDEOpjg2vU0XDOTw65Sfcpyp9++mkefvhh3nrrLZxznH/+\n+bz88stUVlYyZswYnnjiCcDPGZWTk8PPf/5zXnjhBQoLC/vxi4qIDDx1Q4Eft8CPVcRypvKnn36a\np59+mtmzZzNnzhzWrFnD+vXrOfroo3nmmWf49re/zd///ndycnJiV4SIyAcwfFoWh2oB1G2Hpko2\nu4nkZ0QYkxubu9A557jxxhv50pe+9L51S5YsYeHChXz3u9/l9NNP56abburhHURE4kMtC4BwMoYj\nNewG/FqL7lOUn3322dxzzz00NjYCsH37dnbv3s2OHTtIT0/nyiuv5Fvf+hZLlix5374iIvE0fFoW\nhxL2M82mhrpoHeCruLtPUX7OOedw+eWXc9JJJwGQmZnJH/7wB8rLy/nWt75FKBQiOTmZ22+/HYDr\nrruO+fPnM2bMGA1wi0hcaYpygLZGqFpPZaSYyo4UZo7OjlGVsaUpykWkvzRFeX+EfQMrYp1EO7vo\nGiIBKiIyUBQWACHfDZVMJ4Cm/RAROcCQD4s+dbPtu4rbh0X7IJx9dqh0J4pIYhrSYZGamkpVVVXf\nvki7XcU92Gafdc5RVVVFampqvEsRkSFqSJ8NVVxcTEVFBZWVlb1v3FiJc45d0VpaK5PISh1c9+JO\nTU2luLg43mWIyBA1pMMiOTmZiRMn9m3jR34O297g8vr/5hPHjOaHF+qsIhGRvYZ0N1S/ZI2ChncZ\nnZ3KztrWeFcjIpJQFBZ7ZY2GznamZnWwo05hISLSncJir6yRAExJb2RnXUucixERSSwKi72yRgNQ\nEqmntrmD5vZonAsSEUkcCou9Mn3LYmxSPQA7NG4hIrJPTMPCzOab2VozKzezG3pY/w0zW2Vmy83s\nOTOb0G3dVWa2PnhcFcs6gX0tixFuD4C6okREuolZWJhZGLgNOAeYCVxmZjMP2GwpUOqcOwZ4GPhp\nsG8+8H3gBOB44PtmlherWgGIpEPWaPLbKgB0RpSISDexbFkcD5Q75zY659qBB4ALum/gnHvBOdcc\nvHwD2HtV2dnAM865audcDfAMMD+GtXr5k0lr2IQZVNSqZSEislcsw2IssK3b64pg2cFcCzzZn33N\n7DozKzOzsj5dpd2bgsmEqjcyLi+dTXuaPvz7iYgMEQkxwG1mVwKlwM/6s59z7i7nXKlzrrSoqOjD\nF1IwGZqrODq/i/LdjR/+/UREhohYhsV2YFy318XBsv2Y2RnAd4DznXNt/dl3wBVMAaA0q5qNlY10\ndWkmVxERiG1YLAKmmtlEM4sAlwILum9gZrOBO/FBsbvbqqeAs8wsLxjYPitYFlv5kwE4IlJJW7SL\n7Rq3EBEBYhgWzrkocD3+S3418JBzbqWZ3Wxm5web/QzIBP5kZsvMbEGwbzXwH/jAWQTcHCyLrbwS\nwBjPTgDKK9UVJSICMZ511jm3EFh4wLKbuj0/4xD73gPcE7vqepCcCrnjKGr3p89u2N3IadNHHNYS\nREQSUUIMcCeU/MlE6jaRnxFhg1oWIiKAwuL9CqZA1QamFGawYbdOnxURAYXF+xVMhrZ6jino0JiF\niEhAYXGg4IyoY9P2UN3UTnVTe5wLEhGJP4XFgQp8WExJ2gWgcQsRERQW75c7AULJjI36awA36Epu\nERGFxfuEkyB/ElmNG0lJCmnaDxERFBY9K5qOVa5lUlGmuqFERFBY9KxoOtRs4ojCiM6IEhFBYdGz\noiPAdTEns4qKmhZaOzrjXZGISFwpLHpSOA2Amck7cQ7d20JEhj2FRU8KpwLG+E5//yUNcovIcKew\n6ElyGuRNIL/Z32JVg9wiMtwpLA6mcDrhqnWMy0tXy0JEhj2FxcEUTYeqcqYWprKhUmMWIjK8KSwO\npmg6dLYzN6eejZWNdOoWqyIyjCksDqboCACOjuykLdrFDt1iVUSGMYXFwRROBWCi83fN07iFiAxn\nCouDSc2BrNEUtW0BdEaUiAxvCotDKZhCSu1GCjMjrNvVEO9qRETiRmFxKIVToWo9M0ZlsWpnfbyr\nERGJG4XFoRRMhdY6Sos6WfduIx2dXfGuSEQkLhQWhxLMETUnYw/tnV2s36VxCxEZnhQWh1I4BYDp\nwS1WV+6oi2c1IiJxo7A4lJxxEE6hsHULaclhVu7QuIWIDE8Ki0MJhaFgMqHqcmaMzmKVwkJEhimF\nRW8KpsCe9Rw5JodVO+vp0rQfIjIMKSx6UzgNajZz9Kg0GtuibK1ujndFIiKHncKiN4VTwXUyK7MW\nQOMWIjIsKSx6U+DniCphB0kh0xlRIjIsKSx6E5w+G6ktZ9rILJZXKCxEZPhRWPQmNQcyR0HlWuZO\nyGPZtlrd20JEhh2FRV+MPhZ2LGPuhDwa26KaVFBEhp2YhoWZzTeztWZWbmY39LD+VDNbYmZRM/v0\nAes6zWxZ8FgQyzp7NWY27FlL6egIAIu31MS1HBGRwy1mYWFmYeA24BxgJnCZmc08YLOtwNXAfT28\nRYtzblbwOD9WdfbJmNnguhjbtp6irBSWKCxEZJiJZcvieKDcObfROdcOPABc0H0D59xm59xyILGn\ncx0zCwDb+TZzx+exeKvCQkSGl1iGxVhgW7fXFcGyvko1szIze8PMLuxpAzO7LtimrLKy8sPUemhZ\noyBrNOxYytwJeWypaqayoS12nycikmASeYB7gnOuFLgcuMXMJh+4gXPuLudcqXOutKioKLbVjJkN\nO5YyZ0IeAEvUuhCRYSSWYbEdGNftdXGwrE+cc9uDnxuBF4HZA1lcv42eBXvWc1ShEQmHNG4hIsNK\nLMNiETDVzCaaWQS4FOjTWU1mlmdmKcHzQmAesCpmlfbFmNmAI6VyJUcX5+iMKBEZVmIWFs65KHA9\n8BSwGnjIObfSzG42s/MBzOw4M6sAPgPcaWYrg91nAGVm9jbwAvAT51ycw8IPcu8dt1i+vY62aGdc\nSxIROVySYvnmzrmFwMIDlt3U7fkifPfUgfu9Bhwdy9r6LXMEZBfD9jLmzPg0d728kZU76pkzPi/e\nlYmIxFwiD3AnngknweZXmTM+B0DjFiIybCgs+qPkZGjazYj2Csbnp2vcQkSGDYVFf5Sc4n9u/jtz\nJ+RRtqUG5zSpoIgMfQqL/sif5Geg3fwKcybkUdnQRkVNS7yrEhGJOYVFf5j5rqjNrzJ3XC6gi/NE\nZHhQWPRXycnQ+C7TI7vJiIQp26ywEJGhT2HRX8G4RXjLK5SW5PNq+Z44FyQiEnt9Cgsz+5qZZZt3\nd3APirNiXVxCKpgMmSNh8yucMWMEG/c0Ub67Md5ViYjEVF9bFl9wztUDZwF5wOeAn8SsqkS2d9xi\ny6ucMWMEAM+s2hXnokREYquvYWHBz3OB3zvnVnZbNvyUnAwNOxnduZOjxmbz7GqFhYgMbX0Ni8Vm\n9jQ+LJ4ysywS/YZFsTThZP9z8985c8Yolmyt0f0tRGRI62tYXAvcABznnGsGkoFrYlZVoiucChkj\nYPMrnDlzJM7B82vUuhCRoauvYXESsNY5V2tmVwLfBepiV1aC63a9xYxRmYzNTePplQoLERm6+hoW\ntwPNZnYs8E1gA/C7mFU1GJTMg4YdWM0mPnHMaF5aV0lVo7qiRGRo6mtYRJ2fBOkC4H+cc7cBWbEr\naxDYN0/UK3xqTjHRLsfjb++Ib00iIjHS17BoMLMb8afMPmFmIfy4xfBVOA0yimDTy0wflcWRY7J5\ndGmf7xorIjKo9DUsPgu04a+3eBd/w6KfxayqwcAMpp8LaxdCSy0XzylmeUUd63c1xLsyEZEB16ew\nCALij0COmX0CaHXODe8xC4DSa6CjGZY/xPmzxhAOGY8sUetCRIaevk73cQnwFv5e2ZcAb5rZp2NZ\n2KAwZrZ/lN1DYUaEU6cW8sSKHbrHhYgMOX3thvoO/hqLq5xznweOB74Xu7IGkdIvQOVq2PoGH58x\nkm3VLWza0xTvqkREBlRfwyLknNvd7XVVP/Yd2o66GFKyoexuPjq1CICX11XGuSgRkYHV1y/8v5nZ\nU2Z2tZldDTwBLIxdWYNIJANmXwkrH2N8UjWTCjN4SWEhIkNMXwe4vwXcBRwTPO5yzn07loUNKid8\nGZyDN+/g1GlFvL6xitaOznhXJSIyYPrcleSce8Q5943g8Vgsixp08ibAkRdC2W/5+MRUWju6WLS5\nOt5ViYgMmEOGhZk1mFl9D48GM6s/XEUOCh/5J2hv4KSax4kkhXhprbqiRGToOGRYOOeynHPZPTyy\nnHPZh6vIQWHMbJhwMsnLfscJE/N5dvUunUIrIkOGzmgaSDMvgOoNXDk1yuaqZl4tr4p3RSIiA0Jh\nMZCmngHAx5OWU5AR4d7XN8e1HBGRgaKwGEj5kyB/Mskbn+Ozx43judW7qKhpjndVIiIfmsJioE09\nCzb/nStKRwDwxze3xrkgEZEPT2Ex0KaeAdFWxtYs5owZI3lw0Tbao8P3duUiMjQoLAbahJMhKQ3W\nP8Nlx4+nuqld9+cWkUFPYTHQklNh4qmw5q+cMimHEVkp/KmsIt5ViYh8KDENCzObb2ZrzazczG7o\nYf2pZrbEzKIHTnluZleZ2frgcVUs6xxwpddA/XaS1izg4rnFvLiukt0NrfGuSkTkA4tZWJhZGLgN\nOAeYCVxmZjMP2GwrcDVw3wH75gPfB07AT4f+fTPLi1WtA27q2VA4HV79JZ+ZM5bOLsdjuimSiAxi\nsWxZHA+UO+c2OufagQeAC7pv4Jzb7JxbDhw4Anw28Ixzrto5VwM8A8yPYa0DKxTy03/sWsGkhkXM\nnZDHg2Xb6OrSFd0iMjjFMizGAtu6va4Ilg3YvmZ2nZmVmVlZZWWCzcV0zCWQOQpe/RWfP2kCGyub\neGLFznhXJSLygQzqAW7n3F3OuVLnXGlRUVG8y9lfUoofu9j4Ip8ogakjMrnl2XV0qnUhIoNQLMNi\nOzCu2+viYFms900cR34KcITXPM4/nzmNDZVNLHh78P0aIiKxDItFwFQzm2hmEeBSYEEf930KOMvM\n8oKB7bOCZYNL0TQYeRSsfIz5R47iiFFZ/PLZ9WpdiMigE7OwcM5FgevxX/KrgYeccyvN7GYzOx/A\nzI4zswrgM8CdZrYy2Lca+A984CwCbg6WDT5HXgjb3iDUsJ2vnzGVzVXNPPmOxi5EZHCxoXLPhdLS\nUldWVhbvMt5vTzn8z1w4+8d0nvAVzvz5S6SnhHn8+pMxs3hXJyLDnJktds6V9rbdoB7gHhQKp8Co\no+GdRwiHjOtOncQ72+t5pXxPvCsTEekzhcXhcOzlsH0xbH6Vi+aMZURWCre/uCHeVYmI9JnC4nCY\nezVkjoQXfkRKOMQXT5nIaxuqeHOj7qQnIoODwuJwiKTDKd+ELa/Cxhf53IkljMxO4adPrdV9ukVk\nUFBYHC5zr4bsYnj+h6QlGV87fRqLt9Tw7Ord8a5MRKRXCovDJSkFTrsRtpfB0t9xSWkxkwoz+NlT\na+jo1M2RRCSxKSwOp1lXQMkp8PRNJDXv5tvnHMG6XY3c9kJ5vCsTETkkhcXhZAaf/CVEW+GJb3L2\nzJFcNHsstz5fzrJttfGuTkTkoBQWh1vBZDjt32DNX+HNO/n3849kZFYK//zgMupaOuJdnYhIjxQW\n8fCRr8L08+CpfyNn56v84rOzqKhp5trfLqKlvTPe1YmIvI/CIh5CIfjUnVA4Fe77LCe89Hken72Y\npVur+PIfFhPVgLeIJBiFRbykZMEVD8Ocz0NrHUe881/cdmonL62r5M/LdsS7OhGR/Sgs4il3HJz7\nM7jmCQglcXbSUo4am82vnluv02lFJKEoLBJBag6UnIytfZJvnDmNrdXNPLK4It5ViYjso7BIFNPP\nhT1rOa2wgVnjcrn1+XJaOzTYLSKJQWGRKKbNB8DW/Y1/PXs622tb+OZDb9Olu+qJSAJQWCSKvAkw\n4khY+yQfmVLId86dwRMrdvKjhavjXZmIiMIioUw/B7a+Bg99ni8mPcEXTirm7lc2cd+bW+NdmYgM\ncwqLRDL3KjjiPNi5HHv6u3x35Ot8dFoR/75gpaYDEZG4Ulgkktzx8Nk/wFeXQskphF7+Kb+8aDIj\nslP4hz8spqqxLd4VisgwpbBIRGZw5s3QXEXu0tu548q5VDW2c8OjK3SzJBGJC4VFoho7B478FLx+\nG0dlNvKv86fzzKpdPFS2Ld6VicgwpLBIZKff5H8uuJ4vfKSEeVMK+MHjq1hRURffukRk2FFYJLL8\niXDWf8CG5wktvpv//sws8tIjfPqO1/jLsu3xrk5EhhGFRaIrvRYmnw5Pf49RjSv5y/XzOHZcLl97\nYBn/++qmeFcnIsOEwiLRmcEFt0FmEfz2ExTueIk/fvEEzpo5kpv/uoq/LtcMtSISewqLwSB7NFz7\nLBRMgfsvJXnlI/zqstnMGZ/HNx58m5fXVca7QhEZ4hQWg0XWSLhmIUz4CDx2HamrH+Xuq0qZVJTB\ntfcu4onlO+NdoYgMYQqLwSQlCy5/ECbMg8euI/elm3jo8kkcW5zL9fcv4Y9vbol3hSIyRCksBptI\nhg+M2VfCW3eRfVcp952wlY9NK+I7j73DbS+U68I9ERlwCovBKJIB598K1y+CsXOJPP4P/Gb2Ji6Y\nNYafPbWWHz6xWlObi8iAUlgMZgWT4YqHYMI8kv7yZW5Ju5ubj3yX/31lA//y8Nu6NauIDBgbKl0W\npaWlrqysLN5lxEd7Ezz5bVj5Z2hvYFtOKefv+iLRlHxKS/L44imTmDelMN5VikgCMrPFzrnS3rZT\ny2IoiGTABf8D3yqHT9zCuMYVvFbwQ740rZ51uxr5/D1v8eAi3RNDRD64mIaFmc03s7VmVm5mN/Sw\nPsXMHgzWv2lmJcHyEjNrMbNlweOOWNY5ZCSnQuk1cM2TpIW6uL78S7ww5xVOmZTNtx9ZwY8XrqY9\nqq4pEem/mIWFmYWB24BzgJnAZWY284DNrgVqnHNTgF8A/9lt3Qbn3Kzg8eVY1TkkFc+Fr7wGx1xC\n5LX/5h77D66dm8tdL2/k4ttfo3x3Q7wrFJFBJpYti+OBcufcRudcO/AAcMEB21wA3Bs8fxg43cws\nhjUNH2l5cNEdcPHdhHYs4XuV/8KDZzsaq3dy9i0vc+Ojy9lV3xrvKkVkkIhlWIwFut98oSJY1uM2\nzrkoUAcUBOsmmtlSM3vJzE7p6QPM7DozKzOzsspKTXnRo6M/DVc8DLXbOOGlK3jBfZG/jLybRxdv\n5axfvMzf3nk33hWKyCCQqAPcO4HxzrnZwDeA+8ws+8CNnHN3OedKnXOlRUVFh73IQWPSR+GfFsNl\nD8JJ13NUzXO8ddzLTChI58t/WMw3HlzG1qrmeFcpIgksKYbvvR0Y1+11cbCsp20qzCwJyAGqnD+f\ntw3AObfYzDYA04Bhem7sAMgaCdPn+0dnBzlv3clj8zK4teQ8fv3GTv7y9g4unjOW60+byviC9HhX\nKyIJJpZhsQiYamYT8aFwKXD5AdssAK4CXgc+DTzvnHNmVgRUO+c6zWwSMBXYGMNah5f5/xdaqgm/\n+nO+nnI3X55xKquqHc+/ncU1S45jzuxS/vG0KZQUZsS7UhFJEDG9KM/MzgVuAcLAPc65H5nZzUCZ\nc26BmaUCvwdmA9XApc65jWZ2MXAz0AF0Ad93zj1+qM8a1hflfVA7lsJrt8LO5dDWAI1+/OKNriP5\np47rOXLaFGaMzmbu+Dw+fsQIQiGdeyAy1PT1ojxdwS3vqauAdx7FvfBj6kI5/GvyjbxQO4KOTseR\nY7L59vwjOHWaxoZEhhJdwS39l1MM876KXbOQ3BTjrpZ/Yc1HXuC288dQ39rB5+95i3+6fymVDW3x\nrlREDjO1LKRnjbvhhR/Dkt+B68QVTmdF0lH8oGIOq2wyZ84cxYWzx/DRaSMIq3tKZNBSN5QMjD3l\nsPIxqFgEm/8OHc3siRSzqn3BvcRcAAATNklEQVQEy6PjeCLzYi486Sg+e9w4ctMj8a5WRPpJYSED\nr7UOVjwMG56nq2Yz7F5No2Xys7aLWGnTOPrIozjt2KmcOHUkqcnheFcrIn2gsJDYe3cF/PUbUPHW\nfosbXBqVyWNpyplGxqlfYdKxPV6ALyIJQGEhh0dXF+xeBTWb6KjeSsW771L57g6S6zYxqW0NOdbE\n65GTWHXEVymZMZd5uTWkrvgjzL3a37xJROJKYSFxV1tTxcbHf8oRm+4ltauVt9wRlNpakqyLlpQC\nai56gDHT5kLNZsidAOFYXiMqIj1RWEjiaK6m4+Wf07X8TyxJm8fdtcdyc/QWsmkGC5FJMy0FR5J6\n0a1Y8dx4VysyrCgsJGE559i8YS088102NCTzVn0e14afoNDq2Zk2lWj+NIrSIL31XSxzJIw7Hgqn\nQdYoKJgKKZnx/hVEhgyFhQwa1U3tvLS8nKQ3b6OwdjkTXAUtLoWqcAEl4WpGRHfs29ZZGBszG2Z8\n0o97pOb4bizXBfmToLMd1i6EcASmnwu6PYrIISksZFDq6nJsqGxk0eYayrZUU7a5hobqdxlnlYy2\naualb+VjkdWMb15FRzgdl5pLpCkIk6wxEG2Blhr/+tjL4bz/hohm0RU5GIWFDBm7G1pZs7OBdbsa\neHNTNa9vqGJ8ezlXh58i3VrZmDGbySOymNG2nOz0FArmXY1VLIIXfwKZI6G4FEbM9NOZ5E+EsaXv\nBUhLLbxxu7/g8Kwfwtg58f1lRQ4zhYUMWdHOLmpbOmhojfLahj38eel2Vmyvo7WjC4CSgnTOnDmS\nWR1LmVW5gBFN60iq3YQR/L8eSvZjIBaC2i3QVu+7s9qb4ZRvQijspzsZdwJMPMVv7zoho8h3azkH\n7Y2QkhXHoyAyMBQWMqw456hp7uDFtbt54K1tLNtWS3tn1771RWnG3II2jkvfxWy3muLoFlKTk0jN\nKSLykX/wrY7Hvgzrn/I7JGdAR9P+HxLJhLyJUL8dWqph5NEw6zIonB4MupsfO0lK8UGSPVZdYJLw\nFBYyrDnnaGrvZPOeJpZuq2XVjjo2VDaxsbKJPY37z5p7THEOp0wtZMaoLKanVJNdOIbszCxS96zA\nKt7yLRAMqsqheiNkj/GPdX/z9wQ5KIPc8b6FMuUM32LZtdIHS/HxkDvOb9PRAs1Vvivs7Qcg2gqz\nP+eDKG/ie62ZlhqoC25rP+oYDd7LgFBYiBxEfWsHW/Y0s6OuhbXvNvDSukqWbq2h64A/hUg4RH5G\nhFnjciktySM/I0J6JIkjx2RTnJeGmfkzsRp3+5tH4XywRNv865rNPhw2vwLNe/ybhpKgK3qQygwm\nnurP5Cp/1r9fWj5kjvD3GmlvfG/ToiNgxvnQ2QZdnf704qwx8Pb9/vMmfRSmnwNNe6BhJ0z62P4B\n0xmFytXQXO3DaWwpZBT070C2NfjfN6I7Kg5mCguRfmjt6GRDZSOb9zRT29JOXUsHdS0d7K5vY9Hm\naipqWvbbflR2KuPy0yjKSqEoM4WirBTG5adTUpBBSWEGOWnJ723c1QXvLvcti8Lp/ku/ogyaKv36\npBRIL4Ci6b7FAlC9CTY8BzvfhqYq3wrJGed/ttTA4nthxxIIp/gAiLYG75UKxcfBtrd8kHRXOM2/\nv3O+RdRW/966vacaRzKhbqt/38wRfl17kz8xYMqZ/j03vewfO5aChaFkHkw9C6ae7adwMfN3X3zt\nVl/raf/mTxzo7PDhlZoNyelB3W3QuMuvy5+k1lIcKCxEBlBVYxuNbVFqmzt4u6KWpVtr2VnXQmVD\nG5UNbdS37t9ayM+IMCIrhezUZCYWZjB7fC7jC9LJSUve98hMSfKtkw8q2ua/5LuisH2Jb8lMPRPS\n8/0MwRWLILsY0vJgzV/99Set9X770cdAySn+QkeA1Y/Dij/5wfzcccEXeyVgkJzq33tviyiU5AOp\n5BQfUuufhso1fl1yhl/fVueDJynVd7GNmQ27V/tTmw+m6Ag46mIfiqk5/r3bG6F+pw8UnH/vtDxI\nL/S/Z0oWbHnNh9eImXDsZ31Lp3abD57UHH8M8kp8WLfUQHKafxxKW4MPNtflj3HW6CE7HY3CQuQw\nau3oZGt1M5v2NLF5TxObq5qoamyntqWDdbsaqG3ueN8+4ZCRk5ZMbloy2WnJjM5J5YhR2UwsymBE\nlm+tjMhK+fChMhBa6/wXcnIajDvx/VfR12zxoVG9yZ85llPsx13M4KWf+uAaMwcKp0Bbo2+tmEE4\nGTJG+OBb8af3zWC8T1q+7/Lq6vCBR7fvrVASjJ0Lu1ZBe0PP+4cjvkXV1eHfJ3+yD5K2ev9+bcF+\nablBUO7ef/9Qkj8bLpzs3ysc8e/T0eJ/j4Kp/neu3+7DceSRPiCT06G1Ftb+zY9JFUz1XYaRDN99\n6Dp9CHdFfddgW73fPnOkD7/0Av85rbVQv8N/fs4434IbdYz/rHV/8zXO+1q//7OCwkIkYTjn2FLl\nx0jqg+6t7o/aZv+zoqaFzVVNHPgnGUkKkZeeTEFGCmNy0yjOS2Nsbhojc1JJSw6TEQkzNi+NUTmp\nRMKh+AfLh9Fa5/9F31bvWyWRDP/FmZTy3jadUd9CaN7jr5MZOfO9U5/Ln/UtiJxx/ku8tc4H2Z61\ngPmutdY6P5bU0Qwp2b5bLCXbv3dLbfDlP9l/bijJb1e7zbduuqI+2DrbgzPfUv2yPev9l3nOWN/y\n2bl8/+DKnQBTTvcnSFQs9vuEwsEjyT8s/F4t9TugvmL/Y5Oc4cOus/39x23yx+Fzj32gQ66wEBmE\nmtujbK9pYXfQvbW7odW3UJo72NPYxvbaFipqWmhsO9gguQ+XgowIhZkpFGZGKMpKYXSOD5jcdN+K\nyU5NJic9mezUpMRouQw1XZ3+Gp6uTt8ayJ3Q//GY1jrfCnOdPgxTsn3rqGk3VK7195OJpPvxopzi\nD1yqwkJkiHLOUd8SZXdDK23RLupbO9he08Ku+lbao120RbvY09jOnsY29jT60KlsbHtfi2WvkOG7\nw9IjwU8/npIRSSI9JUxGJInc9GQKMiMUZKRQkOmDKC89QiQpdHh/eRlwfQ2LoTliIzKEmRk56b5l\n0Fft0S521bdS19JBfUsH9a0dwfPoe91hLR3UNrdT3dTO1upmmts6aWqL0tQefd9pxXslhw3nICUp\nREFmCvkZEQoyIuRlRMhLfy+A0iNhop2OUMgYkeW3AwiZ+WBKCZOZmkRKkm7Hm6gUFiLDQCQpxLj8\ndMZ9gH2dc9S3RqlqbKO6qZ09je1UNbVR1dhOa0cnZtDc3kl1kw+anXWtrNpZT11LB83tnf2rMxza\nFxwZkSSyUpPISPFdZXsfGSk9LO9h++SwqXttACksROSQzGzf6b6Tivq3b1u004dGWydJYSPa6ahs\nbKOmyQ/Sdnb5K+2b2qI0tkVpaI3ue97YFqWxNepbOlXN+5b1J4BCBulBiGSnJpOVGjxPSyY9kkRK\nUoiU5BCpSeF9P9MiYdIjYTJTkkiPJBFJCtHZ5UgK+1ZRQUYKKUkhQqHhFUQKCxGJmZSkMCOywtBt\nzsWSwg93xbcPmCBUWt8LlqYgbPY+b+90dHU5mts7qW/toKHVTz65p7GdjXuaaG7vpK2jk9ZoF+3R\nrt4/+ABJISM5HCKSFCI5HCIlKURy2Pa97r48I+JbP3tbQnuDJjMlTHbqeycdpEXCpCaHCJnR5Zw/\n29c5MlOSKIrzadQKCxEZVMIh81+wqcmQMzDv2dXlaO/sorWjk+b2vQ8fPO3RLpLDoX3jPlVN7XR0\n+oDZ+7O90+33uqOzi/bgeUNrlF31rTS2RmkIguxgY0B9EUkKkRL2LaJIOERKcpijxuZw62WzB+Zg\nHITCQkSGvVDISA2FSU0Ok3uYJgp27r1Wz94TDVo6Omlp78RfdGiEzHcDNrR2UNnQRlNblLYghNqi\n7/0cn9/LFekDQGEhIhIHZkZGMGA/eoBaSLGkk6RFRKRXCgsREemVwkJERHqlsBARkV7FNCzMbL6Z\nrTWzcjO7oYf1KWb2YLD+TTMr6bbuxmD5WjM7O5Z1iojIocUsLMwsDNwGnAPMBC4zs5kHbHYtUOOc\nmwL8AvjPYN+ZwKXAkcB84NfB+4mISBzEsmVxPFDunNvonGsHHgAuOGCbC4B7g+cPA6ebvzzxAuAB\n51ybc24TUB68n4iIxEEsw2IssK3b64pgWY/bOOeiQB1Q0Md9MbPrzKzMzMoqKysHsHQREeluUF+U\n55y7C7gLwMwqzWzLh3i7QmDPgBQWO4leY6LXB6pxoKjGgZEINU7oy0axDIvtsN+MyMXBsp62qTCz\nJPxML1V93Hc/zrl+zoe5PzMr68sNQOIp0WtM9PpANQ4U1TgwBkONe8WyG2oRMNXMJppZBD9gveCA\nbRYAVwXPPw087/yt+xYAlwZnS00EpgIHuZO7iIjEWsxaFs65qJldDzwFhIF7nHMrzexmoMw5twC4\nG/i9mZUD1fhAIdjuIWAVEAX+0TnXv7uoiIjIgInpmIVzbiGw8IBlN3V73gp85iD7/gj4USzrO8Bd\nh/GzPqhErzHR6wPVOFBU48AYDDUCYO5gd3EXEREJaLoPERHplcJCRER6NezDorf5q+LBzMaZ2Qtm\ntsrMVprZ14Ll+Wb2jJmtD37mJUCtYTNbamZ/DV5PDOb5Kg/m/YrEub5cM3vYzNaY2WozOymRjqOZ\n/XPw3/gdM7vfzFIT4Ria2T1mttvM3um2rMfjZt6vgnqXm9mcONX3s+C/83Ize8zMcrutO+xzzfVU\nY7d13zQzZ2aFwevDfgz7a1iHRR/nr4qHKPBN59xM4ETgH4O6bgCec85NBZ4LXsfb14DV3V7/J/CL\nYL6vGvz8X/H0S+BvzrkjgGPxtSbEcTSzscBXgVLn3FH4swYvJTGO4W/x87J1d7Djdg7+9PapwHXA\n7XGq7xngKOfcMcA64EaI61xzPdWImY0DzgK2dlscj2PYL8M6LOjb/FWHnXNup3NuSfC8Af8FN5b9\n59K6F7gwPhV6ZlYMnAf8v+C1AR/Hz/MFca7RzHKAU/GnaOOca3fO1ZJYxzEJSAsuSk0HdpIAx9A5\n9zL+dPbuDnbcLgB+57w3gFwzG32463POPR1MGwTwBv5i3r31Hfa55g5yDMFPmvqv+Btt73XYj2F/\nDfew6NMcVPFkftr22cCbwEjn3M5g1bvAyDiVtdct+P/pu4LXBUBttz/YeB/PiUAl8L9BV9n/M7MM\nEuQ4Oue2A/+F/xfmTvzcaItJrGPY3cGOWyL+HX0BeDJ4njD1mdkFwHbn3NsHrEqYGg9muIdFQjOz\nTOAR4OvOufru64Ir3eN23rOZfQLY7ZxbHK8a+iAJmAPc7pybDTRxQJdTPI9j0Od/AT7UxgAZ9NBt\nkYji/f/foZjZd/BduX+Mdy3dmVk68G/ATb1tm4iGe1j0ew6qw8XMkvFB8Ufn3KPB4l17m6bBz93x\nqg+YB5xvZpvx3Xcfx48P5AZdKhD/41kBVDjn3gxeP4wPj0Q5jmcAm5xzlc65DuBR/HFNpGPY3cGO\nW8L8HZnZ1cAngCvcexeRJUp9k/H/MHg7+LspBpaY2SgSp8aDGu5h0Zf5qw67oO//bmC1c+7n3VZ1\nn0vrKuAvh7u2vZxzNzrnip1zJfjj9rxz7grgBfw8XxD/Gt8FtpnZ9GDR6fgpZBLlOG4FTjSz9OC/\n+d76EuYYHuBgx20B8PngjJ4Tgbpu3VWHjZnNx3eLnu+ca+62KiHmmnPOrXDOjXDOlQR/NxXAnOD/\n04Q4hofknBvWD+Bc/JkTG4DvxLueoKaT8U385cCy4HEufkzgOWA98CyQH+9ag3o/Bvw1eD4J/4dY\nDvwJSIlzbbOAsuBY/hnIS6TjCPwAWAO8A/weSEmEYwjcjx9H6cB/qV17sOMGGP6swg3ACvzZXfGo\nrxzf77/3b+aObtt/J6hvLXBOvI7hAes3A4XxOob9fWi6DxER6dVw74YSEZE+UFiIiEivFBYiItIr\nhYWIiPRKYSEiIr1SWIgkADP7mAUz94okIoWFiIj0SmEh0g9mdqWZvWVmy8zsTvP382g0s18E96V4\nzsyKgm1nmdkb3e6vsPf+D1PM7Fkze9vMlpjZ5ODtM+29e2/8MbiqWyQhKCxE+sjMZgCfBeY552YB\nncAV+AkAy5xzRwIvAd8Pdvkd8G3n76+wotvyPwK3OeeOBT6Cv8oX/OzCX8ffW2USfp4okYSQ1Psm\nIhI4HZgLLAr+0Z+Gn0yvC3gw2OYPwKPBvTRynXMvBcvvBf5kZlnAWOfcYwDOuVaA4P3ecs5VBK+X\nASXAK7H/tUR6p7AQ6TsD7nXO3bjfQrPvHbDdB51Dp63b80709ykJRN1QIn33HPBpMxsB++5JPQH/\nd7R3ltjLgVecc3VAjZmdEiz/HPCS83c+rDCzC4P3SAnucyCS0PQvF5E+cs6tMrPvAk+bWQg/m+g/\n4m+qdHywbjd+XAP8NN53BGGwEbgmWP454E4zuzl4j88cxl9D5APRrLMiH5KZNTrnMuNdh0gsqRtK\nRER6pZaFiIj0Si0LERHplcJCRER6pbAQEZFeKSxERKRXCgsREenV/wevphPr8iM8SgAAAABJRU5E\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP8a2jGSONR-",
        "colab_type": "text"
      },
      "source": [
        "**We see we get something quite nice now, where the learning curve bottoms out around 100 epochs of testing data. This took plenty of playing around with the parameters, choosing a good optimizer with a decent learning rate, as well as a good loss function. We changed the 1 and 0 data to a categorical value using sigmoid as our output.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTaci-JhRkDt",
        "colab_type": "text"
      },
      "source": [
        "Perform KFold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z55Mhct5RnuK",
        "colab_type": "code",
        "outputId": "b7db337e-9e8f-4584-ae72-48d6285bd09d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# this wraps the network to put into sklearn,\n",
        "# so we can use cross_val_score method for kfold\n",
        "# cross validations\n",
        "neural_network = KerasClassifier(build_fn=create_model1, \n",
        "                                 epochs=150, \n",
        "                                 batch_size=100)\n",
        "\n",
        "cross_score = cross_val_score(neural_network, features_ntc, labels_ntc, cv=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.2462 - acc: 0.4975\n",
            "Epoch 2/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.2378 - acc: 0.5000\n",
            "Epoch 3/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.2300 - acc: 0.5000\n",
            "Epoch 4/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.2235 - acc: 0.5000\n",
            "Epoch 5/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.2180 - acc: 0.5000\n",
            "Epoch 6/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.2128 - acc: 0.5087\n",
            "Epoch 7/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.2085 - acc: 0.5150\n",
            "Epoch 8/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.2041 - acc: 0.5225\n",
            "Epoch 9/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.2005 - acc: 0.5687\n",
            "Epoch 10/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.1971 - acc: 0.6000\n",
            "Epoch 11/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.1935 - acc: 0.6213\n",
            "Epoch 12/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.1903 - acc: 0.6825\n",
            "Epoch 13/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.1874 - acc: 0.7038\n",
            "Epoch 14/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.1839 - acc: 0.7312\n",
            "Epoch 15/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.1810 - acc: 0.7488\n",
            "Epoch 16/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.1784 - acc: 0.7800\n",
            "Epoch 17/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.1748 - acc: 0.7987\n",
            "Epoch 18/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.1718 - acc: 0.8063\n",
            "Epoch 19/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.1687 - acc: 0.8488\n",
            "Epoch 20/150\n",
            "800/800 [==============================] - 0s 155us/step - loss: 0.1663 - acc: 0.8450\n",
            "Epoch 21/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.1639 - acc: 0.8575\n",
            "Epoch 22/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.1610 - acc: 0.8700\n",
            "Epoch 23/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.1592 - acc: 0.8637\n",
            "Epoch 24/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.1567 - acc: 0.8800\n",
            "Epoch 25/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.1547 - acc: 0.8937\n",
            "Epoch 26/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.1527 - acc: 0.8913\n",
            "Epoch 27/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.1505 - acc: 0.8950\n",
            "Epoch 28/150\n",
            "800/800 [==============================] - 0s 128us/step - loss: 0.1487 - acc: 0.9013\n",
            "Epoch 29/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.1470 - acc: 0.9113\n",
            "Epoch 30/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.1452 - acc: 0.9113\n",
            "Epoch 31/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.1437 - acc: 0.9138\n",
            "Epoch 32/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.1417 - acc: 0.9213\n",
            "Epoch 33/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.1402 - acc: 0.9225\n",
            "Epoch 34/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.1389 - acc: 0.9275\n",
            "Epoch 35/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.1371 - acc: 0.9200\n",
            "Epoch 36/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.1358 - acc: 0.9288\n",
            "Epoch 37/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.1343 - acc: 0.9337\n",
            "Epoch 38/150\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.1329 - acc: 0.9300\n",
            "Epoch 39/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.1315 - acc: 0.9362\n",
            "Epoch 40/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.1304 - acc: 0.9387\n",
            "Epoch 41/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.1292 - acc: 0.9425\n",
            "Epoch 42/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.1277 - acc: 0.9463\n",
            "Epoch 43/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.1264 - acc: 0.9488\n",
            "Epoch 44/150\n",
            "800/800 [==============================] - 0s 127us/step - loss: 0.1252 - acc: 0.9475\n",
            "Epoch 45/150\n",
            "800/800 [==============================] - 0s 126us/step - loss: 0.1243 - acc: 0.9438\n",
            "Epoch 46/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.1229 - acc: 0.9487\n",
            "Epoch 47/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.1216 - acc: 0.9550\n",
            "Epoch 48/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.1207 - acc: 0.9537\n",
            "Epoch 49/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.1196 - acc: 0.9537\n",
            "Epoch 50/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.1185 - acc: 0.9563\n",
            "Epoch 51/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.1172 - acc: 0.9588\n",
            "Epoch 52/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.1164 - acc: 0.9575\n",
            "Epoch 53/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.1152 - acc: 0.9612\n",
            "Epoch 54/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.1142 - acc: 0.9625\n",
            "Epoch 55/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.1131 - acc: 0.9612\n",
            "Epoch 56/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.1121 - acc: 0.9637\n",
            "Epoch 57/150\n",
            "800/800 [==============================] - 0s 155us/step - loss: 0.1112 - acc: 0.9600\n",
            "Epoch 58/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.1105 - acc: 0.9650\n",
            "Epoch 59/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.1093 - acc: 0.9650\n",
            "Epoch 60/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.1084 - acc: 0.9650\n",
            "Epoch 61/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.1075 - acc: 0.9638\n",
            "Epoch 62/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.1066 - acc: 0.9663\n",
            "Epoch 63/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.1057 - acc: 0.9688\n",
            "Epoch 64/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.1047 - acc: 0.9663\n",
            "Epoch 65/150\n",
            "800/800 [==============================] - 0s 126us/step - loss: 0.1038 - acc: 0.9675\n",
            "Epoch 66/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.1029 - acc: 0.9688\n",
            "Epoch 67/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.1021 - acc: 0.9700\n",
            "Epoch 68/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.1013 - acc: 0.9675\n",
            "Epoch 69/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.1005 - acc: 0.9713\n",
            "Epoch 70/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0997 - acc: 0.9687\n",
            "Epoch 71/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0989 - acc: 0.9713\n",
            "Epoch 72/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0980 - acc: 0.9713\n",
            "Epoch 73/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0972 - acc: 0.9725\n",
            "Epoch 74/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0965 - acc: 0.9738\n",
            "Epoch 75/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0957 - acc: 0.9738\n",
            "Epoch 76/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0950 - acc: 0.9750\n",
            "Epoch 77/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0943 - acc: 0.9750\n",
            "Epoch 78/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0936 - acc: 0.9763\n",
            "Epoch 79/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0929 - acc: 0.9750\n",
            "Epoch 80/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0922 - acc: 0.9738\n",
            "Epoch 81/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0913 - acc: 0.9750\n",
            "Epoch 82/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0906 - acc: 0.9775\n",
            "Epoch 83/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0900 - acc: 0.9775\n",
            "Epoch 84/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0892 - acc: 0.9788\n",
            "Epoch 85/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0886 - acc: 0.9775\n",
            "Epoch 86/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0880 - acc: 0.9813\n",
            "Epoch 87/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0872 - acc: 0.9825\n",
            "Epoch 88/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0866 - acc: 0.9813\n",
            "Epoch 89/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0859 - acc: 0.9813\n",
            "Epoch 90/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0854 - acc: 0.9813\n",
            "Epoch 91/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0845 - acc: 0.9813\n",
            "Epoch 92/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0840 - acc: 0.9800\n",
            "Epoch 93/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0832 - acc: 0.9825\n",
            "Epoch 94/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0828 - acc: 0.9813\n",
            "Epoch 95/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0821 - acc: 0.9838\n",
            "Epoch 96/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0815 - acc: 0.9825\n",
            "Epoch 97/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0808 - acc: 0.9825\n",
            "Epoch 98/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0803 - acc: 0.9813\n",
            "Epoch 99/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0797 - acc: 0.9825\n",
            "Epoch 100/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0791 - acc: 0.9838\n",
            "Epoch 101/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0785 - acc: 0.9825\n",
            "Epoch 102/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0780 - acc: 0.9838\n",
            "Epoch 103/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0775 - acc: 0.9838\n",
            "Epoch 104/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0768 - acc: 0.9863\n",
            "Epoch 105/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0762 - acc: 0.9850\n",
            "Epoch 106/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0757 - acc: 0.9863\n",
            "Epoch 107/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0752 - acc: 0.9863\n",
            "Epoch 108/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0749 - acc: 0.9863\n",
            "Epoch 109/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0741 - acc: 0.9850\n",
            "Epoch 110/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0736 - acc: 0.9863\n",
            "Epoch 111/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0730 - acc: 0.9863\n",
            "Epoch 112/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0726 - acc: 0.9863\n",
            "Epoch 113/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0721 - acc: 0.9863\n",
            "Epoch 114/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0716 - acc: 0.9863\n",
            "Epoch 115/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0712 - acc: 0.9875\n",
            "Epoch 116/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0706 - acc: 0.9863\n",
            "Epoch 117/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0701 - acc: 0.9875\n",
            "Epoch 118/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0696 - acc: 0.9863\n",
            "Epoch 119/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0692 - acc: 0.9863\n",
            "Epoch 120/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.0690 - acc: 0.9875\n",
            "Epoch 121/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0683 - acc: 0.9875\n",
            "Epoch 122/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0681 - acc: 0.9850\n",
            "Epoch 123/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0675 - acc: 0.9863\n",
            "Epoch 124/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0671 - acc: 0.9875\n",
            "Epoch 125/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0665 - acc: 0.9875\n",
            "Epoch 126/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0661 - acc: 0.9875\n",
            "Epoch 127/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0656 - acc: 0.9875\n",
            "Epoch 128/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0651 - acc: 0.9875\n",
            "Epoch 129/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.0648 - acc: 0.9875\n",
            "Epoch 130/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0644 - acc: 0.9875\n",
            "Epoch 131/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0640 - acc: 0.9875\n",
            "Epoch 132/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0635 - acc: 0.9875\n",
            "Epoch 133/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0632 - acc: 0.9888\n",
            "Epoch 134/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0628 - acc: 0.9875\n",
            "Epoch 135/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0623 - acc: 0.9875\n",
            "Epoch 136/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0619 - acc: 0.9875\n",
            "Epoch 137/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0616 - acc: 0.9875\n",
            "Epoch 138/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.0612 - acc: 0.9875\n",
            "Epoch 139/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0609 - acc: 0.9875\n",
            "Epoch 140/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0605 - acc: 0.9875\n",
            "Epoch 141/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0601 - acc: 0.9875\n",
            "Epoch 142/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0597 - acc: 0.9888\n",
            "Epoch 143/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0593 - acc: 0.9888\n",
            "Epoch 144/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0589 - acc: 0.9888\n",
            "Epoch 145/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0587 - acc: 0.9900\n",
            "Epoch 146/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0583 - acc: 0.9888\n",
            "Epoch 147/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0579 - acc: 0.9888\n",
            "Epoch 148/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0576 - acc: 0.9888\n",
            "Epoch 149/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0572 - acc: 0.9888\n",
            "Epoch 150/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0569 - acc: 0.9888\n",
            "200/200 [==============================] - 1s 5ms/step\n",
            "Epoch 1/150\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.2371 - acc: 0.5200\n",
            "Epoch 2/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.2144 - acc: 0.5600\n",
            "Epoch 3/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.1982 - acc: 0.6300\n",
            "Epoch 4/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.1869 - acc: 0.6887\n",
            "Epoch 5/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.1802 - acc: 0.7425\n",
            "Epoch 6/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.1730 - acc: 0.7600\n",
            "Epoch 7/150\n",
            "800/800 [==============================] - 0s 150us/step - loss: 0.1664 - acc: 0.8050\n",
            "Epoch 8/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.1621 - acc: 0.8300\n",
            "Epoch 9/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.1590 - acc: 0.8425\n",
            "Epoch 10/150\n",
            "800/800 [==============================] - 0s 152us/step - loss: 0.1543 - acc: 0.8613\n",
            "Epoch 11/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.1506 - acc: 0.8788\n",
            "Epoch 12/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.1480 - acc: 0.8938\n",
            "Epoch 13/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.1450 - acc: 0.9075\n",
            "Epoch 14/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.1429 - acc: 0.9162\n",
            "Epoch 15/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.1409 - acc: 0.9163\n",
            "Epoch 16/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.1401 - acc: 0.9263\n",
            "Epoch 17/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.1370 - acc: 0.9275\n",
            "Epoch 18/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.1356 - acc: 0.9288\n",
            "Epoch 19/150\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.1345 - acc: 0.9313\n",
            "Epoch 20/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.1325 - acc: 0.9350\n",
            "Epoch 21/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.1305 - acc: 0.9487\n",
            "Epoch 22/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.1299 - acc: 0.9400\n",
            "Epoch 23/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.1286 - acc: 0.9463\n",
            "Epoch 24/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.1265 - acc: 0.9513\n",
            "Epoch 25/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.1248 - acc: 0.9550\n",
            "Epoch 26/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.1237 - acc: 0.9537\n",
            "Epoch 27/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.1224 - acc: 0.9563\n",
            "Epoch 28/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.1215 - acc: 0.9563\n",
            "Epoch 29/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.1200 - acc: 0.9650\n",
            "Epoch 30/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.1190 - acc: 0.9575\n",
            "Epoch 31/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.1174 - acc: 0.9600\n",
            "Epoch 32/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.1166 - acc: 0.9600\n",
            "Epoch 33/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.1155 - acc: 0.9675\n",
            "Epoch 34/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.1142 - acc: 0.9663\n",
            "Epoch 35/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.1131 - acc: 0.9650\n",
            "Epoch 36/150\n",
            "800/800 [==============================] - 0s 152us/step - loss: 0.1119 - acc: 0.9700\n",
            "Epoch 37/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.1111 - acc: 0.9713\n",
            "Epoch 38/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.1098 - acc: 0.9713\n",
            "Epoch 39/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.1086 - acc: 0.9750\n",
            "Epoch 40/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.1078 - acc: 0.9763\n",
            "Epoch 41/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.1068 - acc: 0.9750\n",
            "Epoch 42/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.1056 - acc: 0.9763\n",
            "Epoch 43/150\n",
            "800/800 [==============================] - 0s 150us/step - loss: 0.1049 - acc: 0.9775\n",
            "Epoch 44/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.1038 - acc: 0.9788\n",
            "Epoch 45/150\n",
            "800/800 [==============================] - 0s 155us/step - loss: 0.1027 - acc: 0.9800\n",
            "Epoch 46/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.1017 - acc: 0.9813\n",
            "Epoch 47/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.1011 - acc: 0.9825\n",
            "Epoch 48/150\n",
            "800/800 [==============================] - 0s 153us/step - loss: 0.1000 - acc: 0.9825\n",
            "Epoch 49/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0988 - acc: 0.9838\n",
            "Epoch 50/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0980 - acc: 0.9813\n",
            "Epoch 51/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0969 - acc: 0.9838\n",
            "Epoch 52/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0960 - acc: 0.9813\n",
            "Epoch 53/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0951 - acc: 0.9825\n",
            "Epoch 54/150\n",
            "800/800 [==============================] - 0s 155us/step - loss: 0.0942 - acc: 0.9838\n",
            "Epoch 55/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0932 - acc: 0.9825\n",
            "Epoch 56/150\n",
            "800/800 [==============================] - 0s 154us/step - loss: 0.0920 - acc: 0.9850\n",
            "Epoch 57/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0912 - acc: 0.9813\n",
            "Epoch 58/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0900 - acc: 0.9838\n",
            "Epoch 59/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0890 - acc: 0.9863\n",
            "Epoch 60/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0880 - acc: 0.9850\n",
            "Epoch 61/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0871 - acc: 0.9850\n",
            "Epoch 62/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0860 - acc: 0.9850\n",
            "Epoch 63/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.0851 - acc: 0.9850\n",
            "Epoch 64/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0839 - acc: 0.9850\n",
            "Epoch 65/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0829 - acc: 0.9838\n",
            "Epoch 66/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0815 - acc: 0.9850\n",
            "Epoch 67/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0806 - acc: 0.9850\n",
            "Epoch 68/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0792 - acc: 0.9863\n",
            "Epoch 69/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0783 - acc: 0.9850\n",
            "Epoch 70/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0766 - acc: 0.9863\n",
            "Epoch 71/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0755 - acc: 0.9850\n",
            "Epoch 72/150\n",
            "800/800 [==============================] - 0s 150us/step - loss: 0.0740 - acc: 0.9850\n",
            "Epoch 73/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0729 - acc: 0.9850\n",
            "Epoch 74/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0711 - acc: 0.9875\n",
            "Epoch 75/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0699 - acc: 0.9850\n",
            "Epoch 76/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0685 - acc: 0.9888\n",
            "Epoch 77/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0666 - acc: 0.9875\n",
            "Epoch 78/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0652 - acc: 0.9875\n",
            "Epoch 79/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0638 - acc: 0.9888\n",
            "Epoch 80/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0619 - acc: 0.9888\n",
            "Epoch 81/150\n",
            "800/800 [==============================] - 0s 155us/step - loss: 0.0604 - acc: 0.9875\n",
            "Epoch 82/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0584 - acc: 0.9900\n",
            "Epoch 83/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0567 - acc: 0.9875\n",
            "Epoch 84/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0551 - acc: 0.9888\n",
            "Epoch 85/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0535 - acc: 0.9900\n",
            "Epoch 86/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0516 - acc: 0.9888\n",
            "Epoch 87/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0501 - acc: 0.9888\n",
            "Epoch 88/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0482 - acc: 0.9900\n",
            "Epoch 89/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0467 - acc: 0.9888\n",
            "Epoch 90/150\n",
            "800/800 [==============================] - 0s 154us/step - loss: 0.0453 - acc: 0.9900\n",
            "Epoch 91/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0437 - acc: 0.9900\n",
            "Epoch 92/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0424 - acc: 0.9900\n",
            "Epoch 93/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0406 - acc: 0.9900\n",
            "Epoch 94/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.0393 - acc: 0.9900\n",
            "Epoch 95/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0381 - acc: 0.9900\n",
            "Epoch 96/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0367 - acc: 0.9900\n",
            "Epoch 97/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0355 - acc: 0.9900\n",
            "Epoch 98/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0344 - acc: 0.9900\n",
            "Epoch 99/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.0334 - acc: 0.9900\n",
            "Epoch 100/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0324 - acc: 0.9925\n",
            "Epoch 101/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0317 - acc: 0.9913\n",
            "Epoch 102/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0306 - acc: 0.9913\n",
            "Epoch 103/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0298 - acc: 0.9900\n",
            "Epoch 104/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0289 - acc: 0.9913\n",
            "Epoch 105/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0281 - acc: 0.9925\n",
            "Epoch 106/150\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0274 - acc: 0.9925\n",
            "Epoch 107/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0268 - acc: 0.9925\n",
            "Epoch 108/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0261 - acc: 0.9925\n",
            "Epoch 109/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0256 - acc: 0.9925\n",
            "Epoch 110/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0250 - acc: 0.9925\n",
            "Epoch 111/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0244 - acc: 0.9925\n",
            "Epoch 112/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0239 - acc: 0.9925\n",
            "Epoch 113/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0233 - acc: 0.9925\n",
            "Epoch 114/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0229 - acc: 0.9925\n",
            "Epoch 115/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0225 - acc: 0.9925\n",
            "Epoch 116/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0220 - acc: 0.9925\n",
            "Epoch 117/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0218 - acc: 0.9925\n",
            "Epoch 118/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0213 - acc: 0.9925\n",
            "Epoch 119/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0208 - acc: 0.9925\n",
            "Epoch 120/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0204 - acc: 0.9925\n",
            "Epoch 121/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0202 - acc: 0.9925\n",
            "Epoch 122/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0198 - acc: 0.9925\n",
            "Epoch 123/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0197 - acc: 0.9925\n",
            "Epoch 124/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0192 - acc: 0.9925\n",
            "Epoch 125/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0189 - acc: 0.9925\n",
            "Epoch 126/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0186 - acc: 0.9925\n",
            "Epoch 127/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0185 - acc: 0.9925\n",
            "Epoch 128/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0180 - acc: 0.9925\n",
            "Epoch 129/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0177 - acc: 0.9925\n",
            "Epoch 130/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0175 - acc: 0.9925\n",
            "Epoch 131/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0172 - acc: 0.9925\n",
            "Epoch 132/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0171 - acc: 0.9925\n",
            "Epoch 133/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0169 - acc: 0.9925\n",
            "Epoch 134/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0167 - acc: 0.9925\n",
            "Epoch 135/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0165 - acc: 0.9925\n",
            "Epoch 136/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0162 - acc: 0.9925\n",
            "Epoch 137/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0160 - acc: 0.9925\n",
            "Epoch 138/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0158 - acc: 0.9925\n",
            "Epoch 139/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0156 - acc: 0.9925\n",
            "Epoch 140/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0155 - acc: 0.9925\n",
            "Epoch 141/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0152 - acc: 0.9925\n",
            "Epoch 142/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0150 - acc: 0.9925\n",
            "Epoch 143/150\n",
            "800/800 [==============================] - 0s 153us/step - loss: 0.0149 - acc: 0.9925\n",
            "Epoch 144/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0147 - acc: 0.9925\n",
            "Epoch 145/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0145 - acc: 0.9925\n",
            "Epoch 146/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0144 - acc: 0.9925\n",
            "Epoch 147/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0144 - acc: 0.9925\n",
            "Epoch 148/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0141 - acc: 0.9925\n",
            "Epoch 149/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0139 - acc: 0.9925\n",
            "Epoch 150/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0138 - acc: 0.9925\n",
            "200/200 [==============================] - 1s 5ms/step\n",
            "Epoch 1/150\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.2508 - acc: 0.4975\n",
            "Epoch 2/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.2413 - acc: 0.5050\n",
            "Epoch 3/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.2310 - acc: 0.5237\n",
            "Epoch 4/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.2228 - acc: 0.6200\n",
            "Epoch 5/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.2079 - acc: 0.7600\n",
            "Epoch 6/150\n",
            "800/800 [==============================] - 0s 128us/step - loss: 0.1807 - acc: 0.8738\n",
            "Epoch 7/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.1643 - acc: 0.8950\n",
            "Epoch 8/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.1510 - acc: 0.9113\n",
            "Epoch 9/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.1387 - acc: 0.9275\n",
            "Epoch 10/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.1272 - acc: 0.9313\n",
            "Epoch 11/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.1175 - acc: 0.9387\n",
            "Epoch 12/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.1086 - acc: 0.9475\n",
            "Epoch 13/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.1015 - acc: 0.9538\n",
            "Epoch 14/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0947 - acc: 0.9600\n",
            "Epoch 15/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0899 - acc: 0.9562\n",
            "Epoch 16/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0842 - acc: 0.9638\n",
            "Epoch 17/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0803 - acc: 0.9638\n",
            "Epoch 18/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0751 - acc: 0.9688\n",
            "Epoch 19/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0713 - acc: 0.9688\n",
            "Epoch 20/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0679 - acc: 0.9700\n",
            "Epoch 21/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0649 - acc: 0.9700\n",
            "Epoch 22/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0618 - acc: 0.9700\n",
            "Epoch 23/150\n",
            "800/800 [==============================] - 0s 150us/step - loss: 0.0591 - acc: 0.9713\n",
            "Epoch 24/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0567 - acc: 0.9750\n",
            "Epoch 25/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.0546 - acc: 0.9763\n",
            "Epoch 26/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0523 - acc: 0.9763\n",
            "Epoch 27/150\n",
            "800/800 [==============================] - 0s 128us/step - loss: 0.0504 - acc: 0.9763\n",
            "Epoch 28/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0487 - acc: 0.9775\n",
            "Epoch 29/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0471 - acc: 0.9775\n",
            "Epoch 30/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0455 - acc: 0.9788\n",
            "Epoch 31/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0441 - acc: 0.9800\n",
            "Epoch 32/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0427 - acc: 0.9800\n",
            "Epoch 33/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0413 - acc: 0.9813\n",
            "Epoch 34/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0403 - acc: 0.9813\n",
            "Epoch 35/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0391 - acc: 0.9813\n",
            "Epoch 36/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0379 - acc: 0.9813\n",
            "Epoch 37/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0371 - acc: 0.9825\n",
            "Epoch 38/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0362 - acc: 0.9825\n",
            "Epoch 39/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.0350 - acc: 0.9825\n",
            "Epoch 40/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0344 - acc: 0.9838\n",
            "Epoch 41/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0333 - acc: 0.9825\n",
            "Epoch 42/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0327 - acc: 0.9863\n",
            "Epoch 43/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0319 - acc: 0.9838\n",
            "Epoch 44/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0312 - acc: 0.9850\n",
            "Epoch 45/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0304 - acc: 0.9850\n",
            "Epoch 46/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0298 - acc: 0.9850\n",
            "Epoch 47/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0294 - acc: 0.9838\n",
            "Epoch 48/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0288 - acc: 0.9863\n",
            "Epoch 49/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0286 - acc: 0.9850\n",
            "Epoch 50/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0276 - acc: 0.9863\n",
            "Epoch 51/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0270 - acc: 0.9863\n",
            "Epoch 52/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.0269 - acc: 0.9863\n",
            "Epoch 53/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0262 - acc: 0.9875\n",
            "Epoch 54/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0257 - acc: 0.9875\n",
            "Epoch 55/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0252 - acc: 0.9875\n",
            "Epoch 56/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0249 - acc: 0.9875\n",
            "Epoch 57/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0246 - acc: 0.9863\n",
            "Epoch 58/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0241 - acc: 0.9875\n",
            "Epoch 59/150\n",
            "800/800 [==============================] - 0s 127us/step - loss: 0.0237 - acc: 0.9875\n",
            "Epoch 60/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0233 - acc: 0.9875\n",
            "Epoch 61/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0229 - acc: 0.9875\n",
            "Epoch 62/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0227 - acc: 0.9875\n",
            "Epoch 63/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0226 - acc: 0.9875\n",
            "Epoch 64/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0219 - acc: 0.9875\n",
            "Epoch 65/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0219 - acc: 0.9875\n",
            "Epoch 66/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0215 - acc: 0.9875\n",
            "Epoch 67/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0211 - acc: 0.9875\n",
            "Epoch 68/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0211 - acc: 0.9875\n",
            "Epoch 69/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0207 - acc: 0.9875\n",
            "Epoch 70/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0205 - acc: 0.9875\n",
            "Epoch 71/150\n",
            "800/800 [==============================] - 0s 127us/step - loss: 0.0201 - acc: 0.9875\n",
            "Epoch 72/150\n",
            "800/800 [==============================] - 0s 128us/step - loss: 0.0199 - acc: 0.9875\n",
            "Epoch 73/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0196 - acc: 0.9875\n",
            "Epoch 74/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0193 - acc: 0.9888\n",
            "Epoch 75/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0192 - acc: 0.9875\n",
            "Epoch 76/150\n",
            "800/800 [==============================] - 0s 128us/step - loss: 0.0190 - acc: 0.9875\n",
            "Epoch 77/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0187 - acc: 0.9875\n",
            "Epoch 78/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0184 - acc: 0.9875\n",
            "Epoch 79/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0183 - acc: 0.9888\n",
            "Epoch 80/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0182 - acc: 0.9888\n",
            "Epoch 81/150\n",
            "800/800 [==============================] - 0s 123us/step - loss: 0.0180 - acc: 0.9875\n",
            "Epoch 82/150\n",
            "800/800 [==============================] - 0s 127us/step - loss: 0.0178 - acc: 0.9875\n",
            "Epoch 83/150\n",
            "800/800 [==============================] - 0s 127us/step - loss: 0.0176 - acc: 0.9888\n",
            "Epoch 84/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0174 - acc: 0.9875\n",
            "Epoch 85/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0172 - acc: 0.9888\n",
            "Epoch 86/150\n",
            "800/800 [==============================] - 0s 128us/step - loss: 0.0171 - acc: 0.9888\n",
            "Epoch 87/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0170 - acc: 0.9875\n",
            "Epoch 88/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0168 - acc: 0.9888\n",
            "Epoch 89/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0165 - acc: 0.9875\n",
            "Epoch 90/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0164 - acc: 0.9888\n",
            "Epoch 91/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0163 - acc: 0.9875\n",
            "Epoch 92/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0160 - acc: 0.9888\n",
            "Epoch 93/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0160 - acc: 0.9888\n",
            "Epoch 94/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0158 - acc: 0.9888\n",
            "Epoch 95/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0157 - acc: 0.9888\n",
            "Epoch 96/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0157 - acc: 0.9888\n",
            "Epoch 97/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0154 - acc: 0.9888\n",
            "Epoch 98/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0152 - acc: 0.9888\n",
            "Epoch 99/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0151 - acc: 0.9888\n",
            "Epoch 100/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0150 - acc: 0.9888\n",
            "Epoch 101/150\n",
            "800/800 [==============================] - 0s 128us/step - loss: 0.0149 - acc: 0.9888\n",
            "Epoch 102/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0149 - acc: 0.9888\n",
            "Epoch 103/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.0146 - acc: 0.9888\n",
            "Epoch 104/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0145 - acc: 0.9888\n",
            "Epoch 105/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0143 - acc: 0.9900\n",
            "Epoch 106/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0142 - acc: 0.9888\n",
            "Epoch 107/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0141 - acc: 0.9900\n",
            "Epoch 108/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.0141 - acc: 0.9900\n",
            "Epoch 109/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0142 - acc: 0.9900\n",
            "Epoch 110/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0139 - acc: 0.9900\n",
            "Epoch 111/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0137 - acc: 0.9900\n",
            "Epoch 112/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0136 - acc: 0.9900\n",
            "Epoch 113/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0136 - acc: 0.9913\n",
            "Epoch 114/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0134 - acc: 0.9913\n",
            "Epoch 115/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0134 - acc: 0.9900\n",
            "Epoch 116/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0133 - acc: 0.9900\n",
            "Epoch 117/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0131 - acc: 0.9900\n",
            "Epoch 118/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0131 - acc: 0.9900\n",
            "Epoch 119/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0130 - acc: 0.9900\n",
            "Epoch 120/150\n",
            "800/800 [==============================] - 0s 126us/step - loss: 0.0128 - acc: 0.9913\n",
            "Epoch 121/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0127 - acc: 0.9913\n",
            "Epoch 122/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0126 - acc: 0.9900\n",
            "Epoch 123/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0126 - acc: 0.9913\n",
            "Epoch 124/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0126 - acc: 0.9913\n",
            "Epoch 125/150\n",
            "800/800 [==============================] - 0s 128us/step - loss: 0.0124 - acc: 0.9900\n",
            "Epoch 126/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0123 - acc: 0.9913\n",
            "Epoch 127/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0122 - acc: 0.9913\n",
            "Epoch 128/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0121 - acc: 0.9913\n",
            "Epoch 129/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0120 - acc: 0.9913\n",
            "Epoch 130/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.0119 - acc: 0.9913\n",
            "Epoch 131/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0120 - acc: 0.9900\n",
            "Epoch 132/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0118 - acc: 0.9913\n",
            "Epoch 133/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0117 - acc: 0.9913\n",
            "Epoch 134/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0116 - acc: 0.9913\n",
            "Epoch 135/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0117 - acc: 0.9913\n",
            "Epoch 136/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.0114 - acc: 0.9913\n",
            "Epoch 137/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0114 - acc: 0.9913\n",
            "Epoch 138/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0114 - acc: 0.9913\n",
            "Epoch 139/150\n",
            "800/800 [==============================] - 0s 124us/step - loss: 0.0113 - acc: 0.9913\n",
            "Epoch 140/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0112 - acc: 0.9913\n",
            "Epoch 141/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0112 - acc: 0.9913\n",
            "Epoch 142/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0110 - acc: 0.9913\n",
            "Epoch 143/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0110 - acc: 0.9913\n",
            "Epoch 144/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0109 - acc: 0.9913\n",
            "Epoch 145/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0108 - acc: 0.9913\n",
            "Epoch 146/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0107 - acc: 0.9913\n",
            "Epoch 147/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0107 - acc: 0.9913\n",
            "Epoch 148/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0106 - acc: 0.9913\n",
            "Epoch 149/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0107 - acc: 0.9913\n",
            "Epoch 150/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0105 - acc: 0.9913\n",
            "200/200 [==============================] - 1s 5ms/step\n",
            "Epoch 1/150\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.2391 - acc: 0.5862\n",
            "Epoch 2/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.2209 - acc: 0.6500\n",
            "Epoch 3/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.1974 - acc: 0.7987\n",
            "Epoch 4/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.1770 - acc: 0.8575\n",
            "Epoch 5/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.1584 - acc: 0.8837\n",
            "Epoch 6/150\n",
            "800/800 [==============================] - 0s 150us/step - loss: 0.1433 - acc: 0.9163\n",
            "Epoch 7/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.1294 - acc: 0.9288\n",
            "Epoch 8/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.1187 - acc: 0.9488\n",
            "Epoch 9/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.1074 - acc: 0.9538\n",
            "Epoch 10/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0990 - acc: 0.9525\n",
            "Epoch 11/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0922 - acc: 0.9612\n",
            "Epoch 12/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0860 - acc: 0.9625\n",
            "Epoch 13/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0800 - acc: 0.9638\n",
            "Epoch 14/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0745 - acc: 0.9675\n",
            "Epoch 15/150\n",
            "800/800 [==============================] - 0s 150us/step - loss: 0.0708 - acc: 0.9663\n",
            "Epoch 16/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0665 - acc: 0.9700\n",
            "Epoch 17/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0640 - acc: 0.9763\n",
            "Epoch 18/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0598 - acc: 0.9788\n",
            "Epoch 19/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0569 - acc: 0.9787\n",
            "Epoch 20/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0545 - acc: 0.9775\n",
            "Epoch 21/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0523 - acc: 0.9763\n",
            "Epoch 22/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0498 - acc: 0.9825\n",
            "Epoch 23/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0475 - acc: 0.9825\n",
            "Epoch 24/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.0453 - acc: 0.9850\n",
            "Epoch 25/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0438 - acc: 0.9825\n",
            "Epoch 26/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0422 - acc: 0.9850\n",
            "Epoch 27/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0410 - acc: 0.9850\n",
            "Epoch 28/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0394 - acc: 0.9850\n",
            "Epoch 29/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0383 - acc: 0.9888\n",
            "Epoch 30/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0368 - acc: 0.9863\n",
            "Epoch 31/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0358 - acc: 0.9875\n",
            "Epoch 32/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0351 - acc: 0.9900\n",
            "Epoch 33/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0339 - acc: 0.9888\n",
            "Epoch 34/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0330 - acc: 0.9863\n",
            "Epoch 35/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0323 - acc: 0.9875\n",
            "Epoch 36/150\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0312 - acc: 0.9900\n",
            "Epoch 37/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.0302 - acc: 0.9888\n",
            "Epoch 38/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0297 - acc: 0.9913\n",
            "Epoch 39/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0294 - acc: 0.9888\n",
            "Epoch 40/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0283 - acc: 0.9900\n",
            "Epoch 41/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0276 - acc: 0.9913\n",
            "Epoch 42/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0273 - acc: 0.9913\n",
            "Epoch 43/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0265 - acc: 0.9913\n",
            "Epoch 44/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0260 - acc: 0.9913\n",
            "Epoch 45/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0255 - acc: 0.9913\n",
            "Epoch 46/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0250 - acc: 0.9913\n",
            "Epoch 47/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0245 - acc: 0.9913\n",
            "Epoch 48/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0241 - acc: 0.9913\n",
            "Epoch 49/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0235 - acc: 0.9913\n",
            "Epoch 50/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0232 - acc: 0.9913\n",
            "Epoch 51/150\n",
            "800/800 [==============================] - 0s 152us/step - loss: 0.0227 - acc: 0.9925\n",
            "Epoch 52/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0226 - acc: 0.9925\n",
            "Epoch 53/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0221 - acc: 0.9925\n",
            "Epoch 54/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0217 - acc: 0.9925\n",
            "Epoch 55/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0215 - acc: 0.9913\n",
            "Epoch 56/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0209 - acc: 0.9925\n",
            "Epoch 57/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0209 - acc: 0.9925\n",
            "Epoch 58/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0204 - acc: 0.9925\n",
            "Epoch 59/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0200 - acc: 0.9925\n",
            "Epoch 60/150\n",
            "800/800 [==============================] - 0s 154us/step - loss: 0.0198 - acc: 0.9925\n",
            "Epoch 61/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0194 - acc: 0.9925\n",
            "Epoch 62/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0192 - acc: 0.9925\n",
            "Epoch 63/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0190 - acc: 0.9925\n",
            "Epoch 64/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0187 - acc: 0.9925\n",
            "Epoch 65/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0184 - acc: 0.9925\n",
            "Epoch 66/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0183 - acc: 0.9925\n",
            "Epoch 67/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0179 - acc: 0.9925\n",
            "Epoch 68/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0179 - acc: 0.9925\n",
            "Epoch 69/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0175 - acc: 0.9925\n",
            "Epoch 70/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0173 - acc: 0.9925\n",
            "Epoch 71/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0171 - acc: 0.9925\n",
            "Epoch 72/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0169 - acc: 0.9925\n",
            "Epoch 73/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0167 - acc: 0.9925\n",
            "Epoch 74/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0165 - acc: 0.9925\n",
            "Epoch 75/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0165 - acc: 0.9925\n",
            "Epoch 76/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0161 - acc: 0.9925\n",
            "Epoch 77/150\n",
            "800/800 [==============================] - 0s 154us/step - loss: 0.0160 - acc: 0.9925\n",
            "Epoch 78/150\n",
            "800/800 [==============================] - 0s 153us/step - loss: 0.0158 - acc: 0.9925\n",
            "Epoch 79/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0158 - acc: 0.9925\n",
            "Epoch 80/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0156 - acc: 0.9925\n",
            "Epoch 81/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0154 - acc: 0.9925\n",
            "Epoch 82/150\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0151 - acc: 0.9925\n",
            "Epoch 83/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0151 - acc: 0.9925\n",
            "Epoch 84/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0148 - acc: 0.9925\n",
            "Epoch 85/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0148 - acc: 0.9925\n",
            "Epoch 86/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.0146 - acc: 0.9925\n",
            "Epoch 87/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0145 - acc: 0.9925\n",
            "Epoch 88/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0143 - acc: 0.9925\n",
            "Epoch 89/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0143 - acc: 0.9925\n",
            "Epoch 90/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0140 - acc: 0.9925\n",
            "Epoch 91/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0139 - acc: 0.9925\n",
            "Epoch 92/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0139 - acc: 0.9925\n",
            "Epoch 93/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0137 - acc: 0.9925\n",
            "Epoch 94/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0135 - acc: 0.9925\n",
            "Epoch 95/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0134 - acc: 0.9925\n",
            "Epoch 96/150\n",
            "800/800 [==============================] - 0s 152us/step - loss: 0.0134 - acc: 0.9925\n",
            "Epoch 97/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0132 - acc: 0.9925\n",
            "Epoch 98/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0130 - acc: 0.9925\n",
            "Epoch 99/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0130 - acc: 0.9925\n",
            "Epoch 100/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0129 - acc: 0.9925\n",
            "Epoch 101/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0127 - acc: 0.9925\n",
            "Epoch 102/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0126 - acc: 0.9925\n",
            "Epoch 103/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0125 - acc: 0.9925\n",
            "Epoch 104/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0125 - acc: 0.9925\n",
            "Epoch 105/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0125 - acc: 0.9925\n",
            "Epoch 106/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0123 - acc: 0.9925\n",
            "Epoch 107/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0121 - acc: 0.9925\n",
            "Epoch 108/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0120 - acc: 0.9925\n",
            "Epoch 109/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0120 - acc: 0.9925\n",
            "Epoch 110/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0121 - acc: 0.9925\n",
            "Epoch 111/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0118 - acc: 0.9925\n",
            "Epoch 112/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0116 - acc: 0.9925\n",
            "Epoch 113/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0116 - acc: 0.9925\n",
            "Epoch 114/150\n",
            "800/800 [==============================] - 0s 153us/step - loss: 0.0114 - acc: 0.9925\n",
            "Epoch 115/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0114 - acc: 0.9925\n",
            "Epoch 116/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0113 - acc: 0.9925\n",
            "Epoch 117/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0112 - acc: 0.9925\n",
            "Epoch 118/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0111 - acc: 0.9925\n",
            "Epoch 119/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0111 - acc: 0.9925\n",
            "Epoch 120/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0110 - acc: 0.9925\n",
            "Epoch 121/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0109 - acc: 0.9925\n",
            "Epoch 122/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0108 - acc: 0.9925\n",
            "Epoch 123/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0107 - acc: 0.9925\n",
            "Epoch 124/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0108 - acc: 0.9925\n",
            "Epoch 125/150\n",
            "800/800 [==============================] - 0s 154us/step - loss: 0.0106 - acc: 0.9925\n",
            "Epoch 126/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.0105 - acc: 0.9925\n",
            "Epoch 127/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0105 - acc: 0.9925\n",
            "Epoch 128/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0104 - acc: 0.9925\n",
            "Epoch 129/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.0102 - acc: 0.9925\n",
            "Epoch 130/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0103 - acc: 0.9938\n",
            "Epoch 131/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0102 - acc: 0.9938\n",
            "Epoch 132/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0101 - acc: 0.9938\n",
            "Epoch 133/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0101 - acc: 0.9925\n",
            "Epoch 134/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0100 - acc: 0.9938\n",
            "Epoch 135/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0099 - acc: 0.9938\n",
            "Epoch 136/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0098 - acc: 0.9925\n",
            "Epoch 137/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0098 - acc: 0.9938\n",
            "Epoch 138/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0097 - acc: 0.9938\n",
            "Epoch 139/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0097 - acc: 0.9938\n",
            "Epoch 140/150\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0095 - acc: 0.9938\n",
            "Epoch 141/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0096 - acc: 0.9938\n",
            "Epoch 142/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0095 - acc: 0.9938\n",
            "Epoch 143/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0094 - acc: 0.9937\n",
            "Epoch 144/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0093 - acc: 0.9938\n",
            "Epoch 145/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0093 - acc: 0.9938\n",
            "Epoch 146/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0092 - acc: 0.9938\n",
            "Epoch 147/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0091 - acc: 0.9938\n",
            "Epoch 148/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0091 - acc: 0.9938\n",
            "Epoch 149/150\n",
            "800/800 [==============================] - 0s 154us/step - loss: 0.0091 - acc: 0.9938\n",
            "Epoch 150/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0089 - acc: 0.9938\n",
            "200/200 [==============================] - 1s 5ms/step\n",
            "Epoch 1/150\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.2456 - acc: 0.5663\n",
            "Epoch 2/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.2203 - acc: 0.7450\n",
            "Epoch 3/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.1880 - acc: 0.8263\n",
            "Epoch 4/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.1641 - acc: 0.8438\n",
            "Epoch 5/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.1451 - acc: 0.8875\n",
            "Epoch 6/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.1320 - acc: 0.8913\n",
            "Epoch 7/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.1186 - acc: 0.9225\n",
            "Epoch 8/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.1093 - acc: 0.9300\n",
            "Epoch 9/150\n",
            "800/800 [==============================] - 0s 150us/step - loss: 0.1016 - acc: 0.9300\n",
            "Epoch 10/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0931 - acc: 0.9475\n",
            "Epoch 11/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0871 - acc: 0.9525\n",
            "Epoch 12/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0813 - acc: 0.9538\n",
            "Epoch 13/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0776 - acc: 0.9550\n",
            "Epoch 14/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0722 - acc: 0.9650\n",
            "Epoch 15/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0689 - acc: 0.9650\n",
            "Epoch 16/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0652 - acc: 0.9675\n",
            "Epoch 17/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0621 - acc: 0.9675\n",
            "Epoch 18/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0600 - acc: 0.9688\n",
            "Epoch 19/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0570 - acc: 0.9738\n",
            "Epoch 20/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0546 - acc: 0.9700\n",
            "Epoch 21/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0526 - acc: 0.9713\n",
            "Epoch 22/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0508 - acc: 0.9750\n",
            "Epoch 23/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0488 - acc: 0.9738\n",
            "Epoch 24/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0472 - acc: 0.9750\n",
            "Epoch 25/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0459 - acc: 0.9763\n",
            "Epoch 26/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0446 - acc: 0.9750\n",
            "Epoch 27/150\n",
            "800/800 [==============================] - 0s 151us/step - loss: 0.0430 - acc: 0.9788\n",
            "Epoch 28/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0420 - acc: 0.9750\n",
            "Epoch 29/150\n",
            "800/800 [==============================] - 0s 153us/step - loss: 0.0407 - acc: 0.9763\n",
            "Epoch 30/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0399 - acc: 0.9750\n",
            "Epoch 31/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0389 - acc: 0.9788\n",
            "Epoch 32/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0376 - acc: 0.9775\n",
            "Epoch 33/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0366 - acc: 0.9788\n",
            "Epoch 34/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0360 - acc: 0.9813\n",
            "Epoch 35/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0350 - acc: 0.9813\n",
            "Epoch 36/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0346 - acc: 0.9825\n",
            "Epoch 37/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0337 - acc: 0.9825\n",
            "Epoch 38/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0327 - acc: 0.9813\n",
            "Epoch 39/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0322 - acc: 0.9838\n",
            "Epoch 40/150\n",
            "800/800 [==============================] - 0s 128us/step - loss: 0.0320 - acc: 0.9813\n",
            "Epoch 41/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0318 - acc: 0.9825\n",
            "Epoch 42/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0304 - acc: 0.9838\n",
            "Epoch 43/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0299 - acc: 0.9838\n",
            "Epoch 44/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0293 - acc: 0.9838\n",
            "Epoch 45/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0288 - acc: 0.9838\n",
            "Epoch 46/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0282 - acc: 0.9838\n",
            "Epoch 47/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0281 - acc: 0.9838\n",
            "Epoch 48/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0274 - acc: 0.9850\n",
            "Epoch 49/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0269 - acc: 0.9838\n",
            "Epoch 50/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0265 - acc: 0.9838\n",
            "Epoch 51/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0260 - acc: 0.9850\n",
            "Epoch 52/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0257 - acc: 0.9838\n",
            "Epoch 53/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0257 - acc: 0.9838\n",
            "Epoch 54/150\n",
            "800/800 [==============================] - 0s 150us/step - loss: 0.0254 - acc: 0.9863\n",
            "Epoch 55/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0245 - acc: 0.9850\n",
            "Epoch 56/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0242 - acc: 0.9850\n",
            "Epoch 57/150\n",
            "800/800 [==============================] - 0s 129us/step - loss: 0.0239 - acc: 0.9850\n",
            "Epoch 58/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0236 - acc: 0.9875\n",
            "Epoch 59/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0234 - acc: 0.9863\n",
            "Epoch 60/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0230 - acc: 0.9850\n",
            "Epoch 61/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0230 - acc: 0.9863\n",
            "Epoch 62/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0225 - acc: 0.9850\n",
            "Epoch 63/150\n",
            "800/800 [==============================] - 0s 150us/step - loss: 0.0222 - acc: 0.9875\n",
            "Epoch 64/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0218 - acc: 0.9863\n",
            "Epoch 65/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0217 - acc: 0.9863\n",
            "Epoch 66/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0212 - acc: 0.9863\n",
            "Epoch 67/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0211 - acc: 0.9863\n",
            "Epoch 68/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0207 - acc: 0.9875\n",
            "Epoch 69/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0206 - acc: 0.9863\n",
            "Epoch 70/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0202 - acc: 0.9850\n",
            "Epoch 71/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0201 - acc: 0.9850\n",
            "Epoch 72/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0199 - acc: 0.9863\n",
            "Epoch 73/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0196 - acc: 0.9863\n",
            "Epoch 74/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0195 - acc: 0.9875\n",
            "Epoch 75/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0193 - acc: 0.9875\n",
            "Epoch 76/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0191 - acc: 0.9863\n",
            "Epoch 77/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0189 - acc: 0.9875\n",
            "Epoch 78/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0186 - acc: 0.9863\n",
            "Epoch 79/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0185 - acc: 0.9875\n",
            "Epoch 80/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0183 - acc: 0.9875\n",
            "Epoch 81/150\n",
            "800/800 [==============================] - 0s 152us/step - loss: 0.0181 - acc: 0.9888\n",
            "Epoch 82/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0182 - acc: 0.9863\n",
            "Epoch 83/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0177 - acc: 0.9875\n",
            "Epoch 84/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0176 - acc: 0.9888\n",
            "Epoch 85/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0175 - acc: 0.9887\n",
            "Epoch 86/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0173 - acc: 0.9875\n",
            "Epoch 87/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0169 - acc: 0.9875\n",
            "Epoch 88/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0168 - acc: 0.9875\n",
            "Epoch 89/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0168 - acc: 0.9875\n",
            "Epoch 90/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0166 - acc: 0.9875\n",
            "Epoch 91/150\n",
            "800/800 [==============================] - 0s 143us/step - loss: 0.0163 - acc: 0.9875\n",
            "Epoch 92/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0164 - acc: 0.9887\n",
            "Epoch 93/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0161 - acc: 0.9888\n",
            "Epoch 94/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0159 - acc: 0.9888\n",
            "Epoch 95/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0159 - acc: 0.9888\n",
            "Epoch 96/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0157 - acc: 0.9888\n",
            "Epoch 97/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0156 - acc: 0.9900\n",
            "Epoch 98/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0153 - acc: 0.9875\n",
            "Epoch 99/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0154 - acc: 0.9888\n",
            "Epoch 100/150\n",
            "800/800 [==============================] - 0s 154us/step - loss: 0.0151 - acc: 0.9888\n",
            "Epoch 101/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0149 - acc: 0.9888\n",
            "Epoch 102/150\n",
            "800/800 [==============================] - 0s 139us/step - loss: 0.0151 - acc: 0.9888\n",
            "Epoch 103/150\n",
            "800/800 [==============================] - 0s 131us/step - loss: 0.0148 - acc: 0.9888\n",
            "Epoch 104/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0146 - acc: 0.9900\n",
            "Epoch 105/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0145 - acc: 0.9888\n",
            "Epoch 106/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0144 - acc: 0.9900\n",
            "Epoch 107/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0143 - acc: 0.9888\n",
            "Epoch 108/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0143 - acc: 0.9900\n",
            "Epoch 109/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0141 - acc: 0.9900\n",
            "Epoch 110/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0142 - acc: 0.9913\n",
            "Epoch 111/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0138 - acc: 0.9900\n",
            "Epoch 112/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0138 - acc: 0.9900\n",
            "Epoch 113/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0136 - acc: 0.9925\n",
            "Epoch 114/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0135 - acc: 0.9925\n",
            "Epoch 115/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0136 - acc: 0.9925\n",
            "Epoch 116/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0133 - acc: 0.9913\n",
            "Epoch 117/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0132 - acc: 0.9925\n",
            "Epoch 118/150\n",
            "800/800 [==============================] - 0s 146us/step - loss: 0.0131 - acc: 0.9925\n",
            "Epoch 119/150\n",
            "800/800 [==============================] - 0s 150us/step - loss: 0.0130 - acc: 0.9900\n",
            "Epoch 120/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0129 - acc: 0.9913\n",
            "Epoch 121/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0128 - acc: 0.9913\n",
            "Epoch 122/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0128 - acc: 0.9925\n",
            "Epoch 123/150\n",
            "800/800 [==============================] - 0s 144us/step - loss: 0.0127 - acc: 0.9913\n",
            "Epoch 124/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0126 - acc: 0.9925\n",
            "Epoch 125/150\n",
            "800/800 [==============================] - 0s 141us/step - loss: 0.0125 - acc: 0.9925\n",
            "Epoch 126/150\n",
            "800/800 [==============================] - 0s 134us/step - loss: 0.0124 - acc: 0.9913\n",
            "Epoch 127/150\n",
            "800/800 [==============================] - 0s 148us/step - loss: 0.0123 - acc: 0.9925\n",
            "Epoch 128/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0121 - acc: 0.9925\n",
            "Epoch 129/150\n",
            "800/800 [==============================] - 0s 138us/step - loss: 0.0122 - acc: 0.9925\n",
            "Epoch 130/150\n",
            "800/800 [==============================] - 0s 152us/step - loss: 0.0121 - acc: 0.9925\n",
            "Epoch 131/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0120 - acc: 0.9913\n",
            "Epoch 132/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0121 - acc: 0.9925\n",
            "Epoch 133/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0118 - acc: 0.9925\n",
            "Epoch 134/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0117 - acc: 0.9925\n",
            "Epoch 135/150\n",
            "800/800 [==============================] - 0s 145us/step - loss: 0.0116 - acc: 0.9925\n",
            "Epoch 136/150\n",
            "800/800 [==============================] - 0s 147us/step - loss: 0.0115 - acc: 0.9925\n",
            "Epoch 137/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0115 - acc: 0.9925\n",
            "Epoch 138/150\n",
            "800/800 [==============================] - 0s 132us/step - loss: 0.0113 - acc: 0.9925\n",
            "Epoch 139/150\n",
            "800/800 [==============================] - 0s 130us/step - loss: 0.0113 - acc: 0.9925\n",
            "Epoch 140/150\n",
            "800/800 [==============================] - 0s 140us/step - loss: 0.0112 - acc: 0.9925\n",
            "Epoch 141/150\n",
            "800/800 [==============================] - 0s 142us/step - loss: 0.0112 - acc: 0.9925\n",
            "Epoch 142/150\n",
            "800/800 [==============================] - 0s 136us/step - loss: 0.0112 - acc: 0.9925\n",
            "Epoch 143/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0110 - acc: 0.9925\n",
            "Epoch 144/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0108 - acc: 0.9925\n",
            "Epoch 145/150\n",
            "800/800 [==============================] - 0s 149us/step - loss: 0.0108 - acc: 0.9925\n",
            "Epoch 146/150\n",
            "800/800 [==============================] - 0s 133us/step - loss: 0.0108 - acc: 0.9925\n",
            "Epoch 147/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0107 - acc: 0.9925\n",
            "Epoch 148/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0105 - acc: 0.9925\n",
            "Epoch 149/150\n",
            "800/800 [==============================] - 0s 135us/step - loss: 0.0106 - acc: 0.9925\n",
            "Epoch 150/150\n",
            "800/800 [==============================] - 0s 137us/step - loss: 0.0104 - acc: 0.9925\n",
            "200/200 [==============================] - 1s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA5ZuNYcYDap",
        "colab_type": "code",
        "outputId": "3c111bae-8f3b-41d2-b40d-d10ac9c0978e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "print(cross_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-f88ddd85a545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cross_score' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjsfbHkHYATs",
        "colab_type": "text"
      },
      "source": [
        "**The different folds perform similarly, and each well.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vljzp0dYYce5",
        "colab_type": "text"
      },
      "source": [
        "## Part 2: Cancer Type Classifier for 18 Common Tumor Types\n",
        "\n",
        "### 2.a) Using SciKit Learn build a machine learning classifier that classifies Cancer Type from the type.coding.csv and type.all.csv files.  Compare the coding vs all genes cases.\n",
        "\n",
        "### 2.b) Using model selection methods of your choice, determine which classical ML method performs best.\n",
        "\n",
        "We will do both of these at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmjK2-K1YqaB",
        "colab_type": "text"
      },
      "source": [
        "Let's import the type csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-hMItp4YbtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the data frame \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# The normal tumor coding data\n",
        "# nt_coding = pd.read_csv(\"/content/drive/My Drive/MLiM-Datasets/HW2/type.coding.csv\")\n",
        "\n",
        "# All of the normal tumor data\n",
        "# nt_all = pd.read_csv(\"/content/drive/My Drive/MLiM-Datasets/HW2/type.all.csv\")\n",
        "\n",
        "# Uncomment above later, debug using small sets\n",
        "\n",
        "# The normal tumor coding data\n",
        "type_coding = pd.read_csv(\"/content/drive/My Drive/MLiM-Datasets/HW2/type.coding.csv\")\n",
        "\n",
        "# All of the normal tumor data\n",
        "type_all = pd.read_csv(\"/content/drive/My Drive/MLiM-Datasets/HW2/type.all.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rTOJSvka-Vv",
        "colab_type": "text"
      },
      "source": [
        "Wrangle the data and normalize it (This time right away)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uD1rMnKaYKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Organize the data\n",
        "\n",
        "\n",
        "# target labels for data, either 0 for normal\n",
        "# or 1 for tumor\n",
        "\n",
        "# cancer df with all cols except Type (aka, features)\n",
        "# here 'axis = 1' means dropping by columns. to drop by rows,\n",
        "# type 'axis = 0'\n",
        "features_tc = type_coding.drop('Type', axis=1)\n",
        "\n",
        "# the labels of the data\n",
        "labels_tc = type_coding['Type'].to_numpy()\n",
        "# the names of the different features, in this case input gene names\n",
        "feature_names_tc = type_coding.columns\n",
        "# the feature data\n",
        "features_tc = type_coding.to_numpy()\n",
        "\n",
        "\n",
        "# normalize the data usign this function\n",
        "scaler = MinMaxScaler()\n",
        "# fit the normalizer to the data\n",
        "scaler.fit(features_tc)\n",
        "# transform the data based on the new scaler\n",
        "features_tc = scaler.transform(features_tc)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNMY3EoNf8rm",
        "colab_type": "text"
      },
      "source": [
        "Now that the data is normalized, we can go ahead and jump right in and do 2)a) and 2)b) right now as well and just run the data through the same classifiers used earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ngG6Dc6i6PQ",
        "colab_type": "code",
        "outputId": "0bb5ff97-7fae-4b57-90f6-b3d4923f66fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score \n",
        "# initialize the classifiers, mostly using default parameters\n",
        "# due to ignorance on what would be best\n",
        "sigmoid_svm = SVC(kernel=\"sigmoid\", gamma = \"auto\")\n",
        "dtree = DecisionTreeClassifier()\n",
        "kmeans = KMeans(n_clusters = 2)\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "cross_val = KFold(5)\n",
        "\n",
        "# For sigmoid svm\n",
        "acc_sigmoid_svm = cross_val_score(sigmoid_svm, features_tc, labels_tc, cv=cross_val)\n",
        "# For decision tree\n",
        "acc_dtree = cross_val_score(dtree, features_tc, labels_tc, cv=cross_val)\n",
        "# For KMeans\n",
        "acc_kmeans = cross_val_score(kmeans, features_tc, labels_tc, cv=cross_val)\n",
        "# For Gaussian Naive Bayes\n",
        "acc_gaussian_nb = cross_val_score(gaussian_nb, features_tc, labels_tc, cv=cross_val)\n",
        "\n",
        "print(\"Sigmoid SVM accuracy: \", acc_sigmoid_svm)\n",
        "print(\"Decision tree accuracy: \", acc_dtree)\n",
        "print(\"KMeans accuracy: \", acc_kmeans)\n",
        "print(\"Gaussian Naive Bayes accuracy: \", acc_gaussian_nb)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sigmoid SVM accuracy:  [0.185 0.19  0.235 0.175 0.205]\n",
            "Decision tree accuracy:  [0.935 0.97  0.985 0.98  0.98 ]\n",
            "KMeans accuracy:  [-102939.98187166 -105851.3906881  -101859.38045618 -105326.81137902\n",
            " -108450.89267433]\n",
            "Gaussian Naive Bayes accuracy:  [0.23  0.205 0.23  0.215 0.24 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQbUQ-UKkSYA",
        "colab_type": "text"
      },
      "source": [
        "**Not terrible (better than guessing (1/18), except of course for KMeans, which we see the strange accuracy measurement still appearing that appeared before, even now that the data is normalized. I have just learned now that this is because KMeans is clustering, but it is not a classification method if employed just like this. This is because it only returns distance from the mean rather than some actual predicted value. It should never have been used! We do see Decision tree performing quite well still, which is really interesting! I wonder if domain knowledge would point to this.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wkU8I6ioFTN",
        "colab_type": "text"
      },
      "source": [
        "Lets quickly apply this to the type_all.csv to see if we get any difference (again, like before)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y023BU6roKsP",
        "colab_type": "code",
        "outputId": "e8036e66-ab9b-4c6f-f2a7-d420d5eeb550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Organize the data\n",
        "\n",
        "\n",
        "# target labels for data, either 0 for normal\n",
        "# or 1 for tumor\n",
        "\n",
        "# cancer df with all cols except Type (aka, features)\n",
        "# here 'axis = 1' means dropping by columns. to drop by rows,\n",
        "# type 'axis = 0'\n",
        "features_ta = type_all.drop('Type', axis=1)\n",
        "\n",
        "# the labels of the data\n",
        "labels_ta = type_all['Type'].to_numpy()\n",
        "# the names of the different features, in this case input gene names\n",
        "feature_names_ta = type_all.columns\n",
        "# the feature data\n",
        "features_ta = type_all.to_numpy()\n",
        "\n",
        "\n",
        "# normalize the data usign this function\n",
        "scaler = MinMaxScaler()\n",
        "# fit the normalizer to the data\n",
        "scaler.fit(features_ta)\n",
        "# transform the data based on the new scaler\n",
        "features_ta = scaler.transform(features_ta)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score \n",
        "# initialize the classifiers, mostly using default parameters\n",
        "# due to ignorance on what would be best\n",
        "sigmoid_svm = SVC(kernel=\"sigmoid\", gamma = \"auto\")\n",
        "dtree = DecisionTreeClassifier()\n",
        "kmeans = KMeans(n_clusters = 2)\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "cross_val = KFold(5)\n",
        "\n",
        "# For sigmoid svm\n",
        "acc_sigmoid_svm = cross_val_score(sigmoid_svm, features_ta, labels_ta, cv=cross_val)\n",
        "# For decision tree\n",
        "acc_dtree = cross_val_score(dtree, features_ta, labels_ta, cv=cross_val)\n",
        "# For KMeans\n",
        "acc_kmeans = cross_val_score(kmeans, features_ta, labels_ta, cv=cross_val)\n",
        "# For Gaussian Naive Bayes\n",
        "acc_gaussian_nb = cross_val_score(gaussian_nb, features_ta, labels_ta, cv=cross_val)\n",
        "\n",
        "print(\"Sigmoid SVM accuracy: \", acc_sigmoid_svm)\n",
        "print(\"Decision tree accuracy: \", acc_dtree)\n",
        "print(\"KMeans accuracy: \", acc_kmeans)\n",
        "print(\"Gaussian Naive Bayes accuracy: \", acc_gaussian_nb)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sigmoid SVM accuracy:  [0.235 0.17  0.255 0.205 0.215]\n",
            "Decision tree accuracy:  [0.96  0.97  0.965 0.97  0.965]\n",
            "KMeans accuracy:  [-508076.75749404 -520855.65223671 -515192.61005143 -514324.31919128\n",
            " -546008.63823215]\n",
            "Gaussian Naive Bayes accuracy:  [0.035 0.035 0.08  0.06  0.115]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMInqyy-rBb2",
        "colab_type": "text"
      },
      "source": [
        "We get more or less the same results (aside from much poorer Gaussian NB; perhaps it can't handle too many vars?) --> so let's just use the coding data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms1hoqUFmZPH",
        "colab_type": "text"
      },
      "source": [
        "### 2.c) Using feature selection methods of your choice, determine a < 100 gene signature that can be used to classify tumor type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yklprl6mycP",
        "colab_type": "text"
      },
      "source": [
        "Let's try the same methods employed before so we can see if using them on normalized data or for the 18 types is any more interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M39KLQCznro3",
        "colab_type": "text"
      },
      "source": [
        "First let's apply Variance Threshold to Decision Tree and Gaussian Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cbec4cfc-8867-4ccc-f8ab-ea4a6acda0f8",
        "id": "8CddYJ7hm24l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# create a variance threshold selector\n",
        "variance_selector = VarianceThreshold(threshold=0.118)\n",
        "\n",
        "print(\"Original number of features:\", features_tc.shape[1])\n",
        "\n",
        "# fit the features to the selector\n",
        "features_tc_sel = variance_selector.fit_transform(features_tc)\n",
        "\n",
        "print(\"Variance selected features:\", features_tc_sel.shape[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original number of features: 19562\n",
            "Variance selected features: 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YjPErkMssQe",
        "colab_type": "text"
      },
      "source": [
        "Suddenly we need a much smaler variance threshold! This must be because now the data is normalized, whereas before the data was all over the place so it only made sense to have such an astronomical variance (in the trillions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9ae9566c-5fba-43be-fc9b-644da51dde0e",
        "id": "QTk7DU73tMKI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# For Gaussian Naive Bayes\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# check the cross validation score for cv = KFold(5)\n",
        "acc_gaussian_nb_sel = cross_val_score(gaussian_nb, features_tc_sel, labels_tc, cv=cross_val)\n",
        "\n",
        "print(\"The Gaussian NB P ~20,000 data accuracy:\", acc_gaussian_nb)\n",
        "print(\"The Gaussian NB P<100 data accuracy:\", acc_gaussian_nb_sel)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Gaussian NB P ~20,000 data accuracy: [0.035 0.035 0.08  0.06  0.115]\n",
            "The Gaussian NB P<100 data accuracy: [0.675 0.69  0.71  0.79  0.665]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "313bbf28-8ff6-43f5-ba54-3b4713d49246",
        "id": "kHPncJXanHDl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# For decision tree\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "# check the cross validation score for cv = KFold(5)\n",
        "acc_dtree_sel = cross_val_score(dtree, features_tc_sel, labels_tc, cv=cross_val)\n",
        "\n",
        "print(\"The Decision Tree  p~20,000 data accuracy:\", acc_dtree)\n",
        "print(\"The Decision Tree P<100 data accuracy:\", acc_dtree_sel)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Decision Tree  p~20,000 data accuracy: [0.96  0.97  0.965 0.97  0.965]\n",
            "The Decision Tree P<100 data accuracy: [0.565 0.535 0.475 0.515 0.565]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laN5XmE9s5tb",
        "colab_type": "text"
      },
      "source": [
        "**Very interesting result. We see that Gaussian works much better with a smaller subset of data (lending support to my idea it works better with less variables to account for, will have to look into this sometime), and that the decision tree predictive ability falls off with P<100, though still okay.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuDfN9pNnz9U",
        "colab_type": "text"
      },
      "source": [
        "Now let's apply the ExtraTreesClassifier to the same two classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KyXCM93NnQqg",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import numpy as np\n",
        "\n",
        "# This uses a tree model to predict the strongest indicators\n",
        "# of the data\n",
        "tree_class= ExtraTreesClassifier(n_estimators=50)\n",
        "\n",
        "tree_class = tree_class.fit(features_tc, labels_tc)\n",
        "\n",
        "# here we set an infinite threshold so that we can just take the best 99 features\n",
        "model = SelectFromModel(tree_class, prefit=True, threshold=-np.inf, max_features=99)\n",
        "\n",
        "# transform old features to new set\n",
        "features_tc_sel = model.transform(features_tc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA7hs026nmZN",
        "colab_type": "code",
        "outputId": "8f3c117d-4fa0-44f2-a073-867ae6fcb8e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# for Gaussian\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# check the cross validation score for cv = KFold(5)\n",
        "acc_gaussian_nb_sel = cross_val_score(gaussian_nb, features_tc_sel, labels_tc, cv=cross_val)\n",
        "\n",
        "print(\"The Gaussian NB P ~20,000 data accuracy:\", acc_gaussian_nb)\n",
        "print(\"The Gaussian NB P<100 data accuracy:\", acc_gaussian_nb_sel)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Gaussian NB P ~20,000 data accuracy: [0.23  0.205 0.23  0.215 0.24 ]\n",
            "The Gaussian NB P<100 data accuracy: [0.995 0.985 0.98  0.99  0.99 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuIoxg2rnQjk",
        "colab_type": "code",
        "outputId": "ed7c887d-33b0-4988-ab4b-fc6d2c00e8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# For decision tree\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "# check the cross validation score for cv = KFold(5)\n",
        "acc_dtree_sel = cross_val_score(dtree, features_tc_sel, labels_tc, cv=cross_val)\n",
        "\n",
        "print(\"The Decision Tree  p~20,000 data accuracy:\", acc_dtree)\n",
        "print(\"The Decision Tree P<100 data accuracy:\", acc_dtree_sel)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Decision Tree  p~20,000 data accuracy: [0.935 0.97  0.985 0.98  0.98 ]\n",
            "The Decision Tree P<100 data accuracy: [0.975 0.965 0.98  0.985 0.98 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5xyJjxUn6LL",
        "colab_type": "text"
      },
      "source": [
        "**This tree method of variable selection is a rock star, bringing Gaussian NB and Decision tree to awesome predictive levels.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7ZqhF0RrJkp",
        "colab_type": "text"
      },
      "source": [
        "### 2.d) Using Keras, build a deep learning classifier that performs the same classification task, and determine the learning curve (relationship of number of training samples to prediction accuracy) for your network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnqLBuVkvex8",
        "colab_type": "code",
        "outputId": "deea02c0-8177-4a4e-b32e-442551b9cc53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# one hot encode labels_tc to categorical\n",
        "\n",
        "# reevaluate this to bring it back into environment\n",
        "train_tc, test_tc, train_labels_tc, test_labels_tc = train_test_split(\n",
        "                                                          features_tc,\n",
        "                                                          labels_tc,\n",
        "                                                          test_size=0.30, \n",
        "                                                          random_state=30)\n",
        "\n",
        "# we need to code the variables to work with our network.\n",
        "# this is called one hot encoding. in essence it needs a column for \n",
        "# each possible value, and then only one is set to 1. thus onehot\n",
        "\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded_train_tc = label_encoder.fit_transform(train_labels_tc)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded_test_tc = label_encoder.fit_transform(test_labels_tc)\n",
        "\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded_train_tc = integer_encoded_train_tc.reshape(len(integer_encoded_train_tc), 1)\n",
        "onehot_encoded_train_tc = onehot_encoder.fit_transform(integer_encoded_train_tc)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded_test_tc = integer_encoded_test_tc.reshape(len(integer_encoded_test_tc), 1)\n",
        "onehot_encoded_test_tc = onehot_encoder.fit_transform(integer_encoded_test_tc)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "75d0a30f-b981-4da3-a51f-ef1f7ad194d4",
        "id": "k5cVYtxzuDe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "number_features = features_tc.shape[1] # grab the number of columns \n",
        "\n",
        "def create_model2():\n",
        "  # input layer followed by our 2 hidden layers and output\n",
        "  input_layer = Input((number_features, ))\n",
        "  hidden = Dense(160, activation = \"relu\")(input_layer) # each layer feeds into another\n",
        "  hidden  = Dense(120, activation = \"relu\")(hidden)\n",
        "  hidden  = Dense(80, activation = \"relu\")(hidden)\n",
        "  hidden  = Dense(36, activation = \"relu\")(hidden)\n",
        "  output_layer = Dense(18)(hidden)\n",
        "\n",
        "\n",
        "# create the model now\n",
        "  deep_model = Model(input = input_layer, output = output_layer)\n",
        "\n",
        "# compile with some parameters we will change\n",
        "  deep_model.compile(optimizer = SGD(lr=0.01, clipnorm=1), loss = \"mse\",\n",
        "                   metrics = [\"accuracy\"])\n",
        "  \n",
        "  return deep_model\n",
        "\n",
        "deep_model = create_model2()\n",
        "\n",
        "# train model\n",
        "history = deep_model.fit(train_tc, onehot_encoded_train_tc, validation_data=(test_tc, onehot_encoded_test_tc),\n",
        "                         epochs=400, batch_size=100)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_37\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_37 (InputLayer)        (None, 19562)             0         \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (None, 160)               3130080   \n",
            "_________________________________________________________________\n",
            "dense_136 (Dense)            (None, 120)               19320     \n",
            "_________________________________________________________________\n",
            "dense_137 (Dense)            (None, 80)                9680      \n",
            "_________________________________________________________________\n",
            "dense_138 (Dense)            (None, 36)                2916      \n",
            "_________________________________________________________________\n",
            "dense_139 (Dense)            (None, 18)                666       \n",
            "=================================================================\n",
            "Total params: 3,162,662\n",
            "Trainable params: 3,162,662\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izAfyNZW8oN2",
        "colab_type": "code",
        "outputId": "d5d28d10-49e0-4d6a-e1a1-57cd8d6052bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd0lFX6wPHvzaT3DqRAQgi9E0Ca\niICAIsq6oiJ2xbXvWtbuqtt0f66rrr2tBXtHBUQUBAWUCCgdAgQSIL33dn9/3JnMTAoJSJiU53NO\nTuZtM08Gzvu8tyutNUIIIQSAm6sDEEII0X5IUhBCCFFPkoIQQoh6khSEEELUk6QghBCiniQFIYQQ\n9SQpiC5FKfWaUupvrTw3VSk1ra1jEqI9kaQghBCiniQFITogpZS7q2MQnZMkBdHuWKtt7lBK/aqU\nKlVKvaKU6qaUWqqUKlZKrVBKhTicP0cptU0pVaCUWqWUGuBwbIRSaqP1uvcA7wafNVsptdl67Vql\n1NBWxniWUmqTUqpIKZWmlHqwwfGJ1vcrsB6/3LrfRyn1b6XUAaVUoVLqe+u+05RS6U18D9Osrx9U\nSn2olFqklCoCLldKjVFKrbN+xhGl1NNKKU+H6wcppb5WSuUppTKVUvcopborpcqUUmEO541USmUr\npTxa87eLzk2SgmivzgOmA32Bs4GlwD1ABOb/7c0ASqm+wDvAH63HlgCfK6U8rTfIT4E3gVDgA+v7\nYr12BPAqcC0QBrwALFZKebUivlLgUiAYOAu4Til1rvV9e1nj/a81puHAZut1jwGjgPHWmP4M1LXy\nOzkH+ND6mW8BtcCfgHBgHDAVuN4aQwCwAlgGRAF9gG+01hnAKmCew/teAryrta5uZRyiE5OkINqr\n/2qtM7XWh4A1wI9a601a6wrgE2CE9bwLgC+11l9bb2qPAT6Ym+4pgAfwhNa6Wmv9IbDB4TMWAi9o\nrX/UWtdqrV8HKq3XHZXWepXWeovWuk5r/SsmMU22Hp4PrNBav2P93Fyt9WallBtwJXCL1vqQ9TPX\naq0rW/mdrNNaf2r9zHKt9c9a6/Va6xqtdSomqdlimA1kaK3/rbWu0FoXa61/tB57HVgAoJSyABdh\nEqcQkhREu5Xp8Lq8iW1/6+so4IDtgNa6DkgDoq3HDmnnWR8POLzuBdxmrX4pUEoVALHW645KKTVW\nKbXSWu1SCPwB88SO9T32NnFZOKb6qqljrZHWIIa+SqkvlFIZ1iqlf7QiBoDPgIFKqXhMaaxQa/3T\nccYkOhlJCqKjO4y5uQOglFKYG+Ih4AgQbd1n09PhdRrwd611sMOPr9b6nVZ87tvAYiBWax0EPA/Y\nPicNSGjimhygopljpYCvw99hwVQ9OWo4pfFzwE4gUWsdiKlec4yhd1OBW0tb72NKC5cgpQThQJKC\n6OjeB85SSk21NpTehqkCWgusA2qAm5VSHkqp3wFjHK59CfiD9alfKaX8rA3IAa343AAgT2tdoZQa\ng6kysnkLmKaUmqeUcldKhSmlhltLMa8CjyulopRSFqXUOGsbxm7A2/r5HsB9QEttGwFAEVCilOoP\nXOdw7Augh1Lqj0opL6VUgFJqrMPxN4DLgTlIUhAOJCmIDk1rvQvzxPtfzJP42cDZWusqrXUV8DvM\nzS8P0/7wscO1ycA1wNNAPpBiPbc1rgceVkoVAw9gkpPtfQ8CZ2ISVB6mkXmY9fDtwBZM20Ye8Cjg\nprUutL7ny5hSTing1BupCbdjklExJsG95xBDMaZq6GwgA9gDTHE4/gOmgXuj1tqxSk10cUoW2RGi\na1JKfQu8rbV+2dWxiPZDkoIQXZBSajTwNaZNpNjV8Yj2Q6qPhOhilFKvY8Yw/FESgmhISgpCCCHq\nSUlBCCFEvQ43qVZ4eLiOi4tzdRhCCNGh/Pzzzzla64ZjXxrpcEkhLi6O5ORkV4chhBAdilKqVV2P\npfpICCFEPUkKQggh6klSEEIIUa/N2hSUUq9ipu/N0loPbuK4Ap7ETAdQBlyutd54PJ9VXV1Neno6\nFRUVvyXkds/b25uYmBg8PGQtFCFE22jLhubXMHPKvNHM8VlAovVnLGbGx7HNnHtU6enpBAQEEBcX\nh/OEmJ2H1prc3FzS09OJj493dThCiE6qzaqPtNarMRN+Necc4A1trAeClVI9juezKioqCAsL67QJ\nAUApRVhYWKcvDQkhXMuVbQrROC8akm7d14hSaqFSKlkplZydnd3km3XmhGDTFf5GIYRrdYiGZq31\ni1rrJK11UkREi2MvhBCikYzCCtpiWp9PNx0is6jzlOBdOXjtEGaFLJsY674Op6CggLfffpvrr7/+\nmK4788wzefvttwkODm6jyIQ4+Uoqa8gsqiAhwr/lk49RbkklmUWV9OsegJs6eum5orqWDal5VFTX\n4abgqteTeWD2QK6cGM+OI0VEBfkQ5Gs6bWit+WzzYdbtzeWX9AIWntqbc4dHsy+nlIc+30agtweP\nXzCMtXtzqanVlFXV8PkvR1ixw6wSO31gN84bGUOd1iRG+rNkSwaj40MorazFz8tCv24BvLRmP5U1\ntQT5eJBZVMmI2GAGRwcRE+rD48t3szOjiDA/L/527mCq6+rYm1XKql1ZVNXWsf1wEXuzS7lrVn9+\nPyrmhH+vjtp0QjylVBzwRTO9j84CbsT0PhoLPKW1HtPwvIaSkpJ0wxHNO3bsYMCAASci5OOSmprK\n7Nmz2bp1q9P+mpoa3N1PbN519d8q2rfKmlo8LW7HXNWYWVRBWl4Zw2ODcbeYCoQfUnIY2COQED/P\n+vM+2ZROcUUNl46LY/XubLYeLmTOsCiSU/PZeqiQ4T2D+WzzYb7ensmWB8/A38udypo6vD0spGQV\n858Ve/g5NZ8Lx8Ry0+mJVNeaYyu2Z/LuhjR2HCkiOsSHKyfEMXOwaWLck1nMqz/sp1ugN8u3ZbL9\nSBEA4f5eTO4bQWF5NZeO60VkoBfJqfk8tnwXp8SHsTmtgIwGT/AxIT5MG9CN19am4udp4Y2rxjKw\nRyB3ffwrn20+DICfp4XSqlpiQ32oqK4ju7iy/vNySirr36vhtpuCuhZup17ublTV1uHv5U5xRQ0A\nQ6KD2HKokIgA8349Q30pq6qt/1ybWYO7c/n4OMb2Dmv1v6sjpdTPWuukFs9rq6SglHoHOA2zkHgm\n8BfAA0Br/by1S+rTwExMl9QrrCthHVV7TAoXXnghn332Gf369cPDwwNvb29CQkLYuXMnu3fv5txz\nzyUtLY2KigpuueUWFi5cCNin7CgpKWHWrFlMnDiRtWvXEh0dzWeffYaPj0+jz3L13ypOPK01uaVV\nhPsfffXNvNIqQh1u0A1lFlUw9h/f8K/zhjJvtL0Q/tP+PJQyN8QfUnI5b2Q0uaVVfLMjk+5BPvQI\n8ubBxdtYuzcXgNlDezA+IZx7PtnCoKhAPrpuPN4eFt796SB3fbwFgBW3Tmbusz/U39ia8tRFI8gs\nrOCZVSmsuv007vt0K1/8eqT+eFyYLxlFFVyQFMs7G9Koqqmjbzd/qmrqSM0t4+FzBlFRXcs/luzE\n19NCeXUtWsO0AZEMjg7iiRV7AFAKHG9jwb4eKCDE15N7zxrA/pxS/vblDqfYZgzqxs6MYkoqaogI\n8GJXZjG3Te/LhWN6EuDtzpItR/jrFzvw87LwzPyR/Hv5br7bnc3EPuHcNas/SkHfbgFsOljAS2v2\n8fX2TCICvBifEEZWUSVzhkfh62khIcKf9PxydmcWc1q/CPpE+lNaWUuYnyc7Mop4Zc1+Pt50iNlD\ne/D0/JEs35bBjW9vwuKmmD+2J78fFUNNrSaruIKpA7od9f9HS1yeFNpKS0nhoc+3sf1w0Qn9zIFR\ngfzl7EHNHncsKaxatYqzzjqLrVu31ncdzcvLIzQ0lPLyckaPHs13331HWFiYU1Lo06cPycnJDB8+\nnHnz5jFnzhwWLFjQ6LMkKXQuWmvu+mgL7/+cxod/GM+oXiH1xwrKqvjfD6lcfEpPPkhO5/++2sVb\nV49lQp9wlm09wu7MEm6Y0geLm6KwrJr/fruHl7/fz9CYIBbfOJG1e3P4dkcWL3+/nxBfD3qF+bE5\nrYCn54/gjbUH+Cm1cefACX3C+CHFJIe4MF9Sc8v46zmDiAjw4g+LNjKwRyA7M4oYGhPM5rQCPCyK\n60/rwym9wziYV8qdH2056t87PiGMS8fFcedHv1JYXo23hxsV1XVEBHix9JZJhPt7UVVTxxWv/VQf\nh1Lw0XXjifD3Ym92CZP7RqCUYs2ebDYeKODqSfHMffYHdmeWcGrfCG6d3pch0UH11UsV1bX0v38Z\nQ6KDiA72Ibe0kreuPoW0/DLmPvMDSimeumgEk/s6t1dqrdEa3NwUheXVPLFiN1eMj6dnmK/Tebsz\ni7nklR959uJRTv9+rf33X7s3lyExQQR6m6qs4opq3N3c8PG0HNN7taS1SaHDTYjXEYwZM8ZpLMFT\nTz3FJ598AkBaWhp79uwhLMy5CBgfH8/w4cMBGDVqFKmpqSctXnFs8kur+NdXO/nzjP5OVSstWbb1\nCO9tSGPLoSJumdqHXZnFLFp/sP747R/8QllVDTdM6cO+7FLeXH+A2jrNk9/sqT/n3Q1pKAV/WGTG\neb65/gAXjenJtzsz2XrIPAz9ml5I3F1fOsdcVk1+WQEAN769qVFsFjfFV3+cRHy4P+Mf+YbMokru\nOXMAT69M4alvU6iurWNQVCCf3jCB//2wn38s2QmYm/XQGNMmNi4hjNV7cvjy1yOcPSyK3JJK1u7N\nJTrYh0MF5SRG+vPA2QPp3z2QCX3CSM0pIyrYmxvf3sS1k3vXl5Q83d149fLRvPp9KomR/sSE+tC/\neyAAsaH2G/KkxAgmJZob+Wc3TCS7uLLRDRvA28PC6jumEOznUX/jBUiI8GfpH0/F0+JGREDjUppS\nClstXJCPR7MPhn27BfDjPdOaPNYSpRQT+oQ77Qvwdu3g1E6XFI72RH+y+Pn51b9etWoVK1asYN26\ndfj6+nLaaac1OdbAy8v+n9JisVBeXn5SYu1qSitrsLgpvD1a9xRWW6cpqaipb5AEeGz5Lt75KY2E\nCH8OF1QQFezNVRPjnerxtdY8/90+NqTm8Z8LhrNs6xHu/GgL3QJNvfGjy3ZRUmmvenl+wSj+sOhn\nAB74bFv9/oQIP/ZmlwIwc1B3vt6eQVVNLf5e7ozoGczPB/J5yiFp9A73Y1+OOX9MfCh/OXsgaXnl\nFJZXsXRrBmcN6cEdH/4KwL1nDiCvrIrbz+hHbkklkYHeAFw0xpRMTusXiVKKa94wJfM3rxyLh8WN\nKyfE4+/lgUYzJDrI6fv684x+VNXU8ddzBhHs60luSWX9E6+vp/12E+DtwZAYc+07C09p9L17uVu4\n7rSEVv0bAfh4WppMCDbNHYsOblxF29V1uqTgCgEBARQXN72qYWFhISEhIfj6+rJz507Wr19/kqMT\nNhXVtZz93+/x8bTw6Q0T8LDYe2TX1WkWvvkzw2KCqKip5exhUfTrFsDl//uJH1JyuHZyAvOSYknJ\nKuHzX0yD5MaD+SzZkgHAMytTGNUrhL+cPYgXVu91KgFMfORbiitrmJQYzsuXJfHJxkP1dfPzkmL4\n/ahYxsSHcu+ZA1i5K4u1e3N59uKRBHi7M7JnCEu2HMHbw0JUsDfLtmXw1bZMLhwdy9/nDqFOa35J\nK2DJlgz+PLMfBWXVPL1yD7ef0Y8Abw8sbopBUebme8HongBkFVeS1CvEqcHSlhAAbpmayA1T+uBh\ncWP6wG7867yhJgFYb+LuFjfmj+3Z5HfcK8yPly6111CEtdBOItofSQonQFhYGBMmTGDw4MH4+PjQ\nrZu9QWjmzJk8//zzDBgwgH79+nHKKY2fisSJU1pZQ/KB/Pr64RdX7+WD5HT+O38EK7Zn1j9F/++H\n/QyPDWFMfCiFZdU89MU2VuzIrO9i+Mr3+7l/9kDW7MkB4LlVe1m2NYPaOo1Sit7hfvUJYdqASA7m\nlfHNziy+2ZmF1uDjYWFITBA/7c+juLKGGYO68Z8LhuPlbuHcEdEcKaxgYmI4o+NC62O/5tTe9d0l\nBzs8gZ+fZBqNHdv/bjzdtCVYUCTFhZJkfZ/uQRb+du6Qo35HN0zpc9TjSik8LPZSj2Ojtej8Ol1D\nc2fXlf7Wpmit+XjjIWJCfOqfdDMKK+geZJ50H1y8jdfWpvLBH8YxOi6UU/7xDRlFFQyKCqS4oobY\nUB/S8so5mFcGwJ0z+7M7s5hPNtmHyHh7mBJERXUdPUN9OXd4FN/tyeGXNFMnf+v0vvh4WPj7EtOj\nZektkxjQI5DdmcU8unQnF4yO5ZSEMDwtbgx7aDmVNXWsuHUyfSJ/e7/9zWkFVFTXcspxdksUXZc0\nNItO44tfD+PlbiE21IfHvtpd/zT/xU0TKa+u5fzn19En0p8gHw82HswHTEngvQ1pZBRV0DvCj23W\nHmm3TE3kh5Sc+qTw6DLTYPr7UTGM6BnMvZ9sZWhMMNdNTuC+T7dy71kDmDGoO9dP6UP/+5cBMDg6\nkOhg3/qkEBdm2pD6dgvglctHO8W+6OqxrN+be0ISAsDwWBnoKNqWJAXhUg0HW1XV1OHpbp7U1+3N\n5aKX7G0wDQcHrd+XS3lVLQApWSVO72ur2gH470UjeGjxdhIi/ThraA/S800j/rWn9mbawG6s3p3N\nVRPjySmpAuDc4dFM6R/JD3edXv8ejg3Tg6OCnHqrHK3r4Oi4UKcqIiEoSANPP/B1+H9RUwWFaRDW\n+sb1ttIh5j4SnUtmUQV1dZq80ir63beMl9bsA+DX9AKGP7ycN9alAvB+cprTdXUaPCyKz2+cSHSw\nD5vSCvglvaD++G3T+wLw5IXD8fGwMDQmiJ/umcqgqCDe/8M4/vm7oXh7WJg/tidnDOzG1ZN6Mzou\nlNvO6Eewryd9Iv3ZcO80LhrTdB36oqvGMn9sTyIDvVFK8fe5g7nnzP4n/gsSdrl74fsnoK7u2K+t\nqYJVj0JlMdRWw6pHoDij5esc5e2H1f/X8ufnpMAPT5nPaShlBWz50Lze9x08MRjevRiO/AI/vWT2\nf/FHeHo0ZO1ofP1JJiUFcVIVV1Qz9h/fMCYulBG9TFXIC9/tY+GpCTy2fDdlVbU88Nk2Vu/O4WBe\naf11g6MD2XqoiGsm9WZITBDDYoP40jo6dmhMEON6h3HDlD5cOzkBT3c34sP9CPf3cupVYxMR4MWL\nlzZdtdpUf3WbiYnhTEy09ym/eGyv4/oOTpqSLPANA7cWut8WHQHvIMjeCd0Gw7aP4Zd34NLPGp9b\nnAkfXgnDL4IRC6AsD9y9zWdUFIF/hLkZVxaDXxPtHtXlUFMJeXthyR1w8YfgEwLFRyAwypxTWw1l\nuRDQHVb+HbZ+BEExMOT3R/87lt0DFYXm7wjobt7z0M8Q0gt8w2HVP2HXUpjzX3MTPu0eiBoO711i\nntBLMuGsf0PaT7DhZfj9/+DZcVBTDhZP2Py2+ZyIfhA1AnxCYcsH8LsXYcntsG8l7F8NuSngFQDn\nvQIHvocvbgU0fPtXyE8173FwLbxwqnm9/jnzfYD5e3//P1j6Z/NdTXsQ/LvBR1ebksWMf4KlbW/b\nkhREmymprGHboUKnro+2Pvc/pebVj6gtqqjm443prN6dza3T+/LVtoz6dgMPi+Jfvx/K578cAYqI\nDzf19+cMj+ZQgWlAvmxcHP26BwDg6WaqoWwDqjqs/WvAOxB6DDPbm9+Bkgw45Xpwb0U3z+pyeCwR\nkq6E2f+BbZ9C9CgIblAKytkDTyeBhy9Ul0HvKebmBlBeAD7W77GuDn59D354wtx0LR4mKTw+EIKi\nTfJJ+xEeyINPFsK2T+D81yF2jP1mrzW8fQGkb4DAaMjdA5sWmeMfXQVT/wK1VbB7GRzeBHcdNH8H\nmJukX4S5tsY6J5BPCIy+ynwf1RWQ/Kq5gTe07VP7zfjIZnh1JlSXwlvn2c85uNb8fsKh59bjDqXA\nrx+wv87eCdsdEuarM6HI2lEh5WvzW1ngpSlQVWK+90M/22M463H48lb79Xl7oed4iBwAP78G3zxs\n/hYwybnneHt8gVEw8U+N/8YTSJKCaBO1dZrZT60hNbeML26ayMAegWSXVLI/x7nu/9bpfXn8693c\n/fEW+kT6c91pCVwzqTc3v7uJr7dncvbQKOaOiCHQ24Nvd2bVd72cMag7MwZ1d8WfZqc11NW2/skt\n5Rv49X2Y+7zZrq0G92ZGRL8+2/xOPANm/AM+/YPZjhwEfc8wr2sqzQ0xc7t5wjznGfDwMU+1JSap\nkvwqnH4/fHAZeAfDXQecP2evNQFUm4b3+oQA5sbcY5h5Qt3/nT0GMDe5l6ebm3BuivkBeHI4FFrH\naHxwGYQmwHVrwcMb9n5j3gdMQgDY+QWEJ5rX3zzkHNv2z0z1UeQgKM+DN+ZYD9i6y2o48ANc+Bak\nrbcnhJA4iJsEm94027uXmt8R/U1JKGc3jL0WPrvB+fPiTzXfaUEaFJuxKJz7vPnbUteYJLfoPMiy\nDy5kwi3ww5Pm9e9ego+vgd6nmRv/uxdDTJJJyovOg+5DYMQlENkfokeakpXtb7rwLVO1lfwKrH3K\nJBI3d5NoD6411078E/Q7k7YmSeEEON6pswGeeOIJFi5ciK9v86MxO6Lth4tIzTU3mveT0zhcUM43\nO7MYHhuMm4I3rxrLur25LDy1N8+sTKGypo6Zg7rjYXHDwwKT+0bw9fZMqmpNXe7UAd3Y+48zsbi1\ng4WGtIY9y2HN45CzC+5MtR9L+QbiJpqb9c4l5mbbbbC5+S/6nTlnws2wawl8+ze45wh4+pobUXme\nvWRgs2c5aIf67ALrTX39c7DyH3DRu7D5LXNz1Rp2fQmn3wdegfZr/mWdcqWiwNyIMn41T62D5sK+\nVfbzTr3D3OBqTYM7b55rfl+3FjKscxqNWGCe3rd+BOk/mX1RI031zo7F9oRgk7cXHh8At2w2N3nv\nIHPTtD1pZ2x1ns1u/vv2OvitH0HePhh/o0kMH19tYjz9PnPusnvgx+fMd75pkbmJ6joYcDb0mWZP\nCjaz/wO9xtu3QxPgfzPN67sPgZdDD7GtH5mS0vCLnN/jjL+atoCKQnODjkkC5QYJp5tElLMHEqeb\n6qgbHAaqXrbY+X2iRjhv+4aan/6zzb/liAWmlJe7F358ASbf2XR1XBuQpHACFBQU8Oyzzx53Uliw\nYEGHTQoV1bV4ubtx/Vsb8fV059/zhlFbp9mTZUZ4h/p58sa6A7i7KcL9vdh0sAB/L3cm9Amvn/Nl\ndFwo36fkMC7B/p9+bLwpEUx0mBemXSQEMDdhx6fMmkrY+62pr974OoxZCJNug3cvavr65xxuTLkp\n0GMoPHuKqWr4SwHUNJgGJWWF/XVhmqnHX/lPqCyC1xyeHHdZ5zv69m/Nx578Cqz+N1QVw7pnTVKz\n6TnOJDLHJ2EwCSFzGwT0MKWRFQ86H79iqSkJvHKGebJtqDwPti82pZL4U83n2JJCdalJLkPmme8t\ndjT0nWHaFHZ8DnXVEN7XtCcERUOMQ5ffnmNh/TP2ZDvqChh2oSkR1DWYvfXyL50TAkA3hylxvBp0\nGR58Hk3qM9X8OJrm8H2cfm/T1zXnxmTnNp/zX4fU1RA/2WyHJcCZ/zq29/yNJCmcAHfddRd79+5l\n+PDhTJ8+ncjISN5//30qKyuZO3cuDz30EKWlpcybN4/09HRqa2u5//77yczM5PDhw0yZMoXw8HBW\nrlzZ8oe1I2VVNQx5cDn9uwew7XARAd7uHCroy10f/Vo/EvjVy0ezend2/Qjjc575gV4N5qE5c0gP\ndmYUOc0wmdgtgOT7phF2DBPOtVr2bvNk63mcifjwZuftjK2w5t+mvhtMD5PoFscIGTm7TVKoslar\nFRygvnpk9n9MVZAtAYXEmxLFmn9DZWHr3t8zwCQAME/oqx4xSSck3n7zn/Nf87Tdc5xzN0kwseTt\nh8ytpsQDpj3AZtLtJiGASQ7vX2qedBta82+T0CbdCqG9zT6/SCjNMq8Tp5uEYBMUay+xhPc1U6U2\nuqk7LNPSfQhMuQf8I+37xiw0iSx7F8SObRyTd2DjfSebrerMxuJuSh0u1PmSwtK77EXdE6X7EJj1\nSLOHH3nkEbZu3crmzZtZvnw5H374IT/99BNaa+bMmcPq1avJzs4mKiqKL780T3OFhYUEBQXx+OOP\ns3LlSsLDw5t9//Zi48F8Nh0sYOmWI9x6Rl/Kq2qprdNsO1yEt4cbxRU1THjk2/rzvT3cGB4b7DTg\n6subJxLg5TwL5EVjYpmXFFO/uItNS+sLHJeyPHhmtKnbPefpo59bnm96oUz4k3O7QWmDdcIPfG+q\nFGxydpnG1qZc9J45f+1/refugVqHp9r0ZFMnDubmmzDVJIXYsaZKquCAaYh1bBAG6D7UVAs1dNtO\n+Kf1Jj5gjr26aPxNplGz4CAMXwAjLzX7va3Ta4xZCMPnm545+1aZrpL9ZpljAda2nJ7jYOr99s9y\ns5gnbMekMOdpU8r48Tmz3XuK+fvmvmiqQxZZn8jD+zrH7dggHtbMtBwh9pmIuXYNNFxY6Mz/a/o6\nR1cub1xK6OI6X1JwseXLl7N8+XJGjDB1hiUlJezZs4dJkyZx2223ceeddzJ79mwmTZrk4kiPTVVN\nHb97dm399r+W7aK7tbvnyJ7BXDWxNze8vdHpmorqxn27bZOzOVJK4W5pg6qh4kzTY8XNmmy2fGh6\nuYB5Qm/OD0+ZroU+wabLYWhv5+qEhklh89v2p9qW9JkK/WbCuJvgyWHw3SP2UgLA4ptNlQqYp16L\nO9ydbuqtl/wZNi8yxwbMdk4KAd3h8i9MHfyrM+1VUF7+5mne4gF+Dg8evqFw1XJT9eXmkIynPmB6\n0px2tzknJM40snr4QpL1u/O03kSDm+iSO/h3pm59/TPg5gEjLzHVRj8+Z27iodYb+bALTEKyaXjj\nD7JOuOffzd4DqiE3Nxj7B1OqOMaV5ur1bKIE0cV1vqRwlCf6k0Frzd133821117b6NjGjRtZsmQJ\n9913H1OnTuWBBx5o4h3an/T8MiY+6ly1tdk6D9DcEdH854Lh1NZpvD3c8PV0J6/U3CD7dmujJ7Ca\nKtNVb8SC5vvg5+2Dp0bAGX9LR9M+AAAgAElEQVQ3DZVg+qnbhFpHju780txQc/fC0HmQfwC+vt/5\nvX5939yk/CNMr5u8fc7Hs3c2H6tj9Y2bu7k5AwR0g96TzVP/OmuJZdQVUFUKW943237WqhAv092W\nEIebsK0KxsbDxzzlR42AW3fYG5fB/jR/wJ7U8Qkx13g0mDo6oh8sXGXftiWFcTdCoFkek/jJph59\n1BVN/83+Ec717D3Hme8h8YwG5zn0Hmv4tG4rKTQsQTQ069GjHxfHrPMlBRdwnDp7xowZ3H///Vx8\n8cX4+/tz6NAhPDw8qKmpITQ0lAULFhAcHMzLL7/sdG17rj76eKPpg+3p7sY1k+L58Od0MovM9MuP\nnjcUMI3AWx+cgcVNUVOn2ZVRTLcmBo4dl7paQJnGQ3dPWPMYfPeoqRMeNNf5XK3NeZnbzfbye6Ew\n3dRd5+23n1dVYgZYvTvfvm/zW6aqxN3bPKHaevrsXmZ+mqIsoGvNk7xuYtSrfwTkWZOCajCBwLnP\nmYTywRXmSfeMv4KHn0NScF4JzKlnkmPVSf13ZOXTzOpfvg7/x3xaOfVG/GSTCMffZN/n5nZsfeU9\nvOEPq+1Jzqa57rhgnv6h+aoj0WYkKZwAjlNnz5o1i/nz5zNu3DgA/P39WbRoESkpKdxxxx24ubnh\n4eHBc8+ZOtaFCxcyc+ZMoqKi2k1D84c/p1NeXcslp5gn08W/HGZsfCjvXWv+pgBvDx5ZupPzRsXU\nz1ME1LcJeFiU09TPv9nDDjewa9eYmxRg76/u4KeXYOkdcIZDDxxbfTbAyMtM/Xh+qqkmcmSrbx+z\n0IwGtiWFhgKj7YOVYseafuRxE02VkyOLl0ku9SWLBvH6hprG09t3Oe/39DdJq+FN07HxOigW5n9g\n2jz2fOXc26a5qpSG1UetMfR88/NbNSzZ2Mx+wp4AHHn5m66nfY5vRTNx/CQpnCBvv/220/Ytt9zi\ntJ2QkMCMGTMaXXfTTTdx0003NdrvKilZxdz+gWk0HR4TzF+/3E5KVgnnz7KP7rx8fByhfp6cNzLm\nt32Y1qZKp/9Zjfvng+lpY2uQtdnygb0uevNbpu582IVmROvqf5leLmCmKmhKaLy5oaf81HTD7NwX\nTeljdYNugBcsgvcWmN/dh8IzY8xnx442SaHPNHtSuOxz85TvFQDL7ra/R8OSQnNu3gRFhxvvd+yn\nbnE3g9gi+8MTX8HQC5zPvWFD45HP3g51882VJk62pGaqoMCMSRAnnSQF4cRxxbDr3/6ZtDwzSjQp\nzn4T8fawMC/pBCy8UlFgqoE2vAJ/3ut8LG8/fHC5mZbAUer3pv4fzMCuPcvhk2uh10TTq8fmkHOj\nd72AKHsPG0ch8Sa5DLPeXG1Prz3Hm5u8xR3uy3Z4erc+jfedaWIdNNdUR0WNMH3x6z+vh/11a5OC\nf6Rz10pHs/9jEqBNcE8ztqFh6SCiibp4xwZlj445Lka0PUkKot7Gg/m8n5zGwB6BbD9SVJ8QgBNb\nHWRTkNb0fq3hqeFNHzvczM3eMSEAFKU3fV5kfzi4zr4952n4/GbTVz/eoUdYkLUUVFdt747qWJ1T\nv6J7DFxgHTlrG2nraPyNZnzAu9YJ5H6rpCsb7zuenjfH21tHdHqdJilorZ0WTu+MTvQqeZU1tfzp\nvc0kRPiTmltWv/bwvKQYXlqzn0MFJincPDURL/fWLXR/TAqtSaHhk3tTVSdgegyd/5p54n7hVNPA\n21rxk82slf4R9s9LnGG6TPadafY7stW/+zYztUBglBmN3FSpw1FwT/NzZ6rz1BNCtFOdIil4e3uT\nm5tLWFhYp00MWmtyc3Px9j5BPXqAd39Kc1qMBsyAsXNHRLN6Tw6HCsqZMagbt05voVtgU+rqTJ/6\nQb9rfnBQQRNJoSQbvrqn6fODos3oXzA9j8rznY9bPJseLxASD1Putd/4bSNZbfXqDRMCmHaDGf9s\nfrrmSz4xo4BbSgo27aUO/4qlpturEM3oFEkhJiaG9PR0srOzWz65A/P29iYm5rc17tbU1qGUoqi8\nmqe+2dPo+KKrxxDs60liN3++3ZlFZMBxJqG0H2HxTWZwVEmmmRZ48HmmIbi22szAaWv4tHiYSb82\nLTIzRdqmPmjIsUujrdQ0ZqEZPFZVAsMuMu9hK0HYpoMePt95kJJt8FXDPvqOlIJxR5nLKrjn0RtJ\n26uGU0UI0UCnSAoeHh7Ex8e3fKJg6EPLGR0XSs9QXwrLq/n73MHc+8nW+uO29YZ7WMcYVNcex4pX\nefvMxGtg5gP69T3zevB5Zq74hg5vMt1MG05i1pB/N/tr27nRSWbtgewdpgfTnKfgQevTe9RI09bQ\n8GneNh+/NLYK0Ygsx9mF1NZpyqpq+W53Nu9uOMgFo2O5eGwvUv4+q/4c21rEtoblQVHHUQ/+2mzT\ndRTM5HAtBlZlBn4NtlbV9JpgplBuyLGax5YUfEPt9f+234PmmgFotpHA3g2mSahPCkcpKQjRRXWK\nkoJoncMF9t5E1bWa66eY0aLuFjduOr0Pbg7tMUlxoSz/06kkRh7DVBX5B+DJoc77ShzaLGoa1PcP\nu8iMCN68yHTd7DcLtn5oGoX7zoAH8uHhEPu5iQ7jPGwjeH1C7CN/bb9//z9TvfTOhWa7YZvG6Ksg\ncwuMa7DIihBCkkJXsGJ7JqVVNdz5kX2wllIQHWx/Ur7tjH6NruvbLeDYPsixu2jkQHPz/fI2+75l\nd9pfD5prViBbZZ2rKjDKNErXVNonn3NzMyOG0360r1ZmU2ddIN0nxKGnULj9j1MKsLY7NBwf4BsK\n8944tr9NiC5CkkInlFVUQU2dJirYh7KqGq5+I7nROZeechyLzv/4opkK4iLr6O2KInPD3bfKLITu\nOLfO2GvNylSOScG27ixAmHUeeVt9v3ewSQIjLnb+zMs+t1f3NMU31KGk0GD+KFuPH6kmEqLVJCl0\nQgvf/JnKmjo+u2EC6/flNjr+1R9PJSHC79jfeKl12oG6OnMDfyTWTKw28BwznbTjlNIh8c2Pyh0w\nB0ZdZl57WuNobsETd6+mF6pPnGHm/PEKMr2LfMMaz+cz61GzFoZtFSshRIskKXQyKVnF9dNa971v\naf3+0/tHMjQmiJpaTb/uragW+vJ2M59Pn6mw/H7nZRYLD9rn0i/Pg/z9ja+3LRTTlNPuto8Yrra2\nc7S2v7/N+a+Zrq5ubua9Rl/V+ByfEOfZPYUQLZKk0MnYprl29Mdpifxx2jEMQCvJhg0vmZ+mPDnM\nTA9hk7XD+fjIS00/fjCDvPatMgvC2ziOErZV/XQf0vr4wCylGSrdkIU40do0KSilZgJPAhbgZa31\nIw2O9wReB4Kt59yltV7SljF1ZnV1ms82H+bUvhF4WtyYOiCSET2D6d/9GLuVHmrcBtHIz6/ZX5dk\nOh+b+Yh9bp2E081Pn+nw+myzz7GaZ9Bcs7JW7ynHFqMQok20WVJQSlmAZ4DpQDqwQSm1WGu93eG0\n+4D3tdbPKaUGAkuAuLaKqTP7aX8eG1LzOFRQzp9n9uOc4dEtX+TINt9QYJRZJ7glDRNHUKx9LiP3\nJhp24yfZF6KxOKzRrJTLFyoXQti1ZUlhDJCitd4HoJR6FzgHcEwKGrA9xgYBzcyEJloy7wUz86dS\ncMbA7i2c3YRPrzMLz5z/unMvIZvopKOXIGLH2JOCWzNjIm/e5Lz6mRCi3WnLEc3RgOPcyOnWfY4e\nBBYopdIxpYQmWwWVUguVUslKqeTOPr/Rsdp2uJC4u76s3x4dF4qPZwszmubth5wU05j8WF/Ytcws\nXJOfCm/ONXMHTb7L+ZrT7oY79sLY65yngP7TNrhpoxmF3JKQOEiQaiIh2jNXNzRfBLymtf63Umoc\n8KZSarDWzovdaq1fBF4ESEpKOrHzR3dwS7YcqX89f2xPbj49sfmTM7fB+mfNpHGAWShGwy/vmMZl\nML2Jkq6ExOnwnUMTkK91kNgs676Rl0HmVnsvovolMoUQHVlbJoVDgOPyXDHWfY6uAmYCaK3XKaW8\ngXCgmWkyhc07Px3k38t3kVNinzrirln9CfT2aP6i7x6F7Z+Z7p8VhdSP+N21FGorzYjitB9h1BVm\niurr18Ozp5hzGk79HDvG/Nh4Hse4ByFEu9OW1UcbgESlVLxSyhO4EFjc4JyDwFQApdQAwBuQ+qEW\nrNqVxSebDtUnhAAvdx753ZCmE0J+KmRsgd3LIWMr9J0FVyyzHw/vaxICmDEJt263r1kQOcB+nk8L\nC717HsMcSUKIdqvNSgpa6xql1I3AV5jupq9qrbcppR4GkrXWi4HbgJeUUn/CPLZerk/08mKdTGpO\nKZf/b4PTvt6R/lw4pmfTFzw5zHm7/1n2sQEAiWdAzm7z2q+ZEcjQ8qphzS2kI4ToUNq0TcE65mBJ\ng30POLzeDrSihVLYpOWX1b++amI8r3y/nzFxzazq1dQKW+F9nccJRI+0v25qBTKb5noU2UhJQYhO\nwdUNzeIYpefbp7+e2Cec+WN7EhPSzIRvqT803hcaD24OvZMcSw1NlRQiB0HWtpYDk6QgRKcgSaGD\nScszJYWZg7oztncovp5N/BNWlphBYtkNpp/oNQGiRjjv8w2zL1vp10RJ4ZpvW14RDaT6SIhOQpJC\nB5OeX05sqA/PXzKq6RPq6uD1syE3BeJPte/vMw0WfNT4fN8wuHYNHFwH7p6Nj3u0co1mD+l9JERn\nIMtxdjBp+WXEhjisLZx/AN6+AMrNzKjsWGwWu6ksgp1fmK6kfaaZuYea4hMK4X1g5CW/LTCLO3gG\nwLSHftv7CCFcSkoKHUhRRTU7jxRzflKMfefSO2H3Mlj3NAw5H9I3mLmH/MLNtBNegU2XEIJ7QcGB\npksHx+ue9BP3XkIIl5Ck0I5tPVTIoKhAlFJorfkgOZ3y6lrmJcWa6ajTk001EcDq/zM/fWdBULRp\n+C1Ma36dgmu+haLG02wLIbo2SQrt1Lq9uVz00noemjOIsb1DufilH8ktrWJ0XAiDo4PgwXOavnD3\nUtOWoKw9jJpLCn7hjZevFEJ0eZIU2qm8UjNa+f++2kW3QC9yrdutWiwnIMq+sP2xrmgmhOjSJCm0\nU6WVphtoSWUNPhUWnl8wkjB/L0bHhUJFUdMX2dY0CIyCymKzr6WRyEII4UB6H7VTeWX2ie7unz2Q\nmYN7mIQA9uUvRyyAy7+09yzqNd789gu3T2An4weEEMdASgrtVL61uuh3I6KZNbjBojlHNpvfp91j\nGpWjRkBpNmx6y36OT/BJilQI0ZlIUmiH/rl0By+s3kePIG8ev2B44xPSk027QZB1zSJPP/Mz/iYz\nJfbIS2HzO+ZYXe3JC1wI0eFJUmhntNa88N0+AEL9mhlDkL4BYpIa7/cOhDP/ZV7bJrDTkhSEEK0n\nbQrtTEZRRf1rpRwO1FabdRHK8iB/f9NJwZFtcjv/41ivWQjRZUlSaEfq6jTPrEyp384sqrQf/Pav\n8PxE2PO12Q5voWvqgLNh7osw6bY2iFQI0VlJ9VE7sn5fLovWH6zfLq6oth88+KP5vcO6eF2Q40qn\nTVAKhl1wgiMUQnR2khTakYPWabE/uX48n20+zIxB1qqf7Z+ZKiMwk9wBBLeQFIQQ4jhIUmhHjhRW\noBQMjg5iRE/rOIMjv8L7lzqf6BUkI5WFEG1C2hTakYzCCiL8vfCwOPyzfPeo6wISQnQ5khTakSNF\nFfQIcljUpjTXTIvdZ5rZPvsp87uy8OQHJ4ToEqT6qB05UlBO7wjrCmZaw8q/m6Uwpz0I8983aysX\nHIDuQ10ZphCiE5Ok0A68vGYf725I42BuGRP6WKezTvkGkl+BsddB9yH2k6c+4JoghRBdgiSFduBv\nX+6of31GzSp44XoISzCT2k1/2HWBCSG6HEkKLpaSVVL/+pzhUYz/db7ZyNhiZkE9kctlCiFECyQp\nuNCsJ9ewL9skhRW3TqZPpD88aD2oayEkzlWhCSG6KEkKLlJVU8eOI/bFcmJCfBqf5BVwEiMSQgjp\nkuoy+3Ls1UYRAV54e1hMjyNHsmqaEOIkk6TgAlU1dTy7cm/9dphtiuyyPOcTZdU0IcRJJknBBT7d\ndIjFvxyu366zlRAKDzqfKNVHQoiTTJKCC6TmlgJw7vAoAGrrrEnBtlqajSQFIcRJJknBBdLyy+kV\n5sv9swcCcO7waNjyIWx4GRJn2E+UNgUhxEkmvY9cID2/jJgQH8L8vdh0/3SCfDzgqfOgx1A4+0l4\nvL85UUoKQoiTTEoKLpCWV05siC8AIX6euJVlmzmNBp8H/t3sJ0pSEEKcZK1KCkqpj5VSZymljimJ\nKKVmKqV2KaVSlFJ3NXPOPKXUdqXUNqXU28fy/h1ReVUtOSWVzuMS0pPN75jR4ObwFbt7I4QQJ1Nr\nb/LPAvOBPUqpR5RS/Vq6QCllAZ4BZgEDgYuUUgMbnJMI3A1M0FoPAv54LMF3RJvS8gFIiHDobnoo\nGdzcoccw55OVOomRCSFEK5OC1nqF1vpiYCSQCqxQSq1VSl2hlPJo5rIxQIrWep/Wugp4FzinwTnX\nAM9orfOtn5N1PH9ER/JhcjoBXu6c1i/SvjNjC4T3A48mRjULIcRJ1OrqIKVUGHA5cDWwCXgSkyS+\nbuaSaCDNYTvdus9RX6CvUuoHpdR6pdTMZj57oVIqWSmVnJ2d3dqQ253iimqWbD3C2cOj8PG02A9k\nbIXug10XmBBCWLWq95FS6hOgH/AmcLbW+oj10HtKqeTf+PmJwGlADLBaKTVEa13geJLW+kXgRYCk\npCTd8E06ii9+PUJFdR3zkmLtO8vyoPgwdBvkusCEEMKqtV1Sn9Jar2zqgNY6qZlrDgEOdz9irPsc\npQM/aq2rgf1Kqd2YJLGhlXF1KO8np5EY6c+wmCD7zsyt5nc3h5LCHfuADpv7hBAdWGurjwYqpYJt\nG0qpEKXU9S1cswFIVErFK6U8gQuBxQ3O+RRTSkApFY6pTtrXypg6lJSsYjYdLGBeUiyqthr2fA11\ndZCzx5wQ4dB27xcGfuGuCVQI0aW1Nilc41ilY20YvuZoF2ita4Abga+AHcD7WuttSqmHlVJzrKd9\nBeQqpbYDK4E7tNa5x/pHdASvfJ+Kp8WNc0dEwy/vwFu/h4+uMknB4gUBUa4OUQghWl19ZFFKKa3N\nzG3W7qYtLgmmtV4CLGmw7wGH1xq41frTab28Zh8fJKcxf2xPIgK8IGsHKDfY9rE5Ibyv8/gEIYRw\nkdbeiZZhGpWnKqWmAu9Y94kWVNbU8o8lO+gZ5sufpvU1O3P3mDaEgdYeurLCmhCinWhtUrgTU71z\nnfXnG+DPbRVUZ5KaU0adhlumJhJiWzchZ7cpHSRMNds1la4LUAghHLSq+khrXQc8Z/0RxyAly6yw\n1ifSOoK5qgwK0mD4Aug92eyLaa4DlxBCnFytHaeQCPwTM11F/YQ8WuvebRRXp5GSVYJS0DvcmhSK\nDgHaVBmFxMGNP0NILxdGKIQQdq2tPvofppRQA0wB3gAWtVVQncmOI0VEB/vYRzDbltz0DTO/w/uA\npbmZQoQQ4uRqbVLw0Vp/Ayit9QGt9YPAWW0XVseXUVjBmL+vYNm2DGYM6m4/UG5LCiGuCUwIIY6i\ntV1SK63TZu9RSt2IGZksq8ofxa/pBWQVVxLg5c6fpve1Hyg3s6TiI0lBCNH+tLakcAvgC9wMjAIW\nAJe1VVCdQWax6VH0zW2T8fdyyL226iOfUBdEJYQQR9diScE6UO0CrfXtQAlwRZtH1QlkFVXgpiDM\n3wtKssE7ENy9TElBWcA7qOU3EUKIk6zFkoLWuhaYeBJi6VQyiyqICPDCQh081gc+vNIcKM8Dn2BZ\nQEcI0S61tk1hk1JqMfABUGrbqbX+uE2i6gQyiyrpFugN2TvNjp1fmN9leVJ1JIRot1qbFLyBXOB0\nh30akKTQjKziSqKDfSDdYRZwrU31ka8kBSFE+9TaEc3SjnCMsooqGNkz2DkpvHU+ZO+CqOGuC0wI\nIY6itSOa/0cTq75ora884RF1AvmlVeSWVhEd7A2/rDbVReV5kGJdudRX1koQQrRPra0++sLhtTcw\nFzh84sPpHH7cb5aEmBRWDAUHYdb/gX+ESQ7pG2DguS6OUAghmtba6qOPHLeVUu8A37dJRJ3Aur25\n+HhYGFC+yexION1MZwH2SfCEEKIdOt6VXRKByBMZSGeRU1LJZ78cZkKfMNzz9oCHL4QluDosIYRo\nlda2KRTj3KaQgVljQTTw5roDFJVXc+fM/rAyDYJiZUyCEKLDaG31UUBbB9JZ7M8pJTrEh8RuAVCY\nBsGxrg5JCCFarVXVR0qpuUqpIIftYKWUtJY24VBBuRmfAGYxnSBJCkKIjqO1bQp/0VoX2ja01gXA\nX9ompI7tUH450cG+UFVquqFKSUEI0YG0Nik0dV5ru7N2GVU1dWQWVxAd4mNKCQBBPV0blBBCHIPW\nJoVkpdTjSqkE68/jwM9tGVhHlFFYgdYQE+xj2hNASgpCiA6ltUnhJqAKeA94F6gAbmiroDqqA3lm\nrkBTUjhodkqbghCiA2lt76NS4K42jqVDe+2H/fxjyU683N0YlfoCrP0PuLlDQPeWLxZCiHaitb2P\nvlZKBTtshyilvmq7sDqeBz/fTlVtHRfE5uP9/b+grhoCo8HN4urQhBCi1VpbfRRu7XEEgNY6HxnR\nXK+yprb+9W3hP9oPWDxdEI0QQhy/1iaFOqVUfTcapVQcTcya2lUdyi8H4PF5wwjK2QwefuZASaYL\noxJCiGPX2qRwL/C9UupNpdQi4Dvg7rYLq2M5mFcGQFyggsytkGRdfmLYRS6MSgghjl1rG5qXKaWS\ngIXAJuBToLwtA+tI0qxJIb5yF9TVQK/xcOod4Onv4siEEOLYtHZCvKuBW4AYYDNwCrAO5+U5u6zt\nR4rx9nAjOHUJuHtD3ETwDmr5QiGEaGdaW310CzAaOKC1ngKMAAqOfknXkFVUwccb07movwW19UPo\nd6YkBCFEh9XapFChta4AUEp5aa13Av3aLqyO49udWVTW1HFH2RNQWwMTbnF1SEIIcdxamxTSreMU\nPgW+Vkp9Bhxo6SKl1Eyl1C6lVIpSqtnBb0qp85RS2tpu0aGkZJXg5e6GT/EBGDgHooa7OiQhhDhu\nrW1onmt9+aBSaiUQBCw72jVKKQvwDDAdSAc2KKUWa623NzgvAFM99WPjd2n/UrJL6B3hjyrOBd9Q\nV4cjhBC/yTEvx6m1/k5rvVhrXdXCqWOAFK31Puu57wLnNHHeX4FHMfMpdTh7s0sYEO4ONeXgG+bq\ncIQQ4jc53jWaWyMaSHPYTrfuq6eUGgnEaq2/PNobKaUWKqWSlVLJ2dnZJz7S4/T9nhzS8soZFFRt\ndvhISUEI0bG1ZVI4KqWUG/A4cFtL52qtX9RaJ2mtkyIiIto+uFYorqjm2jeT6R7ozRnxHmanlBSE\nEB1cWyaFQ4DjvNEx1n02AcBgYJVSKhUz9mFxR2ls/njjIUqrannhklHEepnBa5IUhBAdXVsmhQ1A\nolIqXinlCVwILLYd1FoXaq3DtdZxWus4YD0wR2ud3IYxnTCrd2eTEOHHsMpkKLLmOkkKQogOrs2W\n1NRa1yilbgS+AizAq1rrbUqph4FkrfXio79D+7Yrs5jp3ctg0cX2nZIUhBAdXJuus6y1XgIsabDv\ngWbOPa0tYzmRSitrSM8vZ0C/aoe9CnyCm71GCCE6Apc1NHdke7JKAOjj5zAnoF+4LKgjhOjwJCkc\nh10ZRQDEepbYd/aQkcxCiI5PksJx2HiggGBfD8K0w5yA0aNcF5AQQpwgkhSOw4YDeYzqGYJbmcNA\nurAE1wUkhBAniCSFY/RLWgH7sksZFRcCJVngHQzjboSBTc3gIYQQHYskhWNQV6e5+o1kooK8OWd4\ntEkK3QbBjL+Du5erwxNCiN9MksIx2JFRRHZxJX8bW010XQZk7wT/bq4OSwghTpg2HafQ2azbm0ui\nSuf01X+G1YBnAIy/ydVhCSHECSMlhVban1PKi6v3MSq41L5z0LkQPdJ1QQkhxAkmSaGVPt6YTk5J\nJbeM9rXvTDjddQEJIUQbkKTQSilZJfQK86MHOWZHSDwkTHFtUEIIcYJJm0Ir7c0uISHCDwoPQUAU\n3LLZ1SEJIcQJJyWFVqiprWN/TikJkf5QmAZBMa4OSQgh2oQkhVbYfqSI6lpNnwh/s3ZCUHTLFwkh\nRAckSaEFWmtu/+AXwvw8mZwYbqqPpKQghOikJCm0IKu4kt2ZJdwwpQ+RlhKorYRASQpCiM5JkkIL\nth8x02QPigqEonSzU0oKQohOSpJCC3YeKQagf/dAKLQlBWlTEEJ0TpIUWrDjSBHRwT4EqVL4+TWz\nU6qPhBCdlCSFo9Ba89P+PIbFBsGqRyBlhTngF+7awIQQoo1IUjiKXZnFZBRVMLlvBBQfsR9QynVB\nCSFEG5KkcBTf7TIrq03uGwkFB8EvEq78ysVRCSFE25GkcBTf7c6mfzd/un96PhzeaGZF7XmKq8MS\nQog2I0mhGSWVNWxIzeP3sYWwf7XZGSi9joQQnZskhWb8fCCf6lrNVM/tZoeyQOIZrg1KCCHamMyS\n2owd1kFrMXnrIaI/3PCjiyMSQoi2JyWFZuw4UkRcoBse6etkMR0hRJchSaEZOw8XcZvP51BTAb1l\nMR0hRNcg1UdNqNj8IQ8XPMZYtx3g7gNxE1wdkhBCnBSSFJrg/elVjHWDioBeeF+7Ajz9XB2SEEKc\nFFJ91IQK5QOAx2Wfgn+ki6MRQoiTR5JCQ5UleOtyPg1fiCW8t6ujEUKIk0qSQgOV+WZ6bK9QmQlV\nCNH1tGlSUErNVErtUkqlKKXuauL4rUqp7UqpX5VS3yilerVlPK1xJG0vAEHdXB6KEEKcdG2WFJRS\nFuAZYBYwELhIKTWwwWmbgCSt9VDgQ+BfbRVPa+UfOQBAt5h4F0cihBAnX1uWFMYAKVrrfVrrKuBd\n4BzHE7TWK7XWZdbN9XHGZCgAAAsaSURBVIDL62xKc0xSiI5NcHEkQghx8rVlUogG0hy20637mnMV\nsLSpA0qphUqpZKVUcnZ29gkMsbHqnP3kq2C8ff3b9HOEEKI9ahcNzUqpBUAS8H9NHddav6i1TtJa\nJ0VERLRZHKk5pfiX7KfIL67NPkMIIdqztkwKh4BYh+0Y6z4nSqlpwL3AHK11ZRvGc1RFFdWc9tgq\nEtRhLJH9XBWGEEK4VFsmhQ1AolIqXinlCVwILHY8QSk1AngBkxCy2jCWFh3IKSOEIkJVCd0Throy\nFCGEcJk2Swpa6xrgRuArYAfwvtZ6m1LqYaXUHOtp/wf4Ax8opTYrpRY383ZtLj2/jD7qMADuUlIQ\nQnRRbTr3kdZ6CbCkwb4HHF5Pa8vPPxZp+WWMctttNroPcW0wQgjhIu2iobk9SMsrZ4rHVogcBAHd\nXR2OEEK4hCQFq+zcXEayExJk7QQhRNclScGqV/a3eFAD/We7OhQhhHAZSQpA9S8fcHfFExR4RUHP\nU1wdjhBCuIwkBaDsV9Ppadewu0ApF0cjhBCuI0kBqCrMJLmuL0Ej57o6FCGEcKkunxRSc0rJz0on\nRwcRHy7LbgohurYunxQW/3KYCFVIaLcYvNwtrg5HCCFcqssnhU2p2YSoEsYM7u/qUIQQwuW6dFKo\nrdMcOGjWT8Cv7WZfFUKIjqJLJ4VfUjPxqco1G/6Rrg1GCCHagTad+6hd05qoD87iS68Us+3fzbXx\nCCFEO9BlSwp12XvoXm5NCB6+ECJrMgshRJdNCgeTvwTg26mfw5/3gb+0KQghRJesPvp6eyaV65bi\n7hbO2DHjwaNLfg1CCNFIlyspFFdUc/fHv9LPcgT3HoPx85KEIIQQNl0uKXy3O5u8kgoS3DLo3lsW\n0xFCCEddLynsyqa/dwFudVUQ3tfV4QghRLvS5ZLCmj05zI4qMhuSFIQQwkmXqlDPKqqgpCiPi91f\nAA8/iBzg6pCEEKJd6VJJYevhQmZb1hNUdgAWfATeQa4OSQgh2pUuVX209VAR51nWUBeWCAlTXR2O\nEEK0O10qKVTu+Y7RbrtwG3WZrLAmhBBN6DJJQWvNqRmvUeAeAaOvdnU4QgjRLnWZpHAgdQ+j9VbS\n4s8HDx9XhyOEEO1Sl0kKRT++jZvSBIyZ7+pQhBCi3eoyvY8Kes/m9RzFJQmDXR2KEEK0W10mKZw6\nJolTxyS5OgwhhGjXukz1kRBC/H979x8jV1WGcfz7WNoFKaEWkDSU0BZIsBqsVREFCYGoUI2FpIYq\nYmNISBQSiTHSBlQk8Q9MFDVpLKiVAlUqSGNDYhTapoY/aCmwLVugsEIT21Q2KlRrYpX29Y9z5nY6\nnZn9AffegXk+yWbvvXN35pl3dvbsPXPnHRudBwUzMyt4UDAzs4IHBTMzK3hQMDOzQqmDgqTLJO2U\nNCxpaZvLByStyZdvljSrzDxmZtZdaYOCpEnAcuByYC7weUlzW3a7Fng1Is4C7gBuLyuPmZmNrswj\nhfOA4Yh4KSL+C9wPLGzZZyGwKi8/CFwquVOdmVldynzz2mnAX5rWdwMf6bRPRLwuaR9wEvC35p0k\nXQdcl1f3S9o5wUwnt153j+jVXNC72ZxrfJxrfN6Ouc4Yy05viXc0R8RdwF1v9HokbY2Inntbc6/m\ngt7N5lzj41zj08+5ypw+2gOc3rQ+M29ru4+kY4ATgb+XmMnMzLooc1B4Ajhb0mxJU4DFwLqWfdYB\nS/LyImBDRESJmczMrIvSpo/yawQ3AH8AJgErI2KHpNuArRGxDvgFcK+kYeAfpIGjTG94CqokvZoL\nejebc42Pc41P3+aS/zE3M7MGv6PZzMwKHhTMzKzQN4PCaC03Ks6yS9IzkgYlbc3bpkt6RNKL+fu7\nKsixUtKIpKGmbW1zKPlJrt92SfMrznWrpD25ZoOSFjRdtizn2inpUyXmOl3SRknPStoh6Wt5e601\n65Kr1ppJOlbSFknbcq7v5u2zc1ub4dzmZkreXlnbmy7Z7pb0clPN5uXtVf7+T5L0tKSH83q19YqI\nt/0X6YXuPwNzgCnANmBujXl2ASe3bPs+sDQvLwVuryDHRcB8YGi0HMAC4PeAgPOBzRXnuhX4Rpt9\n5+bHcwCYnR/nSSXlmgHMz8snAC/k26+1Zl1y1VqzfL+n5uXJwOZch98Ai/P2FcBX8vJXgRV5eTGw\npsTfsU7Z7gYWtdm/yt//rwO/Ah7O65XWq1+OFMbScqNuzS0/VgFXlH2DEfEn0llfY8mxELgnkseB\naZJmVJirk4XA/RFxICJeBoZJj3cZufZGxFN5+V/Ac6R35ddasy65OqmkZvl+78+rk/NXAJeQ2trA\n0fWqpO1Nl2ydVPJYSpoJfBr4eV4XFderXwaFdi03uj1pyhbAHyU9qdTCA+DUiNibl/8KnFpPtI45\neqGGN+RD95VN02u15MqH6h8g/YfZMzVryQU11yxPhQwCI8AjpKOS1yLi9Ta3fUTbG6DR9qYUrdki\nolGz7+Wa3SFpoDVbm9xvph8B3wQO5fWTqLhe/TIo9JoLI2I+qYPs9ZIuar4w0vFg7ecK90qO7KfA\nmcA8YC/wg7qCSJoK/Ba4MSL+2XxZnTVrk6v2mkXEwYiYR+pocB5wTtUZOmnNJul9wDJSxg8D04Gb\nqsoj6TPASEQ8WdVtttMvg8JYWm5UJiL25O8jwFrSk+WVxuFo/j5SU7xOOWqtYUS8kp/Eh4CfcXi6\no9JckiaT/vCujoiH8ubaa9YuV6/ULGd5DdgIfJQ09dJ442zzbdfS9qYp22V5Ki4i4gDwS6qt2QXA\nZyXtIk1xXwL8mIrr1S+DwlhablRC0vGSTmgsA58Ehjiy5ccS4Hd15OuSYx3wpXwWxvnAvqYpk9K1\nzN9eSapZI9fifCbGbOBsYEtJGUR6F/5zEfHDpotqrVmnXHXXTNIpkqbl5eOAT5Be79hIamsDR9er\nkrY3HbI93zS4izR331yzUh/LiFgWETMjYhbpb9SGiLiaquv1Zrxa/Vb4Ip098AJpTvPmGnPMIZ35\nsQ3Y0chCmgtcD7wIPApMryDLr0nTCv8jzVVe2ykH6ayL5bl+zwAfqjjXvfl2t+cnw4ym/W/OuXYC\nl5eY60LS1NB2YDB/Lai7Zl1y1Voz4Fzg6Xz7Q8C3m54DW0gvcD8ADOTtx+b14Xz5nBIfy07ZNuSa\nDQH3cfgMpcp+//PtXczhs48qrZfbXJiZWaFfpo/MzGwMPCiYmVnBg4KZmRU8KJiZWcGDgpmZFTwo\nmFVI0sWN7pdmvciDgpmZFTwomLUh6Yu53/6gpDtz87T9uUnaDknrJZ2S950n6fHcRG2tDn+ewlmS\nHlXq2f+UpDPz1U+V9KCk5yWtLqsTqNlEeFAwayHpPcBVwAWRGqYdBK4Gjge2RsR7gU3Ad/KP3APc\nFBHnkt7t2ti+GlgeEe8HPkZ6lzakLqY3kj7XYA6p541ZTzhm9F3M+s6lwAeBJ/I/8ceRmtwdAtbk\nfe4DHpJ0IjAtIjbl7auAB3J/q9MiYi1ARPwHIF/flojYndcHgVnAY+XfLbPReVAwO5qAVRGx7IiN\n0rda9ptoj5gDTcsH8fPQeoinj8yOth5YJOndUHwG8xmk50ujW+UXgMciYh/wqqSP5+3XAJsifQLa\nbklX5OsYkPTOSu+F2QT4PxSzFhHxrKRbSJ+O9w5St9brgX+TPozlFtJ00lX5R5YAK/If/ZeAL+ft\n1wB3SrotX8fnKrwbZhPiLqlmYyRpf0RMrTuHWZk8fWRmZgUfKZiZWcFHCmZmVvCgYGZmBQ8KZmZW\n8KBgZmYFDwpmZlb4P/DHpyOplKmEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOwszM8vlzsLMzHK5szAzs1z/A1qu06vRVLtuAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VElOwKJA8y9z",
        "colab_type": "text"
      },
      "source": [
        "**We see that the model does okay. It's loss keeps lowering but the accuracy bottoms out. It thus gets more confident but not more accurate. Accuracy bottoms out around 200 epochs.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb6FxpK_9x9m",
        "colab_type": "text"
      },
      "source": [
        "Perform KFold cross validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joPeDsDz2bMD",
        "colab_type": "code",
        "outputId": "68297742-18ce-49ea-e401-1a10cfe4a163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score \n",
        "\n",
        "\n",
        "# this wraps the network to put into sklearn,\n",
        "# so we can use cross_val_score method for kfold\n",
        "# cross validations\n",
        "neural_network = KerasClassifier(build_fn=create_model2, \n",
        "                                 epochs=400, \n",
        "                                 batch_size=100)\n",
        "\n",
        "# one hot encode labels_tc\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded_tc = label_encoder.fit_transform(labels_tc)\n",
        "\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded_tc = integer_encoded_tc.reshape(len(integer_encoded_tc), 1)\n",
        "onehot_encoded_tc = onehot_encoder.fit_transform(integer_encoded_tc)\n",
        "\n",
        "cross_score = cross_val_score(neural_network, features_tc, onehot_encoded_tc, cv=5)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.0758 - acc: 0.0525\n",
            "Epoch 2/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0577 - acc: 0.0725\n",
            "Epoch 3/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0552 - acc: 0.0962\n",
            "Epoch 4/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0542 - acc: 0.1113\n",
            "Epoch 5/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0531 - acc: 0.1225\n",
            "Epoch 6/400\n",
            "800/800 [==============================] - 0s 190us/step - loss: 0.0522 - acc: 0.1500\n",
            "Epoch 7/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0516 - acc: 0.1925\n",
            "Epoch 8/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0510 - acc: 0.2000\n",
            "Epoch 9/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0505 - acc: 0.2300\n",
            "Epoch 10/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0501 - acc: 0.2437\n",
            "Epoch 11/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0498 - acc: 0.2625\n",
            "Epoch 12/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0495 - acc: 0.2725\n",
            "Epoch 13/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0491 - acc: 0.3013\n",
            "Epoch 14/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0488 - acc: 0.3087\n",
            "Epoch 15/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0485 - acc: 0.3312\n",
            "Epoch 16/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0482 - acc: 0.3350\n",
            "Epoch 17/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0481 - acc: 0.3463\n",
            "Epoch 18/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0477 - acc: 0.3675\n",
            "Epoch 19/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0475 - acc: 0.3637\n",
            "Epoch 20/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0473 - acc: 0.3812\n",
            "Epoch 21/400\n",
            "800/800 [==============================] - 0s 197us/step - loss: 0.0470 - acc: 0.3975\n",
            "Epoch 22/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0468 - acc: 0.4113\n",
            "Epoch 23/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0465 - acc: 0.4175\n",
            "Epoch 24/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0463 - acc: 0.4325\n",
            "Epoch 25/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0461 - acc: 0.4475\n",
            "Epoch 26/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0459 - acc: 0.4600\n",
            "Epoch 27/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0457 - acc: 0.4712\n",
            "Epoch 28/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0454 - acc: 0.4825\n",
            "Epoch 29/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0453 - acc: 0.4913\n",
            "Epoch 30/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0452 - acc: 0.5000\n",
            "Epoch 31/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0449 - acc: 0.5225\n",
            "Epoch 32/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0447 - acc: 0.5237\n",
            "Epoch 33/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0445 - acc: 0.5287\n",
            "Epoch 34/400\n",
            "800/800 [==============================] - 0s 190us/step - loss: 0.0443 - acc: 0.5500\n",
            "Epoch 35/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0441 - acc: 0.5612\n",
            "Epoch 36/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0440 - acc: 0.5537\n",
            "Epoch 37/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0438 - acc: 0.5688\n",
            "Epoch 38/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0436 - acc: 0.5787\n",
            "Epoch 39/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0435 - acc: 0.5975\n",
            "Epoch 40/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0433 - acc: 0.6038\n",
            "Epoch 41/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0432 - acc: 0.6037\n",
            "Epoch 42/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0431 - acc: 0.6100\n",
            "Epoch 43/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0429 - acc: 0.6150\n",
            "Epoch 44/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0427 - acc: 0.6350\n",
            "Epoch 45/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0425 - acc: 0.6425\n",
            "Epoch 46/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0424 - acc: 0.6575\n",
            "Epoch 47/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0422 - acc: 0.6575\n",
            "Epoch 48/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0420 - acc: 0.6600\n",
            "Epoch 49/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0419 - acc: 0.6763\n",
            "Epoch 50/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0418 - acc: 0.6750\n",
            "Epoch 51/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0416 - acc: 0.6813\n",
            "Epoch 52/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0415 - acc: 0.6762\n",
            "Epoch 53/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0414 - acc: 0.7025\n",
            "Epoch 54/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0413 - acc: 0.6987\n",
            "Epoch 55/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0411 - acc: 0.7038\n",
            "Epoch 56/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0409 - acc: 0.7188\n",
            "Epoch 57/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0407 - acc: 0.7275\n",
            "Epoch 58/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0407 - acc: 0.7262\n",
            "Epoch 59/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0405 - acc: 0.7350\n",
            "Epoch 60/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0403 - acc: 0.7438\n",
            "Epoch 61/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0403 - acc: 0.7387\n",
            "Epoch 62/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0402 - acc: 0.7513\n",
            "Epoch 63/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0399 - acc: 0.7625\n",
            "Epoch 64/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0398 - acc: 0.7562\n",
            "Epoch 65/400\n",
            "800/800 [==============================] - 0s 187us/step - loss: 0.0397 - acc: 0.7688\n",
            "Epoch 66/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0398 - acc: 0.7525\n",
            "Epoch 67/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0394 - acc: 0.7700\n",
            "Epoch 68/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0393 - acc: 0.7750\n",
            "Epoch 69/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0393 - acc: 0.7800\n",
            "Epoch 70/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0390 - acc: 0.7763\n",
            "Epoch 71/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0390 - acc: 0.7887\n",
            "Epoch 72/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0388 - acc: 0.7975\n",
            "Epoch 73/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0386 - acc: 0.7963\n",
            "Epoch 74/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0385 - acc: 0.7962\n",
            "Epoch 75/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0384 - acc: 0.7975\n",
            "Epoch 76/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0382 - acc: 0.8075\n",
            "Epoch 77/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0381 - acc: 0.8050\n",
            "Epoch 78/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0380 - acc: 0.8062\n",
            "Epoch 79/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0379 - acc: 0.8150\n",
            "Epoch 80/400\n",
            "800/800 [==============================] - 0s 192us/step - loss: 0.0377 - acc: 0.8112\n",
            "Epoch 81/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0376 - acc: 0.8137\n",
            "Epoch 82/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0375 - acc: 0.8237\n",
            "Epoch 83/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0374 - acc: 0.8212\n",
            "Epoch 84/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0372 - acc: 0.8313\n",
            "Epoch 85/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0372 - acc: 0.8287\n",
            "Epoch 86/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0370 - acc: 0.8262\n",
            "Epoch 87/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0370 - acc: 0.8350\n",
            "Epoch 88/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0369 - acc: 0.8387\n",
            "Epoch 89/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0367 - acc: 0.8350\n",
            "Epoch 90/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0366 - acc: 0.8350\n",
            "Epoch 91/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0364 - acc: 0.8487\n",
            "Epoch 92/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0365 - acc: 0.8462\n",
            "Epoch 93/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0367 - acc: 0.8425\n",
            "Epoch 94/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0361 - acc: 0.8525\n",
            "Epoch 95/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0360 - acc: 0.8687\n",
            "Epoch 96/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0360 - acc: 0.8588\n",
            "Epoch 97/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0358 - acc: 0.8637\n",
            "Epoch 98/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0357 - acc: 0.8663\n",
            "Epoch 99/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0355 - acc: 0.8675\n",
            "Epoch 100/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0356 - acc: 0.8687\n",
            "Epoch 101/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0358 - acc: 0.8637\n",
            "Epoch 102/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0353 - acc: 0.8850\n",
            "Epoch 103/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0351 - acc: 0.8775\n",
            "Epoch 104/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0351 - acc: 0.8800\n",
            "Epoch 105/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0350 - acc: 0.8900\n",
            "Epoch 106/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0350 - acc: 0.8938\n",
            "Epoch 107/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0347 - acc: 0.8862\n",
            "Epoch 108/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0347 - acc: 0.8850\n",
            "Epoch 109/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0346 - acc: 0.8912\n",
            "Epoch 110/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0345 - acc: 0.9000\n",
            "Epoch 111/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0346 - acc: 0.8938\n",
            "Epoch 112/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0348 - acc: 0.8913\n",
            "Epoch 113/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0342 - acc: 0.9037\n",
            "Epoch 114/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0342 - acc: 0.9037\n",
            "Epoch 115/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0344 - acc: 0.9038\n",
            "Epoch 116/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0343 - acc: 0.8962\n",
            "Epoch 117/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0340 - acc: 0.9000\n",
            "Epoch 118/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0338 - acc: 0.9163\n",
            "Epoch 119/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0338 - acc: 0.9213\n",
            "Epoch 120/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0336 - acc: 0.9112\n",
            "Epoch 121/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0336 - acc: 0.9138\n",
            "Epoch 122/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0339 - acc: 0.9000\n",
            "Epoch 123/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0338 - acc: 0.9000\n",
            "Epoch 124/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0332 - acc: 0.9213\n",
            "Epoch 125/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0331 - acc: 0.9213\n",
            "Epoch 126/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0331 - acc: 0.9150\n",
            "Epoch 127/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0328 - acc: 0.9300\n",
            "Epoch 128/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0333 - acc: 0.9162\n",
            "Epoch 129/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0336 - acc: 0.9162\n",
            "Epoch 130/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0327 - acc: 0.9250\n",
            "Epoch 131/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0327 - acc: 0.9275\n",
            "Epoch 132/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0325 - acc: 0.9225\n",
            "Epoch 133/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0325 - acc: 0.9300\n",
            "Epoch 134/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0323 - acc: 0.9350\n",
            "Epoch 135/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0322 - acc: 0.9275\n",
            "Epoch 136/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0321 - acc: 0.9387\n",
            "Epoch 137/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0319 - acc: 0.9375\n",
            "Epoch 138/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0321 - acc: 0.9412\n",
            "Epoch 139/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0320 - acc: 0.9325\n",
            "Epoch 140/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0319 - acc: 0.9413\n",
            "Epoch 141/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0318 - acc: 0.9350\n",
            "Epoch 142/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0315 - acc: 0.9450\n",
            "Epoch 143/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0315 - acc: 0.9412\n",
            "Epoch 144/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0314 - acc: 0.9425\n",
            "Epoch 145/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0319 - acc: 0.9350\n",
            "Epoch 146/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0317 - acc: 0.9338\n",
            "Epoch 147/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0318 - acc: 0.9363\n",
            "Epoch 148/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0317 - acc: 0.9400\n",
            "Epoch 149/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0311 - acc: 0.9500\n",
            "Epoch 150/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0311 - acc: 0.9513\n",
            "Epoch 151/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0312 - acc: 0.9413\n",
            "Epoch 152/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0312 - acc: 0.9450\n",
            "Epoch 153/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0309 - acc: 0.9525\n",
            "Epoch 154/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0308 - acc: 0.9475\n",
            "Epoch 155/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0308 - acc: 0.9462\n",
            "Epoch 156/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0305 - acc: 0.9513\n",
            "Epoch 157/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0304 - acc: 0.9550\n",
            "Epoch 158/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0303 - acc: 0.9563\n",
            "Epoch 159/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0308 - acc: 0.9488\n",
            "Epoch 160/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0302 - acc: 0.9588\n",
            "Epoch 161/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0301 - acc: 0.9563\n",
            "Epoch 162/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0301 - acc: 0.9562\n",
            "Epoch 163/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0299 - acc: 0.9612\n",
            "Epoch 164/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0299 - acc: 0.9600\n",
            "Epoch 165/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0299 - acc: 0.9575\n",
            "Epoch 166/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0299 - acc: 0.9612\n",
            "Epoch 167/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0297 - acc: 0.9587\n",
            "Epoch 168/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0301 - acc: 0.9513\n",
            "Epoch 169/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0299 - acc: 0.9538\n",
            "Epoch 170/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0300 - acc: 0.9575\n",
            "Epoch 171/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0299 - acc: 0.9588\n",
            "Epoch 172/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0297 - acc: 0.9588\n",
            "Epoch 173/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0295 - acc: 0.9600\n",
            "Epoch 174/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0292 - acc: 0.9625\n",
            "Epoch 175/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0292 - acc: 0.9638\n",
            "Epoch 176/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0294 - acc: 0.9575\n",
            "Epoch 177/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0293 - acc: 0.9612\n",
            "Epoch 178/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0290 - acc: 0.9638\n",
            "Epoch 179/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0289 - acc: 0.9663\n",
            "Epoch 180/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0290 - acc: 0.9650\n",
            "Epoch 181/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0288 - acc: 0.9663\n",
            "Epoch 182/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0292 - acc: 0.9600\n",
            "Epoch 183/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0297 - acc: 0.9575\n",
            "Epoch 184/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0286 - acc: 0.9625\n",
            "Epoch 185/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0285 - acc: 0.9625\n",
            "Epoch 186/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0286 - acc: 0.9625\n",
            "Epoch 187/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0284 - acc: 0.9625\n",
            "Epoch 188/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0285 - acc: 0.9650\n",
            "Epoch 189/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0284 - acc: 0.9650\n",
            "Epoch 190/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0284 - acc: 0.9688\n",
            "Epoch 191/400\n",
            "800/800 [==============================] - 0s 197us/step - loss: 0.0283 - acc: 0.9688\n",
            "Epoch 192/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0285 - acc: 0.9600\n",
            "Epoch 193/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0292 - acc: 0.9588\n",
            "Epoch 194/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0283 - acc: 0.9650\n",
            "Epoch 195/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0281 - acc: 0.9637\n",
            "Epoch 196/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0285 - acc: 0.9613\n",
            "Epoch 197/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0281 - acc: 0.9688\n",
            "Epoch 198/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0278 - acc: 0.9675\n",
            "Epoch 199/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0277 - acc: 0.9675\n",
            "Epoch 200/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0279 - acc: 0.9650\n",
            "Epoch 201/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0278 - acc: 0.9713\n",
            "Epoch 202/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0278 - acc: 0.9675\n",
            "Epoch 203/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0277 - acc: 0.9688\n",
            "Epoch 204/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0276 - acc: 0.9663\n",
            "Epoch 205/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0278 - acc: 0.9650\n",
            "Epoch 206/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0276 - acc: 0.9663\n",
            "Epoch 207/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0274 - acc: 0.9638\n",
            "Epoch 208/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0277 - acc: 0.9700\n",
            "Epoch 209/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0271 - acc: 0.9688\n",
            "Epoch 210/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0271 - acc: 0.9688\n",
            "Epoch 211/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0273 - acc: 0.9663\n",
            "Epoch 212/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0274 - acc: 0.9638\n",
            "Epoch 213/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0270 - acc: 0.9675\n",
            "Epoch 214/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0269 - acc: 0.9675\n",
            "Epoch 215/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0268 - acc: 0.9663\n",
            "Epoch 216/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0270 - acc: 0.9638\n",
            "Epoch 217/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0270 - acc: 0.9700\n",
            "Epoch 218/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0269 - acc: 0.9738\n",
            "Epoch 219/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0273 - acc: 0.9663\n",
            "Epoch 220/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0266 - acc: 0.9663\n",
            "Epoch 221/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0266 - acc: 0.9688\n",
            "Epoch 222/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0265 - acc: 0.9700\n",
            "Epoch 223/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0267 - acc: 0.9663\n",
            "Epoch 224/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0265 - acc: 0.9713\n",
            "Epoch 225/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0264 - acc: 0.9700\n",
            "Epoch 226/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0263 - acc: 0.9725\n",
            "Epoch 227/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0263 - acc: 0.9700\n",
            "Epoch 228/400\n",
            "800/800 [==============================] - 0s 194us/step - loss: 0.0266 - acc: 0.9688\n",
            "Epoch 229/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0264 - acc: 0.9700\n",
            "Epoch 230/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0261 - acc: 0.9688\n",
            "Epoch 231/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0261 - acc: 0.9688\n",
            "Epoch 232/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0259 - acc: 0.9763\n",
            "Epoch 233/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0259 - acc: 0.9688\n",
            "Epoch 234/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0258 - acc: 0.9688\n",
            "Epoch 235/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0258 - acc: 0.9713\n",
            "Epoch 236/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0258 - acc: 0.9713\n",
            "Epoch 237/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0258 - acc: 0.9700\n",
            "Epoch 238/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0257 - acc: 0.9725\n",
            "Epoch 239/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0256 - acc: 0.9750\n",
            "Epoch 240/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0257 - acc: 0.9738\n",
            "Epoch 241/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0256 - acc: 0.9725\n",
            "Epoch 242/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0257 - acc: 0.9688\n",
            "Epoch 243/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0259 - acc: 0.9725\n",
            "Epoch 244/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0266 - acc: 0.9700\n",
            "Epoch 245/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0253 - acc: 0.9725\n",
            "Epoch 246/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0252 - acc: 0.9700\n",
            "Epoch 247/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0252 - acc: 0.9725\n",
            "Epoch 248/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0252 - acc: 0.9713\n",
            "Epoch 249/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0252 - acc: 0.9725\n",
            "Epoch 250/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0251 - acc: 0.9763\n",
            "Epoch 251/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0253 - acc: 0.9725\n",
            "Epoch 252/400\n",
            "800/800 [==============================] - 0s 192us/step - loss: 0.0253 - acc: 0.9688\n",
            "Epoch 253/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0250 - acc: 0.9738\n",
            "Epoch 254/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0250 - acc: 0.9725\n",
            "Epoch 255/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0252 - acc: 0.9700\n",
            "Epoch 256/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0252 - acc: 0.9737\n",
            "Epoch 257/400\n",
            "800/800 [==============================] - 0s 190us/step - loss: 0.0250 - acc: 0.9750\n",
            "Epoch 258/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0249 - acc: 0.9750\n",
            "Epoch 259/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0247 - acc: 0.9763\n",
            "Epoch 260/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0247 - acc: 0.9775\n",
            "Epoch 261/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0247 - acc: 0.9763\n",
            "Epoch 262/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0254 - acc: 0.9662\n",
            "Epoch 263/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0249 - acc: 0.9738\n",
            "Epoch 264/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0246 - acc: 0.9763\n",
            "Epoch 265/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0245 - acc: 0.9775\n",
            "Epoch 266/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0245 - acc: 0.9775\n",
            "Epoch 267/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0243 - acc: 0.9750\n",
            "Epoch 268/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0244 - acc: 0.9763\n",
            "Epoch 269/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0242 - acc: 0.9738\n",
            "Epoch 270/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0242 - acc: 0.9763\n",
            "Epoch 271/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0247 - acc: 0.9738\n",
            "Epoch 272/400\n",
            "800/800 [==============================] - 0s 192us/step - loss: 0.0244 - acc: 0.9738\n",
            "Epoch 273/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0241 - acc: 0.9763\n",
            "Epoch 274/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0240 - acc: 0.9763\n",
            "Epoch 275/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0239 - acc: 0.9788\n",
            "Epoch 276/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0240 - acc: 0.9775\n",
            "Epoch 277/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0240 - acc: 0.9750\n",
            "Epoch 278/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0246 - acc: 0.9688\n",
            "Epoch 279/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0244 - acc: 0.9775\n",
            "Epoch 280/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0245 - acc: 0.9713\n",
            "Epoch 281/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0243 - acc: 0.9788\n",
            "Epoch 282/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0239 - acc: 0.9775\n",
            "Epoch 283/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0237 - acc: 0.9775\n",
            "Epoch 284/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0237 - acc: 0.9763\n",
            "Epoch 285/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0235 - acc: 0.9788\n",
            "Epoch 286/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0241 - acc: 0.9763\n",
            "Epoch 287/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0242 - acc: 0.9700\n",
            "Epoch 288/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0236 - acc: 0.9813\n",
            "Epoch 289/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0239 - acc: 0.9763\n",
            "Epoch 290/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0237 - acc: 0.9738\n",
            "Epoch 291/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0240 - acc: 0.9750\n",
            "Epoch 292/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0236 - acc: 0.9788\n",
            "Epoch 293/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0232 - acc: 0.9788\n",
            "Epoch 294/400\n",
            "800/800 [==============================] - 0s 190us/step - loss: 0.0233 - acc: 0.9750\n",
            "Epoch 295/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0235 - acc: 0.9763\n",
            "Epoch 296/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0234 - acc: 0.9750\n",
            "Epoch 297/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0234 - acc: 0.9775\n",
            "Epoch 298/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0232 - acc: 0.9750\n",
            "Epoch 299/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0230 - acc: 0.9763\n",
            "Epoch 300/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0229 - acc: 0.9775\n",
            "Epoch 301/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0230 - acc: 0.9775\n",
            "Epoch 302/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0239 - acc: 0.9700\n",
            "Epoch 303/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0231 - acc: 0.9775\n",
            "Epoch 304/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0229 - acc: 0.9813\n",
            "Epoch 305/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0228 - acc: 0.9788\n",
            "Epoch 306/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0228 - acc: 0.9788\n",
            "Epoch 307/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0227 - acc: 0.9788\n",
            "Epoch 308/400\n",
            "800/800 [==============================] - 0s 187us/step - loss: 0.0227 - acc: 0.9813\n",
            "Epoch 309/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0227 - acc: 0.9850\n",
            "Epoch 310/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0228 - acc: 0.9788\n",
            "Epoch 311/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0229 - acc: 0.9763\n",
            "Epoch 312/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0230 - acc: 0.9775\n",
            "Epoch 313/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0225 - acc: 0.9825\n",
            "Epoch 314/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0229 - acc: 0.9775\n",
            "Epoch 315/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0230 - acc: 0.9800\n",
            "Epoch 316/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0229 - acc: 0.9763\n",
            "Epoch 317/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0226 - acc: 0.9763\n",
            "Epoch 318/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0227 - acc: 0.9825\n",
            "Epoch 319/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0225 - acc: 0.9788\n",
            "Epoch 320/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0223 - acc: 0.9788\n",
            "Epoch 321/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0222 - acc: 0.9788\n",
            "Epoch 322/400\n",
            "800/800 [==============================] - 0s 194us/step - loss: 0.0224 - acc: 0.9813\n",
            "Epoch 323/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0221 - acc: 0.9800\n",
            "Epoch 324/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0224 - acc: 0.9775\n",
            "Epoch 325/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0223 - acc: 0.9813\n",
            "Epoch 326/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0224 - acc: 0.9813\n",
            "Epoch 327/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0226 - acc: 0.9813\n",
            "Epoch 328/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0224 - acc: 0.9800\n",
            "Epoch 329/400\n",
            "800/800 [==============================] - 0s 187us/step - loss: 0.0220 - acc: 0.9813\n",
            "Epoch 330/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0222 - acc: 0.9800\n",
            "Epoch 331/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0228 - acc: 0.9738\n",
            "Epoch 332/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0219 - acc: 0.9813\n",
            "Epoch 333/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0218 - acc: 0.9825\n",
            "Epoch 334/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0218 - acc: 0.9788\n",
            "Epoch 335/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0218 - acc: 0.9813\n",
            "Epoch 336/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0218 - acc: 0.9825\n",
            "Epoch 337/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0221 - acc: 0.9800\n",
            "Epoch 338/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0217 - acc: 0.9813\n",
            "Epoch 339/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0216 - acc: 0.9825\n",
            "Epoch 340/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0219 - acc: 0.9800\n",
            "Epoch 341/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0222 - acc: 0.9813\n",
            "Epoch 342/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0221 - acc: 0.9825\n",
            "Epoch 343/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0217 - acc: 0.9800\n",
            "Epoch 344/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0216 - acc: 0.9813\n",
            "Epoch 345/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0215 - acc: 0.9775\n",
            "Epoch 346/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0215 - acc: 0.9800\n",
            "Epoch 347/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0216 - acc: 0.9800\n",
            "Epoch 348/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0220 - acc: 0.9788\n",
            "Epoch 349/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0212 - acc: 0.9825\n",
            "Epoch 350/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0213 - acc: 0.9825\n",
            "Epoch 351/400\n",
            "800/800 [==============================] - 0s 192us/step - loss: 0.0212 - acc: 0.9813\n",
            "Epoch 352/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0214 - acc: 0.9838\n",
            "Epoch 353/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0220 - acc: 0.9813\n",
            "Epoch 354/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0214 - acc: 0.9788\n",
            "Epoch 355/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0212 - acc: 0.9788\n",
            "Epoch 356/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0213 - acc: 0.9850\n",
            "Epoch 357/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0210 - acc: 0.9825\n",
            "Epoch 358/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0211 - acc: 0.9800\n",
            "Epoch 359/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0209 - acc: 0.9825\n",
            "Epoch 360/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0213 - acc: 0.9800\n",
            "Epoch 361/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0210 - acc: 0.9800\n",
            "Epoch 362/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0208 - acc: 0.9812\n",
            "Epoch 363/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0210 - acc: 0.9825\n",
            "Epoch 364/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0208 - acc: 0.9800\n",
            "Epoch 365/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0210 - acc: 0.9813\n",
            "Epoch 366/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0208 - acc: 0.9825\n",
            "Epoch 367/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0212 - acc: 0.9825\n",
            "Epoch 368/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0215 - acc: 0.9750\n",
            "Epoch 369/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0209 - acc: 0.9863\n",
            "Epoch 370/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0207 - acc: 0.9825\n",
            "Epoch 371/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0205 - acc: 0.9813\n",
            "Epoch 372/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0205 - acc: 0.9825\n",
            "Epoch 373/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0205 - acc: 0.9813\n",
            "Epoch 374/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0205 - acc: 0.9825\n",
            "Epoch 375/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0205 - acc: 0.9838\n",
            "Epoch 376/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0205 - acc: 0.9813\n",
            "Epoch 377/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0204 - acc: 0.9850\n",
            "Epoch 378/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0204 - acc: 0.9813\n",
            "Epoch 379/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0203 - acc: 0.9838\n",
            "Epoch 380/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0202 - acc: 0.9838\n",
            "Epoch 381/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0206 - acc: 0.9788\n",
            "Epoch 382/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0205 - acc: 0.9838\n",
            "Epoch 383/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0206 - acc: 0.9838\n",
            "Epoch 384/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0202 - acc: 0.9838\n",
            "Epoch 385/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0203 - acc: 0.9825\n",
            "Epoch 386/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0208 - acc: 0.9775\n",
            "Epoch 387/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0206 - acc: 0.9838\n",
            "Epoch 388/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0207 - acc: 0.9800\n",
            "Epoch 389/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0202 - acc: 0.9838\n",
            "Epoch 390/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0200 - acc: 0.9838\n",
            "Epoch 391/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0201 - acc: 0.9850\n",
            "Epoch 392/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0201 - acc: 0.9838\n",
            "Epoch 393/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0200 - acc: 0.9813\n",
            "Epoch 394/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0199 - acc: 0.9813\n",
            "Epoch 395/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0206 - acc: 0.9850\n",
            "Epoch 396/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0200 - acc: 0.9800\n",
            "Epoch 397/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0201 - acc: 0.9863\n",
            "Epoch 398/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0199 - acc: 0.9850\n",
            "Epoch 399/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0199 - acc: 0.9838\n",
            "Epoch 400/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0198 - acc: 0.9813\n",
            "200/200 [==============================] - 0s 2ms/step\n",
            "Epoch 1/400\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.0724 - acc: 0.0500\n",
            "Epoch 2/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0559 - acc: 0.0800\n",
            "Epoch 3/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0542 - acc: 0.1213\n",
            "Epoch 4/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0535 - acc: 0.1375\n",
            "Epoch 5/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0527 - acc: 0.1788\n",
            "Epoch 6/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0521 - acc: 0.1925\n",
            "Epoch 7/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0516 - acc: 0.2213\n",
            "Epoch 8/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0512 - acc: 0.2112\n",
            "Epoch 9/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0512 - acc: 0.2125\n",
            "Epoch 10/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0503 - acc: 0.2537\n",
            "Epoch 11/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0499 - acc: 0.2725\n",
            "Epoch 12/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0495 - acc: 0.2888\n",
            "Epoch 13/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0492 - acc: 0.2787\n",
            "Epoch 14/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0488 - acc: 0.3050\n",
            "Epoch 15/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0485 - acc: 0.3325\n",
            "Epoch 16/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0483 - acc: 0.3388\n",
            "Epoch 17/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0479 - acc: 0.3687\n",
            "Epoch 18/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0476 - acc: 0.3975\n",
            "Epoch 19/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0474 - acc: 0.3938\n",
            "Epoch 20/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0471 - acc: 0.4187\n",
            "Epoch 21/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0470 - acc: 0.4175\n",
            "Epoch 22/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0469 - acc: 0.4200\n",
            "Epoch 23/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0464 - acc: 0.4700\n",
            "Epoch 24/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0465 - acc: 0.4450\n",
            "Epoch 25/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0461 - acc: 0.4900\n",
            "Epoch 26/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0457 - acc: 0.5025\n",
            "Epoch 27/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0454 - acc: 0.5250\n",
            "Epoch 28/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0454 - acc: 0.5037\n",
            "Epoch 29/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0450 - acc: 0.5437\n",
            "Epoch 30/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0448 - acc: 0.5562\n",
            "Epoch 31/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0446 - acc: 0.5600\n",
            "Epoch 32/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0448 - acc: 0.5375\n",
            "Epoch 33/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0447 - acc: 0.5550\n",
            "Epoch 34/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0444 - acc: 0.5538\n",
            "Epoch 35/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0442 - acc: 0.5788\n",
            "Epoch 36/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0437 - acc: 0.6000\n",
            "Epoch 37/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0434 - acc: 0.6187\n",
            "Epoch 38/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0435 - acc: 0.5912\n",
            "Epoch 39/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0433 - acc: 0.6062\n",
            "Epoch 40/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0429 - acc: 0.6475\n",
            "Epoch 41/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0428 - acc: 0.6312\n",
            "Epoch 42/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0430 - acc: 0.6288\n",
            "Epoch 43/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0426 - acc: 0.6562\n",
            "Epoch 44/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0425 - acc: 0.6388\n",
            "Epoch 45/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0421 - acc: 0.6663\n",
            "Epoch 46/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0424 - acc: 0.6500\n",
            "Epoch 47/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0422 - acc: 0.6538\n",
            "Epoch 48/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0422 - acc: 0.6600\n",
            "Epoch 49/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0417 - acc: 0.6800\n",
            "Epoch 50/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0413 - acc: 0.7000\n",
            "Epoch 51/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0411 - acc: 0.7100\n",
            "Epoch 52/400\n",
            "800/800 [==============================] - 0s 195us/step - loss: 0.0412 - acc: 0.7050\n",
            "Epoch 53/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0410 - acc: 0.7275\n",
            "Epoch 54/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0408 - acc: 0.7175\n",
            "Epoch 55/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0406 - acc: 0.7325\n",
            "Epoch 56/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0405 - acc: 0.7300\n",
            "Epoch 57/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0405 - acc: 0.7400\n",
            "Epoch 58/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0403 - acc: 0.7350\n",
            "Epoch 59/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0401 - acc: 0.7550\n",
            "Epoch 60/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0401 - acc: 0.7550\n",
            "Epoch 61/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0400 - acc: 0.7562\n",
            "Epoch 62/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0396 - acc: 0.7625\n",
            "Epoch 63/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0394 - acc: 0.7625\n",
            "Epoch 64/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0395 - acc: 0.7625\n",
            "Epoch 65/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0400 - acc: 0.7375\n",
            "Epoch 66/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0394 - acc: 0.7725\n",
            "Epoch 67/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0390 - acc: 0.7912\n",
            "Epoch 68/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0393 - acc: 0.7638\n",
            "Epoch 69/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0388 - acc: 0.7912\n",
            "Epoch 70/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0388 - acc: 0.7912\n",
            "Epoch 71/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0385 - acc: 0.7987\n",
            "Epoch 72/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0388 - acc: 0.7862\n",
            "Epoch 73/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0394 - acc: 0.7650\n",
            "Epoch 74/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0382 - acc: 0.8112\n",
            "Epoch 75/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0380 - acc: 0.8075\n",
            "Epoch 76/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0379 - acc: 0.8100\n",
            "Epoch 77/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0379 - acc: 0.8088\n",
            "Epoch 78/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0384 - acc: 0.7925\n",
            "Epoch 79/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0384 - acc: 0.7975\n",
            "Epoch 80/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0377 - acc: 0.8225\n",
            "Epoch 81/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0380 - acc: 0.7975\n",
            "Epoch 82/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0379 - acc: 0.8150\n",
            "Epoch 83/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0376 - acc: 0.8175\n",
            "Epoch 84/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0380 - acc: 0.7987\n",
            "Epoch 85/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0379 - acc: 0.8012\n",
            "Epoch 86/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0370 - acc: 0.8363\n",
            "Epoch 87/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0368 - acc: 0.8375\n",
            "Epoch 88/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0372 - acc: 0.8425\n",
            "Epoch 89/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0368 - acc: 0.8387\n",
            "Epoch 90/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0366 - acc: 0.8500\n",
            "Epoch 91/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0368 - acc: 0.8312\n",
            "Epoch 92/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0367 - acc: 0.8425\n",
            "Epoch 93/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0368 - acc: 0.8462\n",
            "Epoch 94/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0370 - acc: 0.8200\n",
            "Epoch 95/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0359 - acc: 0.8588\n",
            "Epoch 96/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0360 - acc: 0.8550\n",
            "Epoch 97/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0358 - acc: 0.8637\n",
            "Epoch 98/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0362 - acc: 0.8463\n",
            "Epoch 99/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0366 - acc: 0.8313\n",
            "Epoch 100/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0363 - acc: 0.8375\n",
            "Epoch 101/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0353 - acc: 0.8813\n",
            "Epoch 102/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0360 - acc: 0.8525\n",
            "Epoch 103/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0355 - acc: 0.8675\n",
            "Epoch 104/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0353 - acc: 0.8700\n",
            "Epoch 105/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0350 - acc: 0.8825\n",
            "Epoch 106/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0350 - acc: 0.8750\n",
            "Epoch 107/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0353 - acc: 0.8613\n",
            "Epoch 108/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0352 - acc: 0.8700\n",
            "Epoch 109/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0347 - acc: 0.8850\n",
            "Epoch 110/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0349 - acc: 0.8688\n",
            "Epoch 111/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0347 - acc: 0.8800\n",
            "Epoch 112/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0343 - acc: 0.8825\n",
            "Epoch 113/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0344 - acc: 0.8837\n",
            "Epoch 114/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0347 - acc: 0.8875\n",
            "Epoch 115/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0344 - acc: 0.8862\n",
            "Epoch 116/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0346 - acc: 0.8675\n",
            "Epoch 117/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0345 - acc: 0.8725\n",
            "Epoch 118/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0345 - acc: 0.8762\n",
            "Epoch 119/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0344 - acc: 0.8762\n",
            "Epoch 120/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0343 - acc: 0.8713\n",
            "Epoch 121/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0337 - acc: 0.9000\n",
            "Epoch 122/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0337 - acc: 0.8862\n",
            "Epoch 123/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0334 - acc: 0.8987\n",
            "Epoch 124/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0333 - acc: 0.8987\n",
            "Epoch 125/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0341 - acc: 0.8813\n",
            "Epoch 126/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0337 - acc: 0.8937\n",
            "Epoch 127/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0336 - acc: 0.8850\n",
            "Epoch 128/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0343 - acc: 0.8700\n",
            "Epoch 129/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0335 - acc: 0.8900\n",
            "Epoch 130/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0329 - acc: 0.9025\n",
            "Epoch 131/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0328 - acc: 0.9000\n",
            "Epoch 132/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0335 - acc: 0.8813\n",
            "Epoch 133/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0329 - acc: 0.9025\n",
            "Epoch 134/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0326 - acc: 0.9075\n",
            "Epoch 135/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0328 - acc: 0.8975\n",
            "Epoch 136/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0334 - acc: 0.8912\n",
            "Epoch 137/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0330 - acc: 0.8850\n",
            "Epoch 138/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0325 - acc: 0.9125\n",
            "Epoch 139/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0325 - acc: 0.9038\n",
            "Epoch 140/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0322 - acc: 0.9075\n",
            "Epoch 141/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0324 - acc: 0.9012\n",
            "Epoch 142/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0325 - acc: 0.8962\n",
            "Epoch 143/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0329 - acc: 0.9000\n",
            "Epoch 144/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0320 - acc: 0.9075\n",
            "Epoch 145/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0316 - acc: 0.9100\n",
            "Epoch 146/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0324 - acc: 0.8987\n",
            "Epoch 147/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0316 - acc: 0.9113\n",
            "Epoch 148/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0314 - acc: 0.9138\n",
            "Epoch 149/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0318 - acc: 0.9063\n",
            "Epoch 150/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0323 - acc: 0.9000\n",
            "Epoch 151/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0320 - acc: 0.9062\n",
            "Epoch 152/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0319 - acc: 0.9038\n",
            "Epoch 153/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0312 - acc: 0.9200\n",
            "Epoch 154/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0313 - acc: 0.9200\n",
            "Epoch 155/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0320 - acc: 0.9100\n",
            "Epoch 156/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0313 - acc: 0.9225\n",
            "Epoch 157/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0313 - acc: 0.9138\n",
            "Epoch 158/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0314 - acc: 0.9162\n",
            "Epoch 159/400\n",
            "800/800 [==============================] - 0s 190us/step - loss: 0.0321 - acc: 0.9013\n",
            "Epoch 160/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0310 - acc: 0.9138\n",
            "Epoch 161/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0309 - acc: 0.9175\n",
            "Epoch 162/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0304 - acc: 0.9262\n",
            "Epoch 163/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0305 - acc: 0.9263\n",
            "Epoch 164/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0310 - acc: 0.9150\n",
            "Epoch 165/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0306 - acc: 0.9225\n",
            "Epoch 166/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0302 - acc: 0.9225\n",
            "Epoch 167/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0308 - acc: 0.9213\n",
            "Epoch 168/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0310 - acc: 0.9188\n",
            "Epoch 169/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0307 - acc: 0.9138\n",
            "Epoch 170/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0304 - acc: 0.9137\n",
            "Epoch 171/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0301 - acc: 0.9300\n",
            "Epoch 172/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0298 - acc: 0.9275\n",
            "Epoch 173/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0301 - acc: 0.9213\n",
            "Epoch 174/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0299 - acc: 0.9288\n",
            "Epoch 175/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0308 - acc: 0.9162\n",
            "Epoch 176/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0295 - acc: 0.9387\n",
            "Epoch 177/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0294 - acc: 0.9312\n",
            "Epoch 178/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0298 - acc: 0.9188\n",
            "Epoch 179/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0300 - acc: 0.9250\n",
            "Epoch 180/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0293 - acc: 0.9312\n",
            "Epoch 181/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0292 - acc: 0.9350\n",
            "Epoch 182/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0304 - acc: 0.9150\n",
            "Epoch 183/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0296 - acc: 0.9325\n",
            "Epoch 184/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0294 - acc: 0.9313\n",
            "Epoch 185/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0299 - acc: 0.9325\n",
            "Epoch 186/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0291 - acc: 0.9362\n",
            "Epoch 187/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0294 - acc: 0.9338\n",
            "Epoch 188/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0288 - acc: 0.9412\n",
            "Epoch 189/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0286 - acc: 0.9438\n",
            "Epoch 190/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0290 - acc: 0.9363\n",
            "Epoch 191/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0300 - acc: 0.9212\n",
            "Epoch 192/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0290 - acc: 0.9338\n",
            "Epoch 193/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0292 - acc: 0.9363\n",
            "Epoch 194/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0289 - acc: 0.9375\n",
            "Epoch 195/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0285 - acc: 0.9462\n",
            "Epoch 196/400\n",
            "800/800 [==============================] - 0s 190us/step - loss: 0.0285 - acc: 0.9450\n",
            "Epoch 197/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0282 - acc: 0.9400\n",
            "Epoch 198/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0285 - acc: 0.9363\n",
            "Epoch 199/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0287 - acc: 0.9412\n",
            "Epoch 200/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0291 - acc: 0.9263\n",
            "Epoch 201/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0285 - acc: 0.9325\n",
            "Epoch 202/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0281 - acc: 0.9425\n",
            "Epoch 203/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0280 - acc: 0.9475\n",
            "Epoch 204/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0282 - acc: 0.9388\n",
            "Epoch 205/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0288 - acc: 0.9263\n",
            "Epoch 206/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0285 - acc: 0.9350\n",
            "Epoch 207/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0279 - acc: 0.9425\n",
            "Epoch 208/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0288 - acc: 0.9312\n",
            "Epoch 209/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0283 - acc: 0.9325\n",
            "Epoch 210/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0283 - acc: 0.9275\n",
            "Epoch 211/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0282 - acc: 0.9363\n",
            "Epoch 212/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0281 - acc: 0.9400\n",
            "Epoch 213/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0277 - acc: 0.9500\n",
            "Epoch 214/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0279 - acc: 0.9363\n",
            "Epoch 215/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0272 - acc: 0.9513\n",
            "Epoch 216/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0274 - acc: 0.9475\n",
            "Epoch 217/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0277 - acc: 0.9400\n",
            "Epoch 218/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0281 - acc: 0.9450\n",
            "Epoch 219/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0279 - acc: 0.9438\n",
            "Epoch 220/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0278 - acc: 0.9375\n",
            "Epoch 221/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0274 - acc: 0.9450\n",
            "Epoch 222/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0274 - acc: 0.9450\n",
            "Epoch 223/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0278 - acc: 0.9363\n",
            "Epoch 224/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0273 - acc: 0.9475\n",
            "Epoch 225/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0267 - acc: 0.9525\n",
            "Epoch 226/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0266 - acc: 0.9438\n",
            "Epoch 227/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0274 - acc: 0.9500\n",
            "Epoch 228/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0273 - acc: 0.9487\n",
            "Epoch 229/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0266 - acc: 0.9525\n",
            "Epoch 230/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0267 - acc: 0.9413\n",
            "Epoch 231/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0270 - acc: 0.9500\n",
            "Epoch 232/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0270 - acc: 0.9475\n",
            "Epoch 233/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0278 - acc: 0.9375\n",
            "Epoch 234/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0265 - acc: 0.9525\n",
            "Epoch 235/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0263 - acc: 0.9475\n",
            "Epoch 236/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0265 - acc: 0.9513\n",
            "Epoch 237/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0269 - acc: 0.9488\n",
            "Epoch 238/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0267 - acc: 0.9412\n",
            "Epoch 239/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0260 - acc: 0.9563\n",
            "Epoch 240/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0267 - acc: 0.9450\n",
            "Epoch 241/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0271 - acc: 0.9488\n",
            "Epoch 242/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0270 - acc: 0.9388\n",
            "Epoch 243/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0258 - acc: 0.9562\n",
            "Epoch 244/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0261 - acc: 0.9475\n",
            "Epoch 245/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0257 - acc: 0.9538\n",
            "Epoch 246/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0257 - acc: 0.9513\n",
            "Epoch 247/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0260 - acc: 0.9500\n",
            "Epoch 248/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0255 - acc: 0.9550\n",
            "Epoch 249/400\n",
            "800/800 [==============================] - 0s 192us/step - loss: 0.0255 - acc: 0.9563\n",
            "Epoch 250/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0257 - acc: 0.9538\n",
            "Epoch 251/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0256 - acc: 0.9525\n",
            "Epoch 252/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0258 - acc: 0.9425\n",
            "Epoch 253/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0263 - acc: 0.9488\n",
            "Epoch 254/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0254 - acc: 0.9550\n",
            "Epoch 255/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0252 - acc: 0.9575\n",
            "Epoch 256/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0251 - acc: 0.9563\n",
            "Epoch 257/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0252 - acc: 0.9550\n",
            "Epoch 258/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0251 - acc: 0.9575\n",
            "Epoch 259/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0251 - acc: 0.9550\n",
            "Epoch 260/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0253 - acc: 0.9500\n",
            "Epoch 261/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0256 - acc: 0.9538\n",
            "Epoch 262/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0256 - acc: 0.9512\n",
            "Epoch 263/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0250 - acc: 0.9537\n",
            "Epoch 264/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0250 - acc: 0.9525\n",
            "Epoch 265/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0253 - acc: 0.9575\n",
            "Epoch 266/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0259 - acc: 0.9500\n",
            "Epoch 267/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0252 - acc: 0.9525\n",
            "Epoch 268/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0247 - acc: 0.9562\n",
            "Epoch 269/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0249 - acc: 0.9575\n",
            "Epoch 270/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0262 - acc: 0.9425\n",
            "Epoch 271/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0245 - acc: 0.9600\n",
            "Epoch 272/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0245 - acc: 0.9537\n",
            "Epoch 273/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0245 - acc: 0.9575\n",
            "Epoch 274/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0245 - acc: 0.9575\n",
            "Epoch 275/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0244 - acc: 0.9588\n",
            "Epoch 276/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0249 - acc: 0.9537\n",
            "Epoch 277/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0250 - acc: 0.9488\n",
            "Epoch 278/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0246 - acc: 0.9575\n",
            "Epoch 279/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0242 - acc: 0.9550\n",
            "Epoch 280/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0245 - acc: 0.9575\n",
            "Epoch 281/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0245 - acc: 0.9550\n",
            "Epoch 282/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0254 - acc: 0.9538\n",
            "Epoch 283/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0245 - acc: 0.9563\n",
            "Epoch 284/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0242 - acc: 0.9550\n",
            "Epoch 285/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0239 - acc: 0.9575\n",
            "Epoch 286/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0243 - acc: 0.9588\n",
            "Epoch 287/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0245 - acc: 0.9613\n",
            "Epoch 288/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0240 - acc: 0.9575\n",
            "Epoch 289/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0248 - acc: 0.9550\n",
            "Epoch 290/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0246 - acc: 0.9488\n",
            "Epoch 291/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0246 - acc: 0.9513\n",
            "Epoch 292/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0238 - acc: 0.9613\n",
            "Epoch 293/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0239 - acc: 0.9550\n",
            "Epoch 294/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0237 - acc: 0.9650\n",
            "Epoch 295/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0239 - acc: 0.9625\n",
            "Epoch 296/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0239 - acc: 0.9575\n",
            "Epoch 297/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0238 - acc: 0.9588\n",
            "Epoch 298/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0235 - acc: 0.9600\n",
            "Epoch 299/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0239 - acc: 0.9600\n",
            "Epoch 300/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0239 - acc: 0.9575\n",
            "Epoch 301/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0234 - acc: 0.9638\n",
            "Epoch 302/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0232 - acc: 0.9575\n",
            "Epoch 303/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0244 - acc: 0.9600\n",
            "Epoch 304/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0232 - acc: 0.9575\n",
            "Epoch 305/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0232 - acc: 0.9588\n",
            "Epoch 306/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0232 - acc: 0.9575\n",
            "Epoch 307/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0232 - acc: 0.9613\n",
            "Epoch 308/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0234 - acc: 0.9638\n",
            "Epoch 309/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0235 - acc: 0.9650\n",
            "Epoch 310/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0228 - acc: 0.9600\n",
            "Epoch 311/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0230 - acc: 0.9600\n",
            "Epoch 312/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0229 - acc: 0.9575\n",
            "Epoch 313/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0232 - acc: 0.9638\n",
            "Epoch 314/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0230 - acc: 0.9650\n",
            "Epoch 315/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0229 - acc: 0.9638\n",
            "Epoch 316/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0235 - acc: 0.9625\n",
            "Epoch 317/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0234 - acc: 0.9600\n",
            "Epoch 318/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0231 - acc: 0.9625\n",
            "Epoch 319/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0225 - acc: 0.9613\n",
            "Epoch 320/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0224 - acc: 0.9600\n",
            "Epoch 321/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0225 - acc: 0.9637\n",
            "Epoch 322/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0224 - acc: 0.9575\n",
            "Epoch 323/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0223 - acc: 0.9600\n",
            "Epoch 324/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0224 - acc: 0.9650\n",
            "Epoch 325/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0221 - acc: 0.9650\n",
            "Epoch 326/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0222 - acc: 0.9662\n",
            "Epoch 327/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0226 - acc: 0.9600\n",
            "Epoch 328/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0232 - acc: 0.9588\n",
            "Epoch 329/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0228 - acc: 0.9675\n",
            "Epoch 330/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0226 - acc: 0.9638\n",
            "Epoch 331/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0225 - acc: 0.9663\n",
            "Epoch 332/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0228 - acc: 0.9675\n",
            "Epoch 333/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0225 - acc: 0.9638\n",
            "Epoch 334/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0226 - acc: 0.9650\n",
            "Epoch 335/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0221 - acc: 0.9625\n",
            "Epoch 336/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0220 - acc: 0.9700\n",
            "Epoch 337/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0218 - acc: 0.9688\n",
            "Epoch 338/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0217 - acc: 0.9688\n",
            "Epoch 339/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0218 - acc: 0.9675\n",
            "Epoch 340/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0218 - acc: 0.9663\n",
            "Epoch 341/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0226 - acc: 0.9700\n",
            "Epoch 342/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0227 - acc: 0.9613\n",
            "Epoch 343/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0223 - acc: 0.9638\n",
            "Epoch 344/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0221 - acc: 0.9625\n",
            "Epoch 345/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0224 - acc: 0.9663\n",
            "Epoch 346/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0216 - acc: 0.9663\n",
            "Epoch 347/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0216 - acc: 0.9688\n",
            "Epoch 348/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0214 - acc: 0.9675\n",
            "Epoch 349/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0216 - acc: 0.9675\n",
            "Epoch 350/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0225 - acc: 0.9562\n",
            "Epoch 351/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0218 - acc: 0.9650\n",
            "Epoch 352/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0212 - acc: 0.9713\n",
            "Epoch 353/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0218 - acc: 0.9637\n",
            "Epoch 354/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0214 - acc: 0.9713\n",
            "Epoch 355/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0209 - acc: 0.9688\n",
            "Epoch 356/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0211 - acc: 0.9638\n",
            "Epoch 357/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0210 - acc: 0.9700\n",
            "Epoch 358/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0216 - acc: 0.9688\n",
            "Epoch 359/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0214 - acc: 0.9700\n",
            "Epoch 360/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0217 - acc: 0.9650\n",
            "Epoch 361/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0209 - acc: 0.9650\n",
            "Epoch 362/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0211 - acc: 0.9638\n",
            "Epoch 363/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0215 - acc: 0.9613\n",
            "Epoch 364/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0207 - acc: 0.9738\n",
            "Epoch 365/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0206 - acc: 0.9738\n",
            "Epoch 366/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0207 - acc: 0.9713\n",
            "Epoch 367/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0210 - acc: 0.9638\n",
            "Epoch 368/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0205 - acc: 0.9725\n",
            "Epoch 369/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0212 - acc: 0.9688\n",
            "Epoch 370/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0215 - acc: 0.9663\n",
            "Epoch 371/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0209 - acc: 0.9738\n",
            "Epoch 372/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0206 - acc: 0.9700\n",
            "Epoch 373/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0202 - acc: 0.9725\n",
            "Epoch 374/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0204 - acc: 0.9713\n",
            "Epoch 375/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0211 - acc: 0.9650\n",
            "Epoch 376/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0209 - acc: 0.9663\n",
            "Epoch 377/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0205 - acc: 0.9713\n",
            "Epoch 378/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0203 - acc: 0.9663\n",
            "Epoch 379/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0207 - acc: 0.9650\n",
            "Epoch 380/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0205 - acc: 0.9700\n",
            "Epoch 381/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0208 - acc: 0.9713\n",
            "Epoch 382/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0201 - acc: 0.9713\n",
            "Epoch 383/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0201 - acc: 0.9713\n",
            "Epoch 384/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0203 - acc: 0.9688\n",
            "Epoch 385/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0206 - acc: 0.9713\n",
            "Epoch 386/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0204 - acc: 0.9713\n",
            "Epoch 387/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0205 - acc: 0.9737\n",
            "Epoch 388/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0206 - acc: 0.9725\n",
            "Epoch 389/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0200 - acc: 0.9738\n",
            "Epoch 390/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0202 - acc: 0.9750\n",
            "Epoch 391/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0199 - acc: 0.9675\n",
            "Epoch 392/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0196 - acc: 0.9725\n",
            "Epoch 393/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0198 - acc: 0.9713\n",
            "Epoch 394/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0197 - acc: 0.9688\n",
            "Epoch 395/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0195 - acc: 0.9750\n",
            "Epoch 396/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0198 - acc: 0.9725\n",
            "Epoch 397/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0195 - acc: 0.9713\n",
            "Epoch 398/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0197 - acc: 0.9713\n",
            "Epoch 399/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0201 - acc: 0.9663\n",
            "Epoch 400/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0213 - acc: 0.9713\n",
            "200/200 [==============================] - 0s 2ms/step\n",
            "Epoch 1/400\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.0750 - acc: 0.0875\n",
            "Epoch 2/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0573 - acc: 0.1212\n",
            "Epoch 3/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0549 - acc: 0.1575\n",
            "Epoch 4/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0539 - acc: 0.1825\n",
            "Epoch 5/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0523 - acc: 0.2087\n",
            "Epoch 6/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0516 - acc: 0.2337\n",
            "Epoch 7/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0509 - acc: 0.2550\n",
            "Epoch 8/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0504 - acc: 0.3063\n",
            "Epoch 9/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0496 - acc: 0.3300\n",
            "Epoch 10/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0493 - acc: 0.3350\n",
            "Epoch 11/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0484 - acc: 0.3662\n",
            "Epoch 12/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0483 - acc: 0.3775\n",
            "Epoch 13/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0479 - acc: 0.4000\n",
            "Epoch 14/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0471 - acc: 0.4187\n",
            "Epoch 15/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0473 - acc: 0.4312\n",
            "Epoch 16/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0469 - acc: 0.4300\n",
            "Epoch 17/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0461 - acc: 0.4500\n",
            "Epoch 18/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0461 - acc: 0.4625\n",
            "Epoch 19/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0458 - acc: 0.4650\n",
            "Epoch 20/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0461 - acc: 0.4725\n",
            "Epoch 21/400\n",
            "800/800 [==============================] - 0s 192us/step - loss: 0.0460 - acc: 0.4500\n",
            "Epoch 22/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0454 - acc: 0.5088\n",
            "Epoch 23/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0443 - acc: 0.5387\n",
            "Epoch 24/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0449 - acc: 0.5188\n",
            "Epoch 25/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0441 - acc: 0.5388\n",
            "Epoch 26/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0436 - acc: 0.5387\n",
            "Epoch 27/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0430 - acc: 0.5700\n",
            "Epoch 28/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0425 - acc: 0.5762\n",
            "Epoch 29/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0425 - acc: 0.5787\n",
            "Epoch 30/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0425 - acc: 0.5625\n",
            "Epoch 31/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0431 - acc: 0.5675\n",
            "Epoch 32/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0433 - acc: 0.5587\n",
            "Epoch 33/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0415 - acc: 0.6062\n",
            "Epoch 34/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0414 - acc: 0.6075\n",
            "Epoch 35/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0425 - acc: 0.5975\n",
            "Epoch 36/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0412 - acc: 0.6250\n",
            "Epoch 37/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0411 - acc: 0.6313\n",
            "Epoch 38/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0407 - acc: 0.6337\n",
            "Epoch 39/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0411 - acc: 0.6375\n",
            "Epoch 40/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0405 - acc: 0.6487\n",
            "Epoch 41/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0395 - acc: 0.6663\n",
            "Epoch 42/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0392 - acc: 0.6862\n",
            "Epoch 43/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0393 - acc: 0.6925\n",
            "Epoch 44/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0388 - acc: 0.7213\n",
            "Epoch 45/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0396 - acc: 0.6750\n",
            "Epoch 46/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0390 - acc: 0.6912\n",
            "Epoch 47/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0386 - acc: 0.7150\n",
            "Epoch 48/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0385 - acc: 0.7150\n",
            "Epoch 49/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0393 - acc: 0.6950\n",
            "Epoch 50/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0376 - acc: 0.7437\n",
            "Epoch 51/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0374 - acc: 0.7500\n",
            "Epoch 52/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0385 - acc: 0.7350\n",
            "Epoch 53/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0375 - acc: 0.7587\n",
            "Epoch 54/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0376 - acc: 0.7650\n",
            "Epoch 55/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0374 - acc: 0.7588\n",
            "Epoch 56/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0373 - acc: 0.7750\n",
            "Epoch 57/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0366 - acc: 0.7737\n",
            "Epoch 58/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0368 - acc: 0.7587\n",
            "Epoch 59/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0368 - acc: 0.7687\n",
            "Epoch 60/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0360 - acc: 0.7937\n",
            "Epoch 61/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0359 - acc: 0.8025\n",
            "Epoch 62/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0370 - acc: 0.7662\n",
            "Epoch 63/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0355 - acc: 0.8125\n",
            "Epoch 64/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0349 - acc: 0.8238\n",
            "Epoch 65/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0351 - acc: 0.8212\n",
            "Epoch 66/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0357 - acc: 0.8050\n",
            "Epoch 67/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0352 - acc: 0.8137\n",
            "Epoch 68/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0349 - acc: 0.8287\n",
            "Epoch 69/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0356 - acc: 0.8013\n",
            "Epoch 70/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0348 - acc: 0.8337\n",
            "Epoch 71/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0340 - acc: 0.8525\n",
            "Epoch 72/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0333 - acc: 0.8650\n",
            "Epoch 73/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0335 - acc: 0.8713\n",
            "Epoch 74/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0342 - acc: 0.8562\n",
            "Epoch 75/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0335 - acc: 0.8663\n",
            "Epoch 76/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0333 - acc: 0.8713\n",
            "Epoch 77/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0347 - acc: 0.8250\n",
            "Epoch 78/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0334 - acc: 0.8575\n",
            "Epoch 79/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0328 - acc: 0.8762\n",
            "Epoch 80/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0331 - acc: 0.8612\n",
            "Epoch 81/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0339 - acc: 0.8413\n",
            "Epoch 82/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0330 - acc: 0.8600\n",
            "Epoch 83/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0323 - acc: 0.8900\n",
            "Epoch 84/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0331 - acc: 0.8725\n",
            "Epoch 85/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0318 - acc: 0.9075\n",
            "Epoch 86/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0316 - acc: 0.8937\n",
            "Epoch 87/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0312 - acc: 0.9075\n",
            "Epoch 88/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0316 - acc: 0.8975\n",
            "Epoch 89/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0322 - acc: 0.8900\n",
            "Epoch 90/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0316 - acc: 0.9087\n",
            "Epoch 91/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0319 - acc: 0.8875\n",
            "Epoch 92/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0311 - acc: 0.9050\n",
            "Epoch 93/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0320 - acc: 0.8850\n",
            "Epoch 94/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0314 - acc: 0.8887\n",
            "Epoch 95/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0308 - acc: 0.9188\n",
            "Epoch 96/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0305 - acc: 0.9163\n",
            "Epoch 97/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0310 - acc: 0.8937\n",
            "Epoch 98/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0304 - acc: 0.9138\n",
            "Epoch 99/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0308 - acc: 0.9000\n",
            "Epoch 100/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0310 - acc: 0.8963\n",
            "Epoch 101/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0300 - acc: 0.9213\n",
            "Epoch 102/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0297 - acc: 0.9250\n",
            "Epoch 103/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0297 - acc: 0.9200\n",
            "Epoch 104/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0296 - acc: 0.9275\n",
            "Epoch 105/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0293 - acc: 0.9350\n",
            "Epoch 106/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0292 - acc: 0.9250\n",
            "Epoch 107/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0303 - acc: 0.8963\n",
            "Epoch 108/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0296 - acc: 0.9113\n",
            "Epoch 109/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0298 - acc: 0.9150\n",
            "Epoch 110/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0284 - acc: 0.9388\n",
            "Epoch 111/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0287 - acc: 0.9237\n",
            "Epoch 112/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0287 - acc: 0.9313\n",
            "Epoch 113/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0296 - acc: 0.9062\n",
            "Epoch 114/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0299 - acc: 0.9050\n",
            "Epoch 115/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0299 - acc: 0.9125\n",
            "Epoch 116/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0283 - acc: 0.9400\n",
            "Epoch 117/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0278 - acc: 0.9400\n",
            "Epoch 118/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0283 - acc: 0.9312\n",
            "Epoch 119/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0291 - acc: 0.9162\n",
            "Epoch 120/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0279 - acc: 0.9337\n",
            "Epoch 121/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0277 - acc: 0.9300\n",
            "Epoch 122/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0279 - acc: 0.9375\n",
            "Epoch 123/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0277 - acc: 0.9338\n",
            "Epoch 124/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0281 - acc: 0.9325\n",
            "Epoch 125/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0280 - acc: 0.9263\n",
            "Epoch 126/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0271 - acc: 0.9387\n",
            "Epoch 127/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0269 - acc: 0.9400\n",
            "Epoch 128/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0272 - acc: 0.9437\n",
            "Epoch 129/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0269 - acc: 0.9425\n",
            "Epoch 130/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0276 - acc: 0.9263\n",
            "Epoch 131/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0271 - acc: 0.9387\n",
            "Epoch 132/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0265 - acc: 0.9438\n",
            "Epoch 133/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0263 - acc: 0.9488\n",
            "Epoch 134/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0264 - acc: 0.9475\n",
            "Epoch 135/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0264 - acc: 0.9438\n",
            "Epoch 136/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0276 - acc: 0.9300\n",
            "Epoch 137/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0262 - acc: 0.9350\n",
            "Epoch 138/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0268 - acc: 0.9375\n",
            "Epoch 139/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0261 - acc: 0.9487\n",
            "Epoch 140/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0260 - acc: 0.9425\n",
            "Epoch 141/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0265 - acc: 0.9437\n",
            "Epoch 142/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0269 - acc: 0.9312\n",
            "Epoch 143/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0263 - acc: 0.9450\n",
            "Epoch 144/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0256 - acc: 0.9413\n",
            "Epoch 145/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0265 - acc: 0.9375\n",
            "Epoch 146/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0254 - acc: 0.9462\n",
            "Epoch 147/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0252 - acc: 0.9550\n",
            "Epoch 148/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0252 - acc: 0.9538\n",
            "Epoch 149/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0252 - acc: 0.9512\n",
            "Epoch 150/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0249 - acc: 0.9537\n",
            "Epoch 151/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0251 - acc: 0.9525\n",
            "Epoch 152/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0247 - acc: 0.9525\n",
            "Epoch 153/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0252 - acc: 0.9513\n",
            "Epoch 154/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0257 - acc: 0.9438\n",
            "Epoch 155/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0258 - acc: 0.9425\n",
            "Epoch 156/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0247 - acc: 0.9563\n",
            "Epoch 157/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0249 - acc: 0.9488\n",
            "Epoch 158/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0251 - acc: 0.9500\n",
            "Epoch 159/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0247 - acc: 0.9537\n",
            "Epoch 160/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0246 - acc: 0.9537\n",
            "Epoch 161/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0249 - acc: 0.9500\n",
            "Epoch 162/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0244 - acc: 0.9537\n",
            "Epoch 163/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0243 - acc: 0.9450\n",
            "Epoch 164/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0250 - acc: 0.9475\n",
            "Epoch 165/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0245 - acc: 0.9537\n",
            "Epoch 166/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0243 - acc: 0.9550\n",
            "Epoch 167/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0238 - acc: 0.9588\n",
            "Epoch 168/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0239 - acc: 0.9550\n",
            "Epoch 169/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0236 - acc: 0.9538\n",
            "Epoch 170/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0236 - acc: 0.9575\n",
            "Epoch 171/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0235 - acc: 0.9538\n",
            "Epoch 172/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0234 - acc: 0.9613\n",
            "Epoch 173/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0250 - acc: 0.9425\n",
            "Epoch 174/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0233 - acc: 0.9600\n",
            "Epoch 175/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0235 - acc: 0.9512\n",
            "Epoch 176/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0237 - acc: 0.9563\n",
            "Epoch 177/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0239 - acc: 0.9488\n",
            "Epoch 178/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0244 - acc: 0.9525\n",
            "Epoch 179/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0232 - acc: 0.9588\n",
            "Epoch 180/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0234 - acc: 0.9612\n",
            "Epoch 181/400\n",
            "800/800 [==============================] - 0s 190us/step - loss: 0.0235 - acc: 0.9600\n",
            "Epoch 182/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0237 - acc: 0.9438\n",
            "Epoch 183/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0230 - acc: 0.9563\n",
            "Epoch 184/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0231 - acc: 0.9562\n",
            "Epoch 185/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0229 - acc: 0.9550\n",
            "Epoch 186/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0233 - acc: 0.9538\n",
            "Epoch 187/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0228 - acc: 0.9550\n",
            "Epoch 188/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0227 - acc: 0.9638\n",
            "Epoch 189/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0234 - acc: 0.9487\n",
            "Epoch 190/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0233 - acc: 0.9513\n",
            "Epoch 191/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0223 - acc: 0.9625\n",
            "Epoch 192/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0233 - acc: 0.9562\n",
            "Epoch 193/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0230 - acc: 0.9525\n",
            "Epoch 194/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0223 - acc: 0.9512\n",
            "Epoch 195/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0221 - acc: 0.9600\n",
            "Epoch 196/400\n",
            "800/800 [==============================] - 0s 192us/step - loss: 0.0226 - acc: 0.9562\n",
            "Epoch 197/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0229 - acc: 0.9563\n",
            "Epoch 198/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0223 - acc: 0.9600\n",
            "Epoch 199/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0222 - acc: 0.9663\n",
            "Epoch 200/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0226 - acc: 0.9588\n",
            "Epoch 201/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0222 - acc: 0.9588\n",
            "Epoch 202/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0218 - acc: 0.9675\n",
            "Epoch 203/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0219 - acc: 0.9613\n",
            "Epoch 204/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0218 - acc: 0.9650\n",
            "Epoch 205/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0215 - acc: 0.9713\n",
            "Epoch 206/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0217 - acc: 0.9650\n",
            "Epoch 207/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0218 - acc: 0.9588\n",
            "Epoch 208/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0228 - acc: 0.9537\n",
            "Epoch 209/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0221 - acc: 0.9612\n",
            "Epoch 210/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0218 - acc: 0.9625\n",
            "Epoch 211/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0215 - acc: 0.9662\n",
            "Epoch 212/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0216 - acc: 0.9612\n",
            "Epoch 213/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0213 - acc: 0.9662\n",
            "Epoch 214/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0226 - acc: 0.9588\n",
            "Epoch 215/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0217 - acc: 0.9638\n",
            "Epoch 216/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0215 - acc: 0.9600\n",
            "Epoch 217/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0210 - acc: 0.9712\n",
            "Epoch 218/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0210 - acc: 0.9700\n",
            "Epoch 219/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0220 - acc: 0.9625\n",
            "Epoch 220/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0210 - acc: 0.9650\n",
            "Epoch 221/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0207 - acc: 0.9700\n",
            "Epoch 222/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0209 - acc: 0.9650\n",
            "Epoch 223/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0214 - acc: 0.9613\n",
            "Epoch 224/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0210 - acc: 0.9613\n",
            "Epoch 225/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0208 - acc: 0.9713\n",
            "Epoch 226/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0206 - acc: 0.9688\n",
            "Epoch 227/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0207 - acc: 0.9688\n",
            "Epoch 228/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0215 - acc: 0.9613\n",
            "Epoch 229/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0217 - acc: 0.9563\n",
            "Epoch 230/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0212 - acc: 0.9550\n",
            "Epoch 231/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0210 - acc: 0.9650\n",
            "Epoch 232/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0208 - acc: 0.9700\n",
            "Epoch 233/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0207 - acc: 0.9688\n",
            "Epoch 234/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0205 - acc: 0.9713\n",
            "Epoch 235/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0203 - acc: 0.9675\n",
            "Epoch 236/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0203 - acc: 0.9675\n",
            "Epoch 237/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0201 - acc: 0.9713\n",
            "Epoch 238/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0201 - acc: 0.9700\n",
            "Epoch 239/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0200 - acc: 0.9738\n",
            "Epoch 240/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0201 - acc: 0.9700\n",
            "Epoch 241/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0202 - acc: 0.9650\n",
            "Epoch 242/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0201 - acc: 0.9688\n",
            "Epoch 243/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0199 - acc: 0.9700\n",
            "Epoch 244/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0199 - acc: 0.9713\n",
            "Epoch 245/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0202 - acc: 0.9625\n",
            "Epoch 246/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0204 - acc: 0.9675\n",
            "Epoch 247/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0197 - acc: 0.9713\n",
            "Epoch 248/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0205 - acc: 0.9600\n",
            "Epoch 249/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0208 - acc: 0.9650\n",
            "Epoch 250/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0196 - acc: 0.9713\n",
            "Epoch 251/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0197 - acc: 0.9700\n",
            "Epoch 252/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0198 - acc: 0.9738\n",
            "Epoch 253/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0195 - acc: 0.9750\n",
            "Epoch 254/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0195 - acc: 0.9750\n",
            "Epoch 255/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0195 - acc: 0.9713\n",
            "Epoch 256/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0195 - acc: 0.9738\n",
            "Epoch 257/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0195 - acc: 0.9725\n",
            "Epoch 258/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0194 - acc: 0.9675\n",
            "Epoch 259/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0194 - acc: 0.9675\n",
            "Epoch 260/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0195 - acc: 0.9700\n",
            "Epoch 261/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0204 - acc: 0.9612\n",
            "Epoch 262/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0193 - acc: 0.9750\n",
            "Epoch 263/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0190 - acc: 0.9750\n",
            "Epoch 264/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0190 - acc: 0.9750\n",
            "Epoch 265/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0200 - acc: 0.9713\n",
            "Epoch 266/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0191 - acc: 0.9700\n",
            "Epoch 267/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0192 - acc: 0.9700\n",
            "Epoch 268/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0189 - acc: 0.9763\n",
            "Epoch 269/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0189 - acc: 0.9750\n",
            "Epoch 270/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0187 - acc: 0.9763\n",
            "Epoch 271/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0188 - acc: 0.9775\n",
            "Epoch 272/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0192 - acc: 0.9700\n",
            "Epoch 273/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0191 - acc: 0.9725\n",
            "Epoch 274/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0191 - acc: 0.9725\n",
            "Epoch 275/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0191 - acc: 0.9725\n",
            "Epoch 276/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0185 - acc: 0.9788\n",
            "Epoch 277/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0184 - acc: 0.9800\n",
            "Epoch 278/400\n",
            "800/800 [==============================] - 0s 155us/step - loss: 0.0185 - acc: 0.9763\n",
            "Epoch 279/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0187 - acc: 0.9688\n",
            "Epoch 280/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0186 - acc: 0.9750\n",
            "Epoch 281/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0185 - acc: 0.9813\n",
            "Epoch 282/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0188 - acc: 0.9663\n",
            "Epoch 283/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0188 - acc: 0.9738\n",
            "Epoch 284/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0187 - acc: 0.9725\n",
            "Epoch 285/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0187 - acc: 0.9713\n",
            "Epoch 286/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0182 - acc: 0.9788\n",
            "Epoch 287/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0182 - acc: 0.9750\n",
            "Epoch 288/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0186 - acc: 0.9750\n",
            "Epoch 289/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0183 - acc: 0.9725\n",
            "Epoch 290/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0182 - acc: 0.9763\n",
            "Epoch 291/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0180 - acc: 0.9775\n",
            "Epoch 292/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0180 - acc: 0.9788\n",
            "Epoch 293/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0183 - acc: 0.9750\n",
            "Epoch 294/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0191 - acc: 0.9738\n",
            "Epoch 295/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0185 - acc: 0.9738\n",
            "Epoch 296/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0178 - acc: 0.9813\n",
            "Epoch 297/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0179 - acc: 0.9788\n",
            "Epoch 298/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0179 - acc: 0.9688\n",
            "Epoch 299/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0181 - acc: 0.9788\n",
            "Epoch 300/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0179 - acc: 0.9800\n",
            "Epoch 301/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0178 - acc: 0.9788\n",
            "Epoch 302/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0179 - acc: 0.9800\n",
            "Epoch 303/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0181 - acc: 0.9775\n",
            "Epoch 304/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0180 - acc: 0.9788\n",
            "Epoch 305/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0176 - acc: 0.9775\n",
            "Epoch 306/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0175 - acc: 0.9800\n",
            "Epoch 307/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0175 - acc: 0.9775\n",
            "Epoch 308/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0181 - acc: 0.9763\n",
            "Epoch 309/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0176 - acc: 0.9775\n",
            "Epoch 310/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0173 - acc: 0.9800\n",
            "Epoch 311/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0173 - acc: 0.9788\n",
            "Epoch 312/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0179 - acc: 0.9738\n",
            "Epoch 313/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0177 - acc: 0.9763\n",
            "Epoch 314/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0174 - acc: 0.9763\n",
            "Epoch 315/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0175 - acc: 0.9788\n",
            "Epoch 316/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0175 - acc: 0.9763\n",
            "Epoch 317/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0177 - acc: 0.9800\n",
            "Epoch 318/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0180 - acc: 0.9775\n",
            "Epoch 319/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0175 - acc: 0.9713\n",
            "Epoch 320/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0173 - acc: 0.9725\n",
            "Epoch 321/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0176 - acc: 0.9788\n",
            "Epoch 322/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0170 - acc: 0.9850\n",
            "Epoch 323/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0170 - acc: 0.9838\n",
            "Epoch 324/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0176 - acc: 0.9813\n",
            "Epoch 325/400\n",
            "800/800 [==============================] - 0s 189us/step - loss: 0.0174 - acc: 0.9875\n",
            "Epoch 326/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0167 - acc: 0.9800\n",
            "Epoch 327/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0169 - acc: 0.9788\n",
            "Epoch 328/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0169 - acc: 0.9825\n",
            "Epoch 329/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0167 - acc: 0.9838\n",
            "Epoch 330/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0168 - acc: 0.9813\n",
            "Epoch 331/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0166 - acc: 0.9863\n",
            "Epoch 332/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0168 - acc: 0.9813\n",
            "Epoch 333/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0167 - acc: 0.9813\n",
            "Epoch 334/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0169 - acc: 0.9813\n",
            "Epoch 335/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0170 - acc: 0.9800\n",
            "Epoch 336/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0169 - acc: 0.9775\n",
            "Epoch 337/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0174 - acc: 0.9800\n",
            "Epoch 338/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0165 - acc: 0.9800\n",
            "Epoch 339/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0171 - acc: 0.9775\n",
            "Epoch 340/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0167 - acc: 0.9800\n",
            "Epoch 341/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0166 - acc: 0.9813\n",
            "Epoch 342/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0163 - acc: 0.9838\n",
            "Epoch 343/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0162 - acc: 0.9850\n",
            "Epoch 344/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0163 - acc: 0.9838\n",
            "Epoch 345/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0163 - acc: 0.9825\n",
            "Epoch 346/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0163 - acc: 0.9813\n",
            "Epoch 347/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0164 - acc: 0.9825\n",
            "Epoch 348/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0164 - acc: 0.9863\n",
            "Epoch 349/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0164 - acc: 0.9800\n",
            "Epoch 350/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0165 - acc: 0.9825\n",
            "Epoch 351/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0166 - acc: 0.9788\n",
            "Epoch 352/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0167 - acc: 0.9800\n",
            "Epoch 353/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0163 - acc: 0.9825\n",
            "Epoch 354/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0163 - acc: 0.9788\n",
            "Epoch 355/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0168 - acc: 0.9788\n",
            "Epoch 356/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0161 - acc: 0.9838\n",
            "Epoch 357/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0165 - acc: 0.9850\n",
            "Epoch 358/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0159 - acc: 0.9838\n",
            "Epoch 359/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0158 - acc: 0.9850\n",
            "Epoch 360/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0164 - acc: 0.9838\n",
            "Epoch 361/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0162 - acc: 0.9838\n",
            "Epoch 362/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0159 - acc: 0.9863\n",
            "Epoch 363/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0164 - acc: 0.9800\n",
            "Epoch 364/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0161 - acc: 0.9813\n",
            "Epoch 365/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0161 - acc: 0.9825\n",
            "Epoch 366/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0162 - acc: 0.9800\n",
            "Epoch 367/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0159 - acc: 0.9850\n",
            "Epoch 368/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0162 - acc: 0.9788\n",
            "Epoch 369/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0163 - acc: 0.9825\n",
            "Epoch 370/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0159 - acc: 0.9813\n",
            "Epoch 371/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0155 - acc: 0.9863\n",
            "Epoch 372/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0159 - acc: 0.9850\n",
            "Epoch 373/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0164 - acc: 0.9800\n",
            "Epoch 374/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0156 - acc: 0.9863\n",
            "Epoch 375/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0156 - acc: 0.9850\n",
            "Epoch 376/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0155 - acc: 0.9825\n",
            "Epoch 377/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0154 - acc: 0.9838\n",
            "Epoch 378/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0155 - acc: 0.9863\n",
            "Epoch 379/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0153 - acc: 0.9863\n",
            "Epoch 380/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0152 - acc: 0.9850\n",
            "Epoch 381/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0151 - acc: 0.9863\n",
            "Epoch 382/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0151 - acc: 0.9875\n",
            "Epoch 383/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0151 - acc: 0.9850\n",
            "Epoch 384/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0151 - acc: 0.9863\n",
            "Epoch 385/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0152 - acc: 0.9888\n",
            "Epoch 386/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0159 - acc: 0.9838\n",
            "Epoch 387/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0157 - acc: 0.9875\n",
            "Epoch 388/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0152 - acc: 0.9875\n",
            "Epoch 389/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0154 - acc: 0.9850\n",
            "Epoch 390/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0150 - acc: 0.9863\n",
            "Epoch 391/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0150 - acc: 0.9863\n",
            "Epoch 392/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0152 - acc: 0.9838\n",
            "Epoch 393/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0151 - acc: 0.9875\n",
            "Epoch 394/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0150 - acc: 0.9838\n",
            "Epoch 395/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0153 - acc: 0.9863\n",
            "Epoch 396/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0149 - acc: 0.9875\n",
            "Epoch 397/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0148 - acc: 0.9863\n",
            "Epoch 398/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0148 - acc: 0.9863\n",
            "Epoch 399/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0148 - acc: 0.9863\n",
            "Epoch 400/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0148 - acc: 0.9850\n",
            "200/200 [==============================] - 1s 3ms/step\n",
            "Epoch 1/400\n",
            "800/800 [==============================] - 2s 2ms/step - loss: 0.0682 - acc: 0.0437\n",
            "Epoch 2/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0569 - acc: 0.0650\n",
            "Epoch 3/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0553 - acc: 0.0787\n",
            "Epoch 4/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0542 - acc: 0.1225\n",
            "Epoch 5/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0532 - acc: 0.1425\n",
            "Epoch 6/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0526 - acc: 0.1800\n",
            "Epoch 7/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0518 - acc: 0.1887\n",
            "Epoch 8/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0512 - acc: 0.2263\n",
            "Epoch 9/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0506 - acc: 0.2512\n",
            "Epoch 10/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0503 - acc: 0.2637\n",
            "Epoch 11/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0499 - acc: 0.2750\n",
            "Epoch 12/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0496 - acc: 0.2963\n",
            "Epoch 13/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0491 - acc: 0.3038\n",
            "Epoch 14/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0488 - acc: 0.3175\n",
            "Epoch 15/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0485 - acc: 0.3300\n",
            "Epoch 16/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0482 - acc: 0.3363\n",
            "Epoch 17/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0482 - acc: 0.3450\n",
            "Epoch 18/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0476 - acc: 0.3663\n",
            "Epoch 19/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0472 - acc: 0.3875\n",
            "Epoch 20/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0470 - acc: 0.3925\n",
            "Epoch 21/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0468 - acc: 0.4175\n",
            "Epoch 22/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0465 - acc: 0.4287\n",
            "Epoch 23/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0463 - acc: 0.4275\n",
            "Epoch 24/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0463 - acc: 0.4350\n",
            "Epoch 25/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0459 - acc: 0.4650\n",
            "Epoch 26/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0456 - acc: 0.4625\n",
            "Epoch 27/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0455 - acc: 0.4725\n",
            "Epoch 28/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0453 - acc: 0.4687\n",
            "Epoch 29/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0449 - acc: 0.4925\n",
            "Epoch 30/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0447 - acc: 0.5100\n",
            "Epoch 31/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0448 - acc: 0.5175\n",
            "Epoch 32/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0444 - acc: 0.5163\n",
            "Epoch 33/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0443 - acc: 0.5162\n",
            "Epoch 34/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0439 - acc: 0.5287\n",
            "Epoch 35/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0437 - acc: 0.5450\n",
            "Epoch 36/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0438 - acc: 0.5275\n",
            "Epoch 37/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0436 - acc: 0.5475\n",
            "Epoch 38/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0431 - acc: 0.5775\n",
            "Epoch 39/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0432 - acc: 0.5712\n",
            "Epoch 40/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0428 - acc: 0.5700\n",
            "Epoch 41/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0428 - acc: 0.5913\n",
            "Epoch 42/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0428 - acc: 0.6013\n",
            "Epoch 43/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0425 - acc: 0.5988\n",
            "Epoch 44/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0421 - acc: 0.6175\n",
            "Epoch 45/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0420 - acc: 0.6337\n",
            "Epoch 46/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0418 - acc: 0.6287\n",
            "Epoch 47/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0419 - acc: 0.6250\n",
            "Epoch 48/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0415 - acc: 0.6487\n",
            "Epoch 49/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0414 - acc: 0.6475\n",
            "Epoch 50/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0413 - acc: 0.6563\n",
            "Epoch 51/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0413 - acc: 0.6500\n",
            "Epoch 52/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0411 - acc: 0.6562\n",
            "Epoch 53/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0410 - acc: 0.6612\n",
            "Epoch 54/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0408 - acc: 0.6775\n",
            "Epoch 55/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0406 - acc: 0.6888\n",
            "Epoch 56/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0408 - acc: 0.6775\n",
            "Epoch 57/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0407 - acc: 0.6525\n",
            "Epoch 58/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0407 - acc: 0.6825\n",
            "Epoch 59/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0402 - acc: 0.6975\n",
            "Epoch 60/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0406 - acc: 0.6812\n",
            "Epoch 61/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0403 - acc: 0.6950\n",
            "Epoch 62/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0397 - acc: 0.7237\n",
            "Epoch 63/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0397 - acc: 0.7250\n",
            "Epoch 64/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0398 - acc: 0.7138\n",
            "Epoch 65/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0398 - acc: 0.7137\n",
            "Epoch 66/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0396 - acc: 0.7238\n",
            "Epoch 67/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0392 - acc: 0.7375\n",
            "Epoch 68/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0393 - acc: 0.7325\n",
            "Epoch 69/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0390 - acc: 0.7563\n",
            "Epoch 70/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0390 - acc: 0.7512\n",
            "Epoch 71/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0389 - acc: 0.7637\n",
            "Epoch 72/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0387 - acc: 0.7475\n",
            "Epoch 73/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0386 - acc: 0.7638\n",
            "Epoch 74/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0389 - acc: 0.7500\n",
            "Epoch 75/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0389 - acc: 0.7500\n",
            "Epoch 76/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0389 - acc: 0.7513\n",
            "Epoch 77/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0382 - acc: 0.7637\n",
            "Epoch 78/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0384 - acc: 0.7712\n",
            "Epoch 79/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0379 - acc: 0.7762\n",
            "Epoch 80/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0380 - acc: 0.7800\n",
            "Epoch 81/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0379 - acc: 0.7812\n",
            "Epoch 82/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0383 - acc: 0.7587\n",
            "Epoch 83/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0376 - acc: 0.7850\n",
            "Epoch 84/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0378 - acc: 0.7737\n",
            "Epoch 85/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0377 - acc: 0.7788\n",
            "Epoch 86/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0373 - acc: 0.8087\n",
            "Epoch 87/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0372 - acc: 0.8100\n",
            "Epoch 88/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0372 - acc: 0.8013\n",
            "Epoch 89/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0370 - acc: 0.8087\n",
            "Epoch 90/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0371 - acc: 0.8200\n",
            "Epoch 91/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0372 - acc: 0.7987\n",
            "Epoch 92/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0371 - acc: 0.7987\n",
            "Epoch 93/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0368 - acc: 0.8162\n",
            "Epoch 94/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0369 - acc: 0.8150\n",
            "Epoch 95/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0366 - acc: 0.8188\n",
            "Epoch 96/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0367 - acc: 0.8188\n",
            "Epoch 97/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0364 - acc: 0.8275\n",
            "Epoch 98/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0364 - acc: 0.8200\n",
            "Epoch 99/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0364 - acc: 0.8200\n",
            "Epoch 100/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0364 - acc: 0.8212\n",
            "Epoch 101/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0366 - acc: 0.8350\n",
            "Epoch 102/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0360 - acc: 0.8275\n",
            "Epoch 103/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0363 - acc: 0.8250\n",
            "Epoch 104/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0361 - acc: 0.8400\n",
            "Epoch 105/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0357 - acc: 0.8437\n",
            "Epoch 106/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0358 - acc: 0.8462\n",
            "Epoch 107/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0359 - acc: 0.8412\n",
            "Epoch 108/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0361 - acc: 0.8338\n",
            "Epoch 109/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0359 - acc: 0.8300\n",
            "Epoch 110/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0354 - acc: 0.8438\n",
            "Epoch 111/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0355 - acc: 0.8488\n",
            "Epoch 112/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0357 - acc: 0.8350\n",
            "Epoch 113/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0355 - acc: 0.8525\n",
            "Epoch 114/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0351 - acc: 0.8563\n",
            "Epoch 115/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0351 - acc: 0.8637\n",
            "Epoch 116/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0350 - acc: 0.8588\n",
            "Epoch 117/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0351 - acc: 0.8462\n",
            "Epoch 118/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0348 - acc: 0.8687\n",
            "Epoch 119/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0350 - acc: 0.8600\n",
            "Epoch 120/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0350 - acc: 0.8575\n",
            "Epoch 121/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0346 - acc: 0.8700\n",
            "Epoch 122/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0345 - acc: 0.8625\n",
            "Epoch 123/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0344 - acc: 0.8687\n",
            "Epoch 124/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0346 - acc: 0.8700\n",
            "Epoch 125/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0347 - acc: 0.8575\n",
            "Epoch 126/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0347 - acc: 0.8563\n",
            "Epoch 127/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0344 - acc: 0.8675\n",
            "Epoch 128/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0344 - acc: 0.8738\n",
            "Epoch 129/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0344 - acc: 0.8725\n",
            "Epoch 130/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0344 - acc: 0.8725\n",
            "Epoch 131/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0341 - acc: 0.8713\n",
            "Epoch 132/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0342 - acc: 0.8725\n",
            "Epoch 133/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0339 - acc: 0.8825\n",
            "Epoch 134/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0339 - acc: 0.8813\n",
            "Epoch 135/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0342 - acc: 0.8738\n",
            "Epoch 136/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0339 - acc: 0.8750\n",
            "Epoch 137/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0338 - acc: 0.8900\n",
            "Epoch 138/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0336 - acc: 0.8850\n",
            "Epoch 139/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0338 - acc: 0.8825\n",
            "Epoch 140/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0336 - acc: 0.8937\n",
            "Epoch 141/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0336 - acc: 0.8837\n",
            "Epoch 142/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0335 - acc: 0.8912\n",
            "Epoch 143/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0333 - acc: 0.8925\n",
            "Epoch 144/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0332 - acc: 0.8875\n",
            "Epoch 145/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0332 - acc: 0.8900\n",
            "Epoch 146/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0334 - acc: 0.8825\n",
            "Epoch 147/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0333 - acc: 0.8825\n",
            "Epoch 148/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0334 - acc: 0.8837\n",
            "Epoch 149/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0335 - acc: 0.8837\n",
            "Epoch 150/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0330 - acc: 0.8975\n",
            "Epoch 151/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0331 - acc: 0.8862\n",
            "Epoch 152/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0329 - acc: 0.9000\n",
            "Epoch 153/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0332 - acc: 0.8863\n",
            "Epoch 154/400\n",
            "800/800 [==============================] - 0s 192us/step - loss: 0.0329 - acc: 0.8950\n",
            "Epoch 155/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0326 - acc: 0.8988\n",
            "Epoch 156/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0329 - acc: 0.8900\n",
            "Epoch 157/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0328 - acc: 0.9013\n",
            "Epoch 158/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0328 - acc: 0.8987\n",
            "Epoch 159/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0330 - acc: 0.8925\n",
            "Epoch 160/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0323 - acc: 0.9013\n",
            "Epoch 161/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0322 - acc: 0.8987\n",
            "Epoch 162/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0327 - acc: 0.8987\n",
            "Epoch 163/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0321 - acc: 0.9088\n",
            "Epoch 164/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0324 - acc: 0.9100\n",
            "Epoch 165/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0320 - acc: 0.9025\n",
            "Epoch 166/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0321 - acc: 0.9088\n",
            "Epoch 167/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0322 - acc: 0.9125\n",
            "Epoch 168/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0317 - acc: 0.9138\n",
            "Epoch 169/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0316 - acc: 0.9062\n",
            "Epoch 170/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0320 - acc: 0.9050\n",
            "Epoch 171/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0316 - acc: 0.9075\n",
            "Epoch 172/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0316 - acc: 0.9175\n",
            "Epoch 173/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0319 - acc: 0.9138\n",
            "Epoch 174/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0319 - acc: 0.8963\n",
            "Epoch 175/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0317 - acc: 0.9112\n",
            "Epoch 176/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0315 - acc: 0.9113\n",
            "Epoch 177/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0315 - acc: 0.9038\n",
            "Epoch 178/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0316 - acc: 0.9188\n",
            "Epoch 179/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0312 - acc: 0.9062\n",
            "Epoch 180/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0314 - acc: 0.9188\n",
            "Epoch 181/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0311 - acc: 0.9163\n",
            "Epoch 182/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0311 - acc: 0.9163\n",
            "Epoch 183/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0310 - acc: 0.9188\n",
            "Epoch 184/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0309 - acc: 0.9162\n",
            "Epoch 185/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0308 - acc: 0.9213\n",
            "Epoch 186/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0310 - acc: 0.9087\n",
            "Epoch 187/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0321 - acc: 0.9050\n",
            "Epoch 188/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0311 - acc: 0.9263\n",
            "Epoch 189/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0310 - acc: 0.9225\n",
            "Epoch 190/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0306 - acc: 0.9263\n",
            "Epoch 191/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0305 - acc: 0.9225\n",
            "Epoch 192/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0307 - acc: 0.9200\n",
            "Epoch 193/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0306 - acc: 0.9237\n",
            "Epoch 194/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0305 - acc: 0.9225\n",
            "Epoch 195/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0303 - acc: 0.9250\n",
            "Epoch 196/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0309 - acc: 0.9100\n",
            "Epoch 197/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0304 - acc: 0.9200\n",
            "Epoch 198/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0308 - acc: 0.9100\n",
            "Epoch 199/400\n",
            "800/800 [==============================] - 0s 196us/step - loss: 0.0302 - acc: 0.9263\n",
            "Epoch 200/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0300 - acc: 0.9312\n",
            "Epoch 201/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0301 - acc: 0.9350\n",
            "Epoch 202/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0303 - acc: 0.9200\n",
            "Epoch 203/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0304 - acc: 0.9237\n",
            "Epoch 204/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0301 - acc: 0.9275\n",
            "Epoch 205/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0302 - acc: 0.9363\n",
            "Epoch 206/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0302 - acc: 0.9225\n",
            "Epoch 207/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0300 - acc: 0.9288\n",
            "Epoch 208/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0300 - acc: 0.9250\n",
            "Epoch 209/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0297 - acc: 0.9325\n",
            "Epoch 210/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0297 - acc: 0.9325\n",
            "Epoch 211/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0302 - acc: 0.9200\n",
            "Epoch 212/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0300 - acc: 0.9225\n",
            "Epoch 213/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0297 - acc: 0.9263\n",
            "Epoch 214/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0294 - acc: 0.9275\n",
            "Epoch 215/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0293 - acc: 0.9350\n",
            "Epoch 216/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0292 - acc: 0.9350\n",
            "Epoch 217/400\n",
            "800/800 [==============================] - 0s 155us/step - loss: 0.0296 - acc: 0.9250\n",
            "Epoch 218/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0295 - acc: 0.9250\n",
            "Epoch 219/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0290 - acc: 0.9388\n",
            "Epoch 220/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0290 - acc: 0.9362\n",
            "Epoch 221/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0290 - acc: 0.9325\n",
            "Epoch 222/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0292 - acc: 0.9362\n",
            "Epoch 223/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0294 - acc: 0.9275\n",
            "Epoch 224/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0294 - acc: 0.9363\n",
            "Epoch 225/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0291 - acc: 0.9388\n",
            "Epoch 226/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0294 - acc: 0.9313\n",
            "Epoch 227/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0293 - acc: 0.9387\n",
            "Epoch 228/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0289 - acc: 0.9450\n",
            "Epoch 229/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0286 - acc: 0.9375\n",
            "Epoch 230/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0285 - acc: 0.9387\n",
            "Epoch 231/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0285 - acc: 0.9425\n",
            "Epoch 232/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0284 - acc: 0.9387\n",
            "Epoch 233/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0286 - acc: 0.9425\n",
            "Epoch 234/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0290 - acc: 0.9350\n",
            "Epoch 235/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0285 - acc: 0.9387\n",
            "Epoch 236/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0283 - acc: 0.9387\n",
            "Epoch 237/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0287 - acc: 0.9362\n",
            "Epoch 238/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0281 - acc: 0.9488\n",
            "Epoch 239/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0285 - acc: 0.9363\n",
            "Epoch 240/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0288 - acc: 0.9325\n",
            "Epoch 241/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0283 - acc: 0.9362\n",
            "Epoch 242/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0280 - acc: 0.9475\n",
            "Epoch 243/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0285 - acc: 0.9350\n",
            "Epoch 244/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0281 - acc: 0.9413\n",
            "Epoch 245/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0280 - acc: 0.9437\n",
            "Epoch 246/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0278 - acc: 0.9412\n",
            "Epoch 247/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0277 - acc: 0.9487\n",
            "Epoch 248/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0276 - acc: 0.9475\n",
            "Epoch 249/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0276 - acc: 0.9462\n",
            "Epoch 250/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0282 - acc: 0.9375\n",
            "Epoch 251/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0282 - acc: 0.9375\n",
            "Epoch 252/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0279 - acc: 0.9437\n",
            "Epoch 253/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0277 - acc: 0.9463\n",
            "Epoch 254/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0276 - acc: 0.9425\n",
            "Epoch 255/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0274 - acc: 0.9488\n",
            "Epoch 256/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0276 - acc: 0.9425\n",
            "Epoch 257/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0275 - acc: 0.9500\n",
            "Epoch 258/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0275 - acc: 0.9500\n",
            "Epoch 259/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0276 - acc: 0.9425\n",
            "Epoch 260/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0276 - acc: 0.9537\n",
            "Epoch 261/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0273 - acc: 0.9425\n",
            "Epoch 262/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0278 - acc: 0.9338\n",
            "Epoch 263/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0277 - acc: 0.9500\n",
            "Epoch 264/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0275 - acc: 0.9375\n",
            "Epoch 265/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0278 - acc: 0.9475\n",
            "Epoch 266/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0272 - acc: 0.9463\n",
            "Epoch 267/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0278 - acc: 0.9412\n",
            "Epoch 268/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0273 - acc: 0.9475\n",
            "Epoch 269/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0276 - acc: 0.9450\n",
            "Epoch 270/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0277 - acc: 0.9387\n",
            "Epoch 271/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0270 - acc: 0.9537\n",
            "Epoch 272/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0268 - acc: 0.9488\n",
            "Epoch 273/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0273 - acc: 0.9337\n",
            "Epoch 274/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0268 - acc: 0.9512\n",
            "Epoch 275/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0267 - acc: 0.9488\n",
            "Epoch 276/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0270 - acc: 0.9438\n",
            "Epoch 277/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0270 - acc: 0.9500\n",
            "Epoch 278/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0269 - acc: 0.9438\n",
            "Epoch 279/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0266 - acc: 0.9475\n",
            "Epoch 280/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0266 - acc: 0.9513\n",
            "Epoch 281/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0267 - acc: 0.9475\n",
            "Epoch 282/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0268 - acc: 0.9413\n",
            "Epoch 283/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0264 - acc: 0.9538\n",
            "Epoch 284/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0266 - acc: 0.9462\n",
            "Epoch 285/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0262 - acc: 0.9587\n",
            "Epoch 286/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0265 - acc: 0.9538\n",
            "Epoch 287/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0265 - acc: 0.9462\n",
            "Epoch 288/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0267 - acc: 0.9437\n",
            "Epoch 289/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0265 - acc: 0.9525\n",
            "Epoch 290/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0265 - acc: 0.9475\n",
            "Epoch 291/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0263 - acc: 0.9500\n",
            "Epoch 292/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0260 - acc: 0.9525\n",
            "Epoch 293/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0262 - acc: 0.9500\n",
            "Epoch 294/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0262 - acc: 0.9500\n",
            "Epoch 295/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0259 - acc: 0.9525\n",
            "Epoch 296/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0260 - acc: 0.9538\n",
            "Epoch 297/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0259 - acc: 0.9475\n",
            "Epoch 298/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0258 - acc: 0.9575\n",
            "Epoch 299/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0258 - acc: 0.9550\n",
            "Epoch 300/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0259 - acc: 0.9525\n",
            "Epoch 301/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0259 - acc: 0.9563\n",
            "Epoch 302/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0259 - acc: 0.9525\n",
            "Epoch 303/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0259 - acc: 0.9500\n",
            "Epoch 304/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0254 - acc: 0.9575\n",
            "Epoch 305/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0258 - acc: 0.9475\n",
            "Epoch 306/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0255 - acc: 0.9588\n",
            "Epoch 307/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0264 - acc: 0.9525\n",
            "Epoch 308/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0260 - acc: 0.9500\n",
            "Epoch 309/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0260 - acc: 0.9475\n",
            "Epoch 310/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0256 - acc: 0.9550\n",
            "Epoch 311/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0255 - acc: 0.9537\n",
            "Epoch 312/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0252 - acc: 0.9588\n",
            "Epoch 313/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0260 - acc: 0.9500\n",
            "Epoch 314/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0260 - acc: 0.9438\n",
            "Epoch 315/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0250 - acc: 0.9575\n",
            "Epoch 316/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0251 - acc: 0.9600\n",
            "Epoch 317/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0250 - acc: 0.9612\n",
            "Epoch 318/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0255 - acc: 0.9537\n",
            "Epoch 319/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0252 - acc: 0.9563\n",
            "Epoch 320/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0249 - acc: 0.9588\n",
            "Epoch 321/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0250 - acc: 0.9600\n",
            "Epoch 322/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0249 - acc: 0.9612\n",
            "Epoch 323/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0255 - acc: 0.9512\n",
            "Epoch 324/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0247 - acc: 0.9625\n",
            "Epoch 325/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0252 - acc: 0.9537\n",
            "Epoch 326/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0249 - acc: 0.9600\n",
            "Epoch 327/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0248 - acc: 0.9563\n",
            "Epoch 328/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0246 - acc: 0.9638\n",
            "Epoch 329/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0249 - acc: 0.9600\n",
            "Epoch 330/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0245 - acc: 0.9625\n",
            "Epoch 331/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0252 - acc: 0.9563\n",
            "Epoch 332/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0248 - acc: 0.9613\n",
            "Epoch 333/400\n",
            "800/800 [==============================] - 0s 200us/step - loss: 0.0248 - acc: 0.9613\n",
            "Epoch 334/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0248 - acc: 0.9588\n",
            "Epoch 335/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0246 - acc: 0.9612\n",
            "Epoch 336/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0246 - acc: 0.9600\n",
            "Epoch 337/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0247 - acc: 0.9563\n",
            "Epoch 338/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0249 - acc: 0.9600\n",
            "Epoch 339/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0245 - acc: 0.9600\n",
            "Epoch 340/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0243 - acc: 0.9650\n",
            "Epoch 341/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0243 - acc: 0.9587\n",
            "Epoch 342/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0242 - acc: 0.9638\n",
            "Epoch 343/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0241 - acc: 0.9650\n",
            "Epoch 344/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0240 - acc: 0.9625\n",
            "Epoch 345/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0239 - acc: 0.9663\n",
            "Epoch 346/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0242 - acc: 0.9650\n",
            "Epoch 347/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0241 - acc: 0.9638\n",
            "Epoch 348/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0239 - acc: 0.9650\n",
            "Epoch 349/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0238 - acc: 0.9638\n",
            "Epoch 350/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0239 - acc: 0.9625\n",
            "Epoch 351/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0242 - acc: 0.9587\n",
            "Epoch 352/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0242 - acc: 0.9588\n",
            "Epoch 353/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0241 - acc: 0.9625\n",
            "Epoch 354/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0241 - acc: 0.9588\n",
            "Epoch 355/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0239 - acc: 0.9587\n",
            "Epoch 356/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0241 - acc: 0.9625\n",
            "Epoch 357/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0238 - acc: 0.9650\n",
            "Epoch 358/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0238 - acc: 0.9613\n",
            "Epoch 359/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0238 - acc: 0.9600\n",
            "Epoch 360/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0242 - acc: 0.9575\n",
            "Epoch 361/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0237 - acc: 0.9663\n",
            "Epoch 362/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0238 - acc: 0.9612\n",
            "Epoch 363/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0241 - acc: 0.9638\n",
            "Epoch 364/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0238 - acc: 0.9600\n",
            "Epoch 365/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0234 - acc: 0.9650\n",
            "Epoch 366/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0236 - acc: 0.9663\n",
            "Epoch 367/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0239 - acc: 0.9625\n",
            "Epoch 368/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0232 - acc: 0.9675\n",
            "Epoch 369/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0234 - acc: 0.9663\n",
            "Epoch 370/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0234 - acc: 0.9675\n",
            "Epoch 371/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0233 - acc: 0.9688\n",
            "Epoch 372/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0235 - acc: 0.9700\n",
            "Epoch 373/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0234 - acc: 0.9688\n",
            "Epoch 374/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0233 - acc: 0.9663\n",
            "Epoch 375/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0232 - acc: 0.9700\n",
            "Epoch 376/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0229 - acc: 0.9675\n",
            "Epoch 377/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0231 - acc: 0.9713\n",
            "Epoch 378/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0232 - acc: 0.9638\n",
            "Epoch 379/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0230 - acc: 0.9663\n",
            "Epoch 380/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0231 - acc: 0.9625\n",
            "Epoch 381/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0231 - acc: 0.9675\n",
            "Epoch 382/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0239 - acc: 0.9613\n",
            "Epoch 383/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0230 - acc: 0.9688\n",
            "Epoch 384/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0232 - acc: 0.9625\n",
            "Epoch 385/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0230 - acc: 0.9675\n",
            "Epoch 386/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0227 - acc: 0.9713\n",
            "Epoch 387/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0226 - acc: 0.9675\n",
            "Epoch 388/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0226 - acc: 0.9688\n",
            "Epoch 389/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0225 - acc: 0.9750\n",
            "Epoch 390/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0225 - acc: 0.9688\n",
            "Epoch 391/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0228 - acc: 0.9700\n",
            "Epoch 392/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0227 - acc: 0.9688\n",
            "Epoch 393/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0228 - acc: 0.9650\n",
            "Epoch 394/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0228 - acc: 0.9637\n",
            "Epoch 395/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0229 - acc: 0.9612\n",
            "Epoch 396/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0224 - acc: 0.9675\n",
            "Epoch 397/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0224 - acc: 0.9700\n",
            "Epoch 398/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0222 - acc: 0.9713\n",
            "Epoch 399/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0224 - acc: 0.9750\n",
            "Epoch 400/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0225 - acc: 0.9725\n",
            "200/200 [==============================] - 1s 3ms/step\n",
            "Epoch 1/400\n",
            "800/800 [==============================] - 2s 2ms/step - loss: 0.0879 - acc: 0.0600\n",
            "Epoch 2/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0586 - acc: 0.0588\n",
            "Epoch 3/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0560 - acc: 0.0637\n",
            "Epoch 4/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0544 - acc: 0.0887\n",
            "Epoch 5/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0535 - acc: 0.1000\n",
            "Epoch 6/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0527 - acc: 0.1100\n",
            "Epoch 7/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0519 - acc: 0.1262\n",
            "Epoch 8/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0516 - acc: 0.1513\n",
            "Epoch 9/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0509 - acc: 0.1688\n",
            "Epoch 10/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0504 - acc: 0.2037\n",
            "Epoch 11/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0500 - acc: 0.2137\n",
            "Epoch 12/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0495 - acc: 0.2512\n",
            "Epoch 13/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0494 - acc: 0.2512\n",
            "Epoch 14/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0490 - acc: 0.2863\n",
            "Epoch 15/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0486 - acc: 0.3088\n",
            "Epoch 16/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0484 - acc: 0.3237\n",
            "Epoch 17/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0480 - acc: 0.3438\n",
            "Epoch 18/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0478 - acc: 0.3637\n",
            "Epoch 19/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0479 - acc: 0.3500\n",
            "Epoch 20/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0471 - acc: 0.3950\n",
            "Epoch 21/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0473 - acc: 0.3787\n",
            "Epoch 22/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0470 - acc: 0.3938\n",
            "Epoch 23/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0465 - acc: 0.4325\n",
            "Epoch 24/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0461 - acc: 0.4537\n",
            "Epoch 25/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0461 - acc: 0.4412\n",
            "Epoch 26/400\n",
            "800/800 [==============================] - 0s 190us/step - loss: 0.0467 - acc: 0.4150\n",
            "Epoch 27/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0454 - acc: 0.4737\n",
            "Epoch 28/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0453 - acc: 0.4950\n",
            "Epoch 29/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0454 - acc: 0.4787\n",
            "Epoch 30/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0447 - acc: 0.5175\n",
            "Epoch 31/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0445 - acc: 0.5262\n",
            "Epoch 32/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0445 - acc: 0.5375\n",
            "Epoch 33/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0446 - acc: 0.5225\n",
            "Epoch 34/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0445 - acc: 0.5312\n",
            "Epoch 35/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0440 - acc: 0.5637\n",
            "Epoch 36/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0440 - acc: 0.5588\n",
            "Epoch 37/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0435 - acc: 0.5975\n",
            "Epoch 38/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0434 - acc: 0.6025\n",
            "Epoch 39/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0431 - acc: 0.5950\n",
            "Epoch 40/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0430 - acc: 0.6062\n",
            "Epoch 41/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0427 - acc: 0.6250\n",
            "Epoch 42/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0427 - acc: 0.6287\n",
            "Epoch 43/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0421 - acc: 0.6438\n",
            "Epoch 44/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0420 - acc: 0.6575\n",
            "Epoch 45/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0417 - acc: 0.6713\n",
            "Epoch 46/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0415 - acc: 0.6775\n",
            "Epoch 47/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0425 - acc: 0.6225\n",
            "Epoch 48/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0414 - acc: 0.6650\n",
            "Epoch 49/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0414 - acc: 0.6900\n",
            "Epoch 50/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0411 - acc: 0.6862\n",
            "Epoch 51/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0409 - acc: 0.6863\n",
            "Epoch 52/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0405 - acc: 0.7037\n",
            "Epoch 53/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0406 - acc: 0.6988\n",
            "Epoch 54/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0402 - acc: 0.7100\n",
            "Epoch 55/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0406 - acc: 0.6925\n",
            "Epoch 56/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0400 - acc: 0.7163\n",
            "Epoch 57/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0397 - acc: 0.7250\n",
            "Epoch 58/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0397 - acc: 0.7262\n",
            "Epoch 59/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0399 - acc: 0.7075\n",
            "Epoch 60/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0394 - acc: 0.7450\n",
            "Epoch 61/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0396 - acc: 0.7150\n",
            "Epoch 62/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0391 - acc: 0.7300\n",
            "Epoch 63/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0388 - acc: 0.7337\n",
            "Epoch 64/400\n",
            "800/800 [==============================] - 0s 187us/step - loss: 0.0391 - acc: 0.7275\n",
            "Epoch 65/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0388 - acc: 0.7488\n",
            "Epoch 66/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0388 - acc: 0.7475\n",
            "Epoch 67/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0386 - acc: 0.7450\n",
            "Epoch 68/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0381 - acc: 0.7463\n",
            "Epoch 69/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0382 - acc: 0.7625\n",
            "Epoch 70/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0378 - acc: 0.7675\n",
            "Epoch 71/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0384 - acc: 0.7400\n",
            "Epoch 72/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0372 - acc: 0.7950\n",
            "Epoch 73/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0370 - acc: 0.7875\n",
            "Epoch 74/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0372 - acc: 0.7850\n",
            "Epoch 75/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0370 - acc: 0.7812\n",
            "Epoch 76/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0371 - acc: 0.7875\n",
            "Epoch 77/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0365 - acc: 0.8050\n",
            "Epoch 78/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0364 - acc: 0.8050\n",
            "Epoch 79/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0361 - acc: 0.8225\n",
            "Epoch 80/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0363 - acc: 0.8000\n",
            "Epoch 81/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0368 - acc: 0.7975\n",
            "Epoch 82/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0361 - acc: 0.8050\n",
            "Epoch 83/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0359 - acc: 0.8150\n",
            "Epoch 84/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0358 - acc: 0.8212\n",
            "Epoch 85/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0357 - acc: 0.8150\n",
            "Epoch 86/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0359 - acc: 0.8075\n",
            "Epoch 87/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0361 - acc: 0.8087\n",
            "Epoch 88/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0363 - acc: 0.8000\n",
            "Epoch 89/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0358 - acc: 0.8162\n",
            "Epoch 90/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0352 - acc: 0.8188\n",
            "Epoch 91/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0348 - acc: 0.8250\n",
            "Epoch 92/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0346 - acc: 0.8350\n",
            "Epoch 93/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0346 - acc: 0.8288\n",
            "Epoch 94/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0343 - acc: 0.8513\n",
            "Epoch 95/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0344 - acc: 0.8413\n",
            "Epoch 96/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0340 - acc: 0.8613\n",
            "Epoch 97/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0339 - acc: 0.8575\n",
            "Epoch 98/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0337 - acc: 0.8600\n",
            "Epoch 99/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0338 - acc: 0.8613\n",
            "Epoch 100/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0345 - acc: 0.8325\n",
            "Epoch 101/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0342 - acc: 0.8550\n",
            "Epoch 102/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0341 - acc: 0.8487\n",
            "Epoch 103/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0333 - acc: 0.8675\n",
            "Epoch 104/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0331 - acc: 0.8662\n",
            "Epoch 105/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0329 - acc: 0.8775\n",
            "Epoch 106/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0328 - acc: 0.8775\n",
            "Epoch 107/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0328 - acc: 0.8675\n",
            "Epoch 108/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0326 - acc: 0.8837\n",
            "Epoch 109/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0331 - acc: 0.8688\n",
            "Epoch 110/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0325 - acc: 0.8763\n",
            "Epoch 111/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0326 - acc: 0.8762\n",
            "Epoch 112/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0325 - acc: 0.8788\n",
            "Epoch 113/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0322 - acc: 0.8775\n",
            "Epoch 114/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0320 - acc: 0.8900\n",
            "Epoch 115/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0323 - acc: 0.8838\n",
            "Epoch 116/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0323 - acc: 0.8888\n",
            "Epoch 117/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0327 - acc: 0.8737\n",
            "Epoch 118/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0315 - acc: 0.8988\n",
            "Epoch 119/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0314 - acc: 0.9025\n",
            "Epoch 120/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0316 - acc: 0.8950\n",
            "Epoch 121/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0314 - acc: 0.8912\n",
            "Epoch 122/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0315 - acc: 0.8850\n",
            "Epoch 123/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0318 - acc: 0.8950\n",
            "Epoch 124/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0311 - acc: 0.8988\n",
            "Epoch 125/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0309 - acc: 0.8950\n",
            "Epoch 126/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0310 - acc: 0.9063\n",
            "Epoch 127/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0309 - acc: 0.9087\n",
            "Epoch 128/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0304 - acc: 0.9075\n",
            "Epoch 129/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0306 - acc: 0.9012\n",
            "Epoch 130/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0304 - acc: 0.9113\n",
            "Epoch 131/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0304 - acc: 0.9063\n",
            "Epoch 132/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0305 - acc: 0.9125\n",
            "Epoch 133/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0301 - acc: 0.9200\n",
            "Epoch 134/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0300 - acc: 0.9112\n",
            "Epoch 135/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0300 - acc: 0.9188\n",
            "Epoch 136/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0298 - acc: 0.9150\n",
            "Epoch 137/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0299 - acc: 0.9175\n",
            "Epoch 138/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0297 - acc: 0.9150\n",
            "Epoch 139/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0300 - acc: 0.9175\n",
            "Epoch 140/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0309 - acc: 0.9000\n",
            "Epoch 141/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0296 - acc: 0.9188\n",
            "Epoch 142/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0294 - acc: 0.9200\n",
            "Epoch 143/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0291 - acc: 0.9250\n",
            "Epoch 144/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0292 - acc: 0.9213\n",
            "Epoch 145/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0292 - acc: 0.9213\n",
            "Epoch 146/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0292 - acc: 0.9225\n",
            "Epoch 147/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0289 - acc: 0.9250\n",
            "Epoch 148/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0288 - acc: 0.9188\n",
            "Epoch 149/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0293 - acc: 0.9237\n",
            "Epoch 150/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0297 - acc: 0.9075\n",
            "Epoch 151/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0290 - acc: 0.9125\n",
            "Epoch 152/400\n",
            "800/800 [==============================] - 0s 156us/step - loss: 0.0284 - acc: 0.9238\n",
            "Epoch 153/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0283 - acc: 0.9300\n",
            "Epoch 154/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0287 - acc: 0.9338\n",
            "Epoch 155/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0290 - acc: 0.9188\n",
            "Epoch 156/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0285 - acc: 0.9263\n",
            "Epoch 157/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0280 - acc: 0.9338\n",
            "Epoch 158/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0282 - acc: 0.9275\n",
            "Epoch 159/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0279 - acc: 0.9400\n",
            "Epoch 160/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0286 - acc: 0.9300\n",
            "Epoch 161/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0290 - acc: 0.9112\n",
            "Epoch 162/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0280 - acc: 0.9363\n",
            "Epoch 163/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0281 - acc: 0.9312\n",
            "Epoch 164/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0279 - acc: 0.9300\n",
            "Epoch 165/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0277 - acc: 0.9388\n",
            "Epoch 166/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0275 - acc: 0.9338\n",
            "Epoch 167/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0278 - acc: 0.9275\n",
            "Epoch 168/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0273 - acc: 0.9412\n",
            "Epoch 169/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0271 - acc: 0.9425\n",
            "Epoch 170/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0271 - acc: 0.9412\n",
            "Epoch 171/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0271 - acc: 0.9400\n",
            "Epoch 172/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0275 - acc: 0.9312\n",
            "Epoch 173/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0271 - acc: 0.9400\n",
            "Epoch 174/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0270 - acc: 0.9425\n",
            "Epoch 175/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0269 - acc: 0.9475\n",
            "Epoch 176/400\n",
            "800/800 [==============================] - 0s 189us/step - loss: 0.0272 - acc: 0.9387\n",
            "Epoch 177/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0266 - acc: 0.9425\n",
            "Epoch 178/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0265 - acc: 0.9425\n",
            "Epoch 179/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0269 - acc: 0.9425\n",
            "Epoch 180/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0274 - acc: 0.9225\n",
            "Epoch 181/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0264 - acc: 0.9463\n",
            "Epoch 182/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0263 - acc: 0.9462\n",
            "Epoch 183/400\n",
            "800/800 [==============================] - 0s 191us/step - loss: 0.0262 - acc: 0.9487\n",
            "Epoch 184/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0264 - acc: 0.9450\n",
            "Epoch 185/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0267 - acc: 0.9412\n",
            "Epoch 186/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0266 - acc: 0.9412\n",
            "Epoch 187/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0262 - acc: 0.9438\n",
            "Epoch 188/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0259 - acc: 0.9513\n",
            "Epoch 189/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0259 - acc: 0.9500\n",
            "Epoch 190/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0261 - acc: 0.9450\n",
            "Epoch 191/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0258 - acc: 0.9513\n",
            "Epoch 192/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0256 - acc: 0.9500\n",
            "Epoch 193/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0260 - acc: 0.9513\n",
            "Epoch 194/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0257 - acc: 0.9488\n",
            "Epoch 195/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0253 - acc: 0.9550\n",
            "Epoch 196/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0253 - acc: 0.9500\n",
            "Epoch 197/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0252 - acc: 0.9563\n",
            "Epoch 198/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0251 - acc: 0.9538\n",
            "Epoch 199/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0253 - acc: 0.9537\n",
            "Epoch 200/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0251 - acc: 0.9487\n",
            "Epoch 201/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0250 - acc: 0.9625\n",
            "Epoch 202/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0251 - acc: 0.9588\n",
            "Epoch 203/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0250 - acc: 0.9550\n",
            "Epoch 204/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0249 - acc: 0.9600\n",
            "Epoch 205/400\n",
            "800/800 [==============================] - 0s 184us/step - loss: 0.0250 - acc: 0.9537\n",
            "Epoch 206/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0252 - acc: 0.9513\n",
            "Epoch 207/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0254 - acc: 0.9450\n",
            "Epoch 208/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0246 - acc: 0.9575\n",
            "Epoch 209/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0249 - acc: 0.9612\n",
            "Epoch 210/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0251 - acc: 0.9500\n",
            "Epoch 211/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0245 - acc: 0.9625\n",
            "Epoch 212/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0246 - acc: 0.9562\n",
            "Epoch 213/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0246 - acc: 0.9562\n",
            "Epoch 214/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0245 - acc: 0.9612\n",
            "Epoch 215/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0241 - acc: 0.9600\n",
            "Epoch 216/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0240 - acc: 0.9600\n",
            "Epoch 217/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0241 - acc: 0.9588\n",
            "Epoch 218/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0241 - acc: 0.9600\n",
            "Epoch 219/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0240 - acc: 0.9600\n",
            "Epoch 220/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0241 - acc: 0.9513\n",
            "Epoch 221/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0240 - acc: 0.9637\n",
            "Epoch 222/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0238 - acc: 0.9638\n",
            "Epoch 223/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0240 - acc: 0.9563\n",
            "Epoch 224/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0238 - acc: 0.9638\n",
            "Epoch 225/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0236 - acc: 0.9613\n",
            "Epoch 226/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0236 - acc: 0.9637\n",
            "Epoch 227/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0241 - acc: 0.9575\n",
            "Epoch 228/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0240 - acc: 0.9650\n",
            "Epoch 229/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0234 - acc: 0.9625\n",
            "Epoch 230/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0233 - acc: 0.9625\n",
            "Epoch 231/400\n",
            "800/800 [==============================] - 0s 181us/step - loss: 0.0235 - acc: 0.9588\n",
            "Epoch 232/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0234 - acc: 0.9588\n",
            "Epoch 233/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0233 - acc: 0.9663\n",
            "Epoch 234/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0236 - acc: 0.9600\n",
            "Epoch 235/400\n",
            "800/800 [==============================] - 0s 187us/step - loss: 0.0234 - acc: 0.9613\n",
            "Epoch 236/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0230 - acc: 0.9638\n",
            "Epoch 237/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0230 - acc: 0.9625\n",
            "Epoch 238/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0231 - acc: 0.9588\n",
            "Epoch 239/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0235 - acc: 0.9575\n",
            "Epoch 240/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0232 - acc: 0.9625\n",
            "Epoch 241/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0229 - acc: 0.9612\n",
            "Epoch 242/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0229 - acc: 0.9663\n",
            "Epoch 243/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0227 - acc: 0.9638\n",
            "Epoch 244/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0230 - acc: 0.9663\n",
            "Epoch 245/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0225 - acc: 0.9700\n",
            "Epoch 246/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0227 - acc: 0.9650\n",
            "Epoch 247/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0228 - acc: 0.9625\n",
            "Epoch 248/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0227 - acc: 0.9663\n",
            "Epoch 249/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0226 - acc: 0.9675\n",
            "Epoch 250/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0229 - acc: 0.9663\n",
            "Epoch 251/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0224 - acc: 0.9713\n",
            "Epoch 252/400\n",
            "800/800 [==============================] - 0s 185us/step - loss: 0.0226 - acc: 0.9600\n",
            "Epoch 253/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0223 - acc: 0.9638\n",
            "Epoch 254/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0222 - acc: 0.9713\n",
            "Epoch 255/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0220 - acc: 0.9663\n",
            "Epoch 256/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0222 - acc: 0.9675\n",
            "Epoch 257/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0223 - acc: 0.9638\n",
            "Epoch 258/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0220 - acc: 0.9713\n",
            "Epoch 259/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0218 - acc: 0.9700\n",
            "Epoch 260/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0222 - acc: 0.9700\n",
            "Epoch 261/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0220 - acc: 0.9625\n",
            "Epoch 262/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0218 - acc: 0.9713\n",
            "Epoch 263/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0218 - acc: 0.9637\n",
            "Epoch 264/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0216 - acc: 0.9663\n",
            "Epoch 265/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0215 - acc: 0.9700\n",
            "Epoch 266/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0215 - acc: 0.9700\n",
            "Epoch 267/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0218 - acc: 0.9675\n",
            "Epoch 268/400\n",
            "800/800 [==============================] - 0s 189us/step - loss: 0.0225 - acc: 0.9663\n",
            "Epoch 269/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0220 - acc: 0.9638\n",
            "Epoch 270/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0213 - acc: 0.9700\n",
            "Epoch 271/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0213 - acc: 0.9725\n",
            "Epoch 272/400\n",
            "800/800 [==============================] - 0s 200us/step - loss: 0.0213 - acc: 0.9713\n",
            "Epoch 273/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0212 - acc: 0.9688\n",
            "Epoch 274/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0212 - acc: 0.9675\n",
            "Epoch 275/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0214 - acc: 0.9688\n",
            "Epoch 276/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0214 - acc: 0.9750\n",
            "Epoch 277/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0211 - acc: 0.9688\n",
            "Epoch 278/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0210 - acc: 0.9725\n",
            "Epoch 279/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0209 - acc: 0.9675\n",
            "Epoch 280/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0209 - acc: 0.9738\n",
            "Epoch 281/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0213 - acc: 0.9675\n",
            "Epoch 282/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0214 - acc: 0.9687\n",
            "Epoch 283/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0210 - acc: 0.9688\n",
            "Epoch 284/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0208 - acc: 0.9725\n",
            "Epoch 285/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0211 - acc: 0.9700\n",
            "Epoch 286/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0211 - acc: 0.9725\n",
            "Epoch 287/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0206 - acc: 0.9725\n",
            "Epoch 288/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0208 - acc: 0.9725\n",
            "Epoch 289/400\n",
            "800/800 [==============================] - 0s 177us/step - loss: 0.0206 - acc: 0.9763\n",
            "Epoch 290/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0206 - acc: 0.9775\n",
            "Epoch 291/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0205 - acc: 0.9738\n",
            "Epoch 292/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0206 - acc: 0.9688\n",
            "Epoch 293/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0206 - acc: 0.9700\n",
            "Epoch 294/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0203 - acc: 0.9738\n",
            "Epoch 295/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0203 - acc: 0.9750\n",
            "Epoch 296/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0202 - acc: 0.9725\n",
            "Epoch 297/400\n",
            "800/800 [==============================] - 0s 179us/step - loss: 0.0203 - acc: 0.9750\n",
            "Epoch 298/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0202 - acc: 0.9713\n",
            "Epoch 299/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0202 - acc: 0.9763\n",
            "Epoch 300/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0207 - acc: 0.9650\n",
            "Epoch 301/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0207 - acc: 0.9675\n",
            "Epoch 302/400\n",
            "800/800 [==============================] - 0s 174us/step - loss: 0.0200 - acc: 0.9738\n",
            "Epoch 303/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0205 - acc: 0.9725\n",
            "Epoch 304/400\n",
            "800/800 [==============================] - 0s 158us/step - loss: 0.0200 - acc: 0.9725\n",
            "Epoch 305/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0199 - acc: 0.9712\n",
            "Epoch 306/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0201 - acc: 0.9738\n",
            "Epoch 307/400\n",
            "800/800 [==============================] - 0s 157us/step - loss: 0.0198 - acc: 0.9738\n",
            "Epoch 308/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0198 - acc: 0.9738\n",
            "Epoch 309/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0198 - acc: 0.9725\n",
            "Epoch 310/400\n",
            "800/800 [==============================] - 0s 186us/step - loss: 0.0197 - acc: 0.9750\n",
            "Epoch 311/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0196 - acc: 0.9763\n",
            "Epoch 312/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0196 - acc: 0.9750\n",
            "Epoch 313/400\n",
            "800/800 [==============================] - 0s 176us/step - loss: 0.0196 - acc: 0.9775\n",
            "Epoch 314/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0200 - acc: 0.9688\n",
            "Epoch 315/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0200 - acc: 0.9738\n",
            "Epoch 316/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0196 - acc: 0.9775\n",
            "Epoch 317/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0200 - acc: 0.9725\n",
            "Epoch 318/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0196 - acc: 0.9775\n",
            "Epoch 319/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0194 - acc: 0.9750\n",
            "Epoch 320/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0196 - acc: 0.9725\n",
            "Epoch 321/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0194 - acc: 0.9750\n",
            "Epoch 322/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0194 - acc: 0.9725\n",
            "Epoch 323/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0193 - acc: 0.9763\n",
            "Epoch 324/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0193 - acc: 0.9763\n",
            "Epoch 325/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0196 - acc: 0.9675\n",
            "Epoch 326/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0195 - acc: 0.9700\n",
            "Epoch 327/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0196 - acc: 0.9725\n",
            "Epoch 328/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0191 - acc: 0.9763\n",
            "Epoch 329/400\n",
            "800/800 [==============================] - 0s 154us/step - loss: 0.0190 - acc: 0.9800\n",
            "Epoch 330/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0194 - acc: 0.9738\n",
            "Epoch 331/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0192 - acc: 0.9750\n",
            "Epoch 332/400\n",
            "800/800 [==============================] - 0s 182us/step - loss: 0.0188 - acc: 0.9788\n",
            "Epoch 333/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0189 - acc: 0.9763\n",
            "Epoch 334/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0190 - acc: 0.9750\n",
            "Epoch 335/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0188 - acc: 0.9788\n",
            "Epoch 336/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0188 - acc: 0.9738\n",
            "Epoch 337/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0189 - acc: 0.9738\n",
            "Epoch 338/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0187 - acc: 0.9713\n",
            "Epoch 339/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0188 - acc: 0.9763\n",
            "Epoch 340/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0187 - acc: 0.9788\n",
            "Epoch 341/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0189 - acc: 0.9763\n",
            "Epoch 342/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0188 - acc: 0.9763\n",
            "Epoch 343/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0187 - acc: 0.9738\n",
            "Epoch 344/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0187 - acc: 0.9738\n",
            "Epoch 345/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0187 - acc: 0.9763\n",
            "Epoch 346/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0185 - acc: 0.9788\n",
            "Epoch 347/400\n",
            "800/800 [==============================] - 0s 180us/step - loss: 0.0186 - acc: 0.9775\n",
            "Epoch 348/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0184 - acc: 0.9800\n",
            "Epoch 349/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0185 - acc: 0.9750\n",
            "Epoch 350/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0184 - acc: 0.9763\n",
            "Epoch 351/400\n",
            "800/800 [==============================] - 0s 163us/step - loss: 0.0184 - acc: 0.9763\n",
            "Epoch 352/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0183 - acc: 0.9825\n",
            "Epoch 353/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0184 - acc: 0.9713\n",
            "Epoch 354/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0182 - acc: 0.9750\n",
            "Epoch 355/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0182 - acc: 0.9775\n",
            "Epoch 356/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0183 - acc: 0.9775\n",
            "Epoch 357/400\n",
            "800/800 [==============================] - 0s 169us/step - loss: 0.0181 - acc: 0.9788\n",
            "Epoch 358/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0180 - acc: 0.9813\n",
            "Epoch 359/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0182 - acc: 0.9775\n",
            "Epoch 360/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0181 - acc: 0.9763\n",
            "Epoch 361/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0181 - acc: 0.9738\n",
            "Epoch 362/400\n",
            "800/800 [==============================] - 0s 187us/step - loss: 0.0180 - acc: 0.9788\n",
            "Epoch 363/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0181 - acc: 0.9775\n",
            "Epoch 364/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0180 - acc: 0.9775\n",
            "Epoch 365/400\n",
            "800/800 [==============================] - 0s 159us/step - loss: 0.0181 - acc: 0.9800\n",
            "Epoch 366/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0182 - acc: 0.9763\n",
            "Epoch 367/400\n",
            "800/800 [==============================] - 0s 175us/step - loss: 0.0178 - acc: 0.9788\n",
            "Epoch 368/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0177 - acc: 0.9775\n",
            "Epoch 369/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0178 - acc: 0.9788\n",
            "Epoch 370/400\n",
            "800/800 [==============================] - 0s 190us/step - loss: 0.0177 - acc: 0.9775\n",
            "Epoch 371/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0177 - acc: 0.9800\n",
            "Epoch 372/400\n",
            "800/800 [==============================] - 0s 170us/step - loss: 0.0177 - acc: 0.9813\n",
            "Epoch 373/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0176 - acc: 0.9763\n",
            "Epoch 374/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0176 - acc: 0.9788\n",
            "Epoch 375/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0176 - acc: 0.9813\n",
            "Epoch 376/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0175 - acc: 0.9813\n",
            "Epoch 377/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0175 - acc: 0.9788\n",
            "Epoch 378/400\n",
            "800/800 [==============================] - 0s 164us/step - loss: 0.0174 - acc: 0.9800\n",
            "Epoch 379/400\n",
            "800/800 [==============================] - 0s 183us/step - loss: 0.0174 - acc: 0.9813\n",
            "Epoch 380/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0175 - acc: 0.9813\n",
            "Epoch 381/400\n",
            "800/800 [==============================] - 0s 173us/step - loss: 0.0175 - acc: 0.9813\n",
            "Epoch 382/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0173 - acc: 0.9813\n",
            "Epoch 383/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0173 - acc: 0.9788\n",
            "Epoch 384/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0173 - acc: 0.9775\n",
            "Epoch 385/400\n",
            "800/800 [==============================] - 0s 178us/step - loss: 0.0173 - acc: 0.9800\n",
            "Epoch 386/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0173 - acc: 0.9763\n",
            "Epoch 387/400\n",
            "800/800 [==============================] - 0s 171us/step - loss: 0.0174 - acc: 0.9825\n",
            "Epoch 388/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0176 - acc: 0.9800\n",
            "Epoch 389/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0172 - acc: 0.9788\n",
            "Epoch 390/400\n",
            "800/800 [==============================] - 0s 172us/step - loss: 0.0174 - acc: 0.9788\n",
            "Epoch 391/400\n",
            "800/800 [==============================] - 0s 160us/step - loss: 0.0173 - acc: 0.9788\n",
            "Epoch 392/400\n",
            "800/800 [==============================] - 0s 193us/step - loss: 0.0171 - acc: 0.9800\n",
            "Epoch 393/400\n",
            "800/800 [==============================] - 0s 162us/step - loss: 0.0172 - acc: 0.9813\n",
            "Epoch 394/400\n",
            "800/800 [==============================] - 0s 165us/step - loss: 0.0172 - acc: 0.9788\n",
            "Epoch 395/400\n",
            "800/800 [==============================] - 0s 167us/step - loss: 0.0169 - acc: 0.9800\n",
            "Epoch 396/400\n",
            "800/800 [==============================] - 0s 168us/step - loss: 0.0169 - acc: 0.9825\n",
            "Epoch 397/400\n",
            "800/800 [==============================] - 0s 166us/step - loss: 0.0169 - acc: 0.9763\n",
            "Epoch 398/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0168 - acc: 0.9800\n",
            "Epoch 399/400\n",
            "800/800 [==============================] - 0s 161us/step - loss: 0.0169 - acc: 0.9813\n",
            "Epoch 400/400\n",
            "800/800 [==============================] - 0s 188us/step - loss: 0.0168 - acc: 0.9813\n",
            "200/200 [==============================] - 1s 3ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOTrAsxDB056",
        "colab_type": "code",
        "outputId": "2ace7a3b-6c3c-44a9-911d-272a321f319b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(cross_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.91       0.88000003 0.91500002 0.92500001 0.95000002]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QP1Wg4rABYm",
        "colab_type": "text"
      },
      "source": [
        "**We see that the different runs perform similarly.**"
      ]
    }
  ]
}